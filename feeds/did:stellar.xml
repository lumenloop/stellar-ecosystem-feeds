<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
  <title type="text">Mavennet - Medium</title>
  <link rel="alternate" type="text/html" href="https://medium.com/mavennet?source=rss----9c1cefd992af---4"/>
  <link rel="self" type="application/atom+xml" href="http://10.0.0.124:3044/?action=display&amp;bridge=FeedFinderBridge&amp;url=https%3A%2F%2Fmedium.com%2Fmavennet&amp;strip=on&amp;_cache_timeout=3600&amp;format=Atom"/>
  <icon>https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png</icon>
  <logo>https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png</logo>
  <id>http://10.0.0.124:3044/?action=display&amp;bridge=FeedFinderBridge&amp;url=https%3A%2F%2Fmedium.com%2Fmavennet&amp;strip=on&amp;_cache_timeout=3600&amp;format=Atom</id>
  <updated>2025-04-11T22:07:44+00:00</updated>
  <author>
    <name>RSS-Bridge</name>
  </author>
  <entry>
    <title type="html">Crafting a Seamless Future with Globally Interoperable Digital Identity</title>
    <published>2023-09-25T16:21:39+00:00</published>
    <updated>2023-09-25T16:21:39+00:00</updated>
    <id>https://medium.com/mavennet/crafting-a-seamless-future-with-globally-interoperable-digital-identity-c583754bafdb?source=rss----9c1cefd992af---4</id>
    <link rel="alternate" type="text/html" href="https://medium.com/mavennet/crafting-a-seamless-future-with-globally-interoperable-digital-identity-c583754bafdb?source=rss----9c1cefd992af---4"/>
    <author>
      <name>Ayelet Saly</name>
    </author>
    <content type="html">Trust, Authentication, and CooperationAyelet SalyThe challenge of establishing trust and authenticating the credentials of individuals or organizations has deep historical roots. In any collaborative endeavor involving individuals or entities, mutual knowledge about each party becomes imperative. However, acquiring this knowledge and validating it presents distinct challenges. While gathering information is a straightforward task, authenticating that information requires placing trust in the source of that information or a trusted third party who can vouch for their claims.In a world characterized by localized economies, this process was relatively straightforward, as people within communities were well-acquainted with each other. However, as individuals began to cooperate beyond the boundaries of their immediate communities, the necessity for third-party authentication became apparent. For example, during medieval Europe, heralds were entrusted by all parties to validate the names, lineage, and other qualifications of knights. With the emergence of modern nations and the expansion of global trade, the task of establishing trust grew more challenging, necessitating authentication methods that could transcend geographical boundaries. Paper certificates filled this role, as people collectively agreed to place trust in these documents when signed by a reputable authority, such as a government or university. The trust in these documents, like passports and driver’s licenses, relied on the issuing entity’s commitment to ensuring that no misuse of the documents or a holder’s identity occurred.In today’s digital landscape, these traditional documents face significant challenges in terms of online and offline authentication, particularly with the possibilities that lay in artificial intelligence.These authentication documents were designed for a paper-based world and worked well in those boundaries, much like how horse-drawn chariots served their purpose during Roman times. However, in today’s digital world, they have many limitations:· Paper-based documents offer limited proof of identity; they are challenging to digitally verify and, paradoxically, susceptible to fraud.· Assertions about an organization or individual are equally static and hard to update over time.· Privacy of those assertions is binary. Either you are able to see the whole document, including information that is not necessary for the transaction, or you can’t see anything. There is no middle ground.· These identity documents are controlled by the issuing authority, not by the entity it identifies. If this authority becomes “unavailable” for any reason — such as during a time of war or revolution — individuals may find themselves unable to verify their identity or qualifications. If the institution that certified an individual goes bankrupt, who will validate their credentials? Relying on the continued existence and credibility of the issuer places a significant burden on individuals.New technologies are emerging to displace these old systems. While traditional documents inspired trust, digitalization provides the opportunity to create new threads of trust. A decentralized model for identity, known as Decentralized IDs (DIDs), coupled with Verifiable Credentials (VCs), offers a near-seamless method for automatically verifying claims about individuals, organizations, and even commodities.What Are DIDs?The concept of self-sovereign identity is not new. As the digital world expanded, the need for a privacy-focused, user-controlled identity system grew apparent. About seven years ago, a group called IIW[1] began exploring the use of blockchain for identity purposes, leading to the creation of the concept of DIDs[2]. The World Wide Web Consortium (W3C)[3], the organization responsible for maintaining the web’s structure, recognized the importance of introducing an identity layer to the internet — a missing piece of the puzzle. Consequently, DIDs emerged as a standardized solution.DIDs represent a paradigm shift. They herald the end of identities controlled by centralized entities. With DIDs, users become the true owners of their digital identities, holding the keys to their personal information in the vast realm of the internet. DIDs revolutionize not only ownership but also the way we manage, represent, secure, and utilize our identities. They introduce a new approach to privacy management and provide a fresh framework for personal and business interactions on a global scale.So, what precisely are DIDs? They are a novel form of identifier that enables verifiable, self-sovereign digital identities. They are entirely under the control of the DID holder, independent of centralized registries, authorities, or intermediaries. Think of a DID as your digital fingerprint, operating like an exclusive keychain. Traditionally, whenever you needed access to a place, such as a website, you received a key or password from a key provider, like Facebook. Now, envision having a universal keychain where you possess all the keys, and no one else can duplicate or confiscate them. This is the power of DIDs — a universal keychain for the digital world.The DID architecture comprises a universally unique identifier generated in a decentralized manner, along with the following components:· DID Documents: When discussing a DID, one is often referring to a DID document. This document, akin to a personal profile card, contains information about the DID, including public keys, authentication protocols, and service endpoints.· DID Methods: Similar to how different countries have distinct rules for issuing passports, various systems have their own rules governing the creation, retrieval, modification, and deactivation of DIDs. These “countries” and rules are delineated in DID Methods, tailored to each registry system which can be a blockchain or other decentralized system.Creating and interacting with a DID· Verifiable Credentials: Using a library card as an example the VC not only confirms your identity but also maintains verifiable information, such as your borrowing history, one can hold verifiable credentials referring to a DID. These credentials provide proof of specific claims, such as “has never lost a library book,” without disclosing all the details.Own Your IdentityIn prior identity models, your identity belonged to the issuer, whether an organization (such as your workplace), a government (as indicated in the fine print of your passport), or an identity provider (e.g., Google). In the self-sovereign realm, individuals assume control over their identity identifier, which they create themselves. The decentralized network ensures that the ID remains universally unique and immutable.A Decentralized Identifier (DID) consists of a unique combination of numbers, letters, and symbols that can be validated to confirm your identity in both digital and non-digital realms. With your DID, individuals or entities can issue Verifiable Credentials (VCs) to acknowledge your status, whether it’s ownership (e.g., a land deed), achievements (e.g., a degree), health status, or even personal information like your name and age when needed for verification.Consider this scenario: Joe works for an organization promoting educational equality. The organization offers a scholarship to Alia if she can prove her enrollment in a university. Here’s how DIDs come into play:Alia shares her DID with the university, which, in conjunction with its own DIDs, provides her with a VC confirming her student status. Alia can then share this VC without disclosing any additional information, and Joe can grant her the scholarship.By using DIDs, our private information transitions from being owned and managed by government, university, insurance entities, etc., to being self-owned and self-governed. This transformation has a profound impact on our communication with others, introduces numerous new use cases, and provides a framework for easily verifying information.Four Key Aspects of DIDs:1. Empowerment through Ownership: When your data belongs to you, you are no longer subject to the control of a centralized party, such as the government. This holds significant relevance in today’s world, particularly for immigrants from conflict zones or unstable countries who often find themselves without an “identity” in new nations or when dealing with various aid organizations. According to the World Bank, an astonishing 1.1 billion people lack legal identity today, with far-reaching consequences, including limited access to healthcare, financial services, and vulnerability to human trafficking.2. Eliminating Trust Costs: Trust, or rather the absence of it, takes a considerable toll. DIDs eliminate this cost by providing a verified, indisputable, and unalterable source of truth. For instance, consider the case of hiring a new employee with impressive credentials. To verify that her diploma is genuine and not created by an AI, traditional methods would require significant verification efforts and costs. However, with DIDs, her credentials can be readily and reliably verified.3. Privacy and Security: DIDs empower individuals to share only the information they choose, enhancing privacy. The robust identity verification and authentication mechanisms inherent in DID protocols mitigate the risks of identity theft and fraud, creating a secure digital environment.4. Interoperability: DIDs facilitate seamless integration across platforms and systems, enabling interoperability among diverse applications and networks. This fosters frictionless collaboration, drives innovation, and expands opportunities for all. One compelling application is an Environmental Product Passport that traces the environmental impact of batteries across the supply chain. Mavennet’s flagship product, Neoflow, exemplifies this, leveraging DIDs in collaboration with the US Department of Homeland Security to build traceability for energy.Real-World ImpactDIDs are not merely a concern for tech-savvy individuals; governments worldwide are increasingly embracing these standards for their own identity programs. Here are a few examples:· The US Department of Homeland Security (DHS) actively promotes Decentralized Identifiers (DIDs) to enhance identity management and document security. Through its Silicon Valley Innovation Program (SVIP), the DHS funds projects aimed at creating tamper-proof digital versions of vital documents like Green Cards. The plan is to incorporate DIDs and VCs into many identity documents issued by the DHS, such as passports and green cards.· The European Union (EU) views DIDs as a means to achieve digital sovereignty and enhance data privacy for its citizens. Initiatives like the European Blockchain Services Infrastructure (EBSI) reflect the EU’s commitment to developing a pan-European model in which DIDs empower citizens to securely manage and share their personal data across various services, aligning with the broader goals of the EU’s Digital Identity Framework.· In Canada, efforts are underway to explore digital identity frameworks that enhance transaction security and privacy. The Digital ID and Authentication Council of Canada (DIACC) and related organizations are laying the groundwork for a unified digital identity approach, with the Pan-Canadian Trust Framework (PCTF) setting guidelines. DIDs play a central role in discussions about creating a secure, user-centric identity ecosystem in the country.Apart from governmental and individual identity use cases, standardized decentralized identity offers numerous other possibilities, including:Refugee Identity ManagementMillions of refugees worldwide lack official identification, making it difficult for them to access essential services, assert their rights, or prove their identity. By issuing DIDs to refugees, aid organizations can establish digital, verifiable, and enduring identities for each individual, enabling access to services, confirming family ties, and securely storing educational or professional credentials. DIDs ensure that a refugee’s identity is no longer contingent on a physical document susceptible to loss or destruction.Example: Refugee Identity ManagementLand Rights and OwnershipIn many developing countries, land ownership disputes arise due to poorly maintained, easily altered, or corrupt land registries. Integrating DIDs with a blockchain-based land registry system can irrefutably link land ownership to an individual’s or community’s digital identity. Such a decentralized system ensures that ownership records remain unchanged and legitimate, even in the face of local disputes or political instability.Financial InclusionOver a billion people globally lack access to financial services due to the absence of formal IDs. Without bank accounts or credit histories, their economic potential remains untapped. DIDs enable financial institutions to onboard individuals lacking traditional identification, with digital IDs also recording financial behaviors. This allows people to build credit histories, access loans, and other essential financial services, fostering economic growth and empowerment.Privacy-Preserving VotingEroding trust in voting systems has become a global concern due to fears of tampering, fraud, and voter disenfranchisement. DIDs have the potential to revolutionize voting by providing each eligible voter with a unique, verifiable, and tamper-proof digital identity. Citizens can vote securely from any location, with their identity verified through their DID while remaining anonymous to safeguard privacy. This reduces the risk of vote tampering and enhances accessibility to voting.InsuranceIn many parts of the world, farmers rely on their crops for survival. However, the claiming process for insurance in cases of drought or floods can be time-consuming, leaving farmers in dire need of funds. DIDs can streamline this process by enabling insurer-farmer contracts based on climate conditions. These contracts can offer immediate payments based on predetermined “world states,” such as the amount of rainfall during specific months.Owning your identity on a distributed platform bestows upon you a range of rights in both the physical and digital realms, rights that cannot be denied and can be easily authenticated.In our upcoming post, we will demonstrate how to adopt and develop DIDs on the Stellar network with DID:STLLR.[1] Session Topics from the Internet Identity Workshop since 2005, Decentralized Identity.[2] Decentralized Identifiers (DIDs) v1.0, Core architecture, data model, and representations W3C Recommendation 19 July 2022[3] https://www.w3.org/Crafting a Seamless Future with Globally Interoperable Digital Identity was originally published in Mavennet on Medium, where people are continuing the conversation by highlighting and responding to this story.</content>
  </entry>
  <entry>
    <title type="html">Casting the foundations of steel digital trade using Verifiable Credentials — A North American…</title>
    <published>2022-06-13T15:49:57+00:00</published>
    <updated>2022-06-13T15:49:57+00:00</updated>
    <id>https://medium.com/mavennet/casting-the-foundations-of-steel-digital-trade-using-verifiable-credentials-a-north-american-2f064a1415b4?source=rss----9c1cefd992af---4</id>
    <link rel="alternate" type="text/html" href="https://medium.com/mavennet/casting-the-foundations-of-steel-digital-trade-using-verifiable-credentials-a-north-american-2f064a1415b4?source=rss----9c1cefd992af---4"/>
    <author>
      <name>Krizia Rust</name>
    </author>
    <content type="html">Casting the foundations of steel digital trade using Verifiable Credentials — A North American cross-border interoperability demonstrationLike in many other supply chain-based industries, efficiency and proof of origination in the steel industry is heavily reliant on information sharing across that value chain. The reality today tends to be that organizations adjacent to each other in a supply chain tend to exchange information, but this information does not travel downstream with the product itself, partly due to lack of digitization and partly due to misalignment of end-to-end data standards to share this information. In short, there is a lack of “interoperability”. In order to solve this challenge and enable multi-vendor supply chain visibility, proof of origination, and even more “programmable supply chains” an emerging standard based on Verifiable Credentials (VC) and Decentralized identifiers (DID) developed in the W3C is arising. To show the potential of interoperability and its impact in the context of steel supply chains three companies using completely different technology stacks, Transmute, Credivera, and Mavennet conducted an interoperability demonstration using the above-mentioned standards as part of a Canadian Federal government Pilot sponsored by ISED to help digitize the Canadian steel industry. In the following, we are providing context about the current challenges, highlights about the standards, an overview of the cross-border scenario that was used for the interoperability demonstration, and a video of the demonstration itself.Credivera, Mavennet and Transmute multi-tech crossborder interoperability showcaseDespite the predicted growth of the global steel market — said to reach $1 trillion by 2025 — the industry faces big challenges that have yet to be efficiently addressed. The steel industry often finds itself dealing with concerns of origination and transshipment claims, information silos, and sustainability that we felt could be addressed with the adoption of a new solution. These challenges are not exclusive to the steel industry and finding a solution to these issues is key to finding a way to advance supply chain management.There is a new trend in the industry that points towards the digitization of supply chain management. Interoperability is one way to address the issues facing the steel industry. If implemented correctly, this will break down information silos and make the supply chain more flexible, efficient, secure, and transparent. In an effort to do so, we chose to align building on W3C Verifiable Credentials and Decentralized Identifiers — in doing so, enabling interoperability between organizations with a high degree of security and efficiency. Under the sponsorship of ISED (Government of Canada), we held an interoperability plugfest, that various organizations that utilize different underlying technologies participated in. Those organizations were Transmute, which is a vendor to the US DHS and US CBP, working on the instant verification of credentials, Credivera, which is an identity provider for employees, and Mavennet, a contractor to the Federal government of Canada for the digitization and improvement of the Canadian steel value chain.The above diagram depicts the scenario that was used to conduct the test– it shows the process of a cross-border supply chain in which the steel moves from supplier to buyer. Instead of a hard copy mill test report (MTR), the MTR is submitted and passed along on the blockchain. When the report is sent by the supplier, and received by Canadian Steel co., the report is verified by them and then passed on to the broker, who reviews and submits the MTR to the Canadian Border Security Agency. The CBSA also reviews the documents received and once again, verifies the information, before accepting the product. When the documents are released, Canadian Steel co. moves onto the next step of the process, smelting the steel into hot rolled steel coils. They then issue another MTR and share it with Inspector co. An inspector is hired by Inspector co. to review the documents but first, their employee identity is authenticated, and then they go on to issue the inspection and verify the MTR. Canadian Steel co. passes all their verified documents — the MTRs and inspection report — to their customer, an American-based steel fabricator who checks the documentation and submits the import entry. The Customs and Border Patrol checks the data received, after the employee’s identity is certified, and then verifies the whole process.The actual interoperability demonstration can be seen here:https://medium.com/media/8f391972be3c7cab6f3791589d41d4c0/hrefThe interoperability demonstrated in this scenario was only one-use case, but obviously, the design is made to be flexible to support virtually any scenario that could potentially happen in the industry. This approach to supply chain is not limited to the steel industry and there are other initiatives using these standards for agricultural goods, oil and gas, and eCommerce. Please refer to the traceability vocabulary for more information and to join forces with us to expand the scope of interoperable industries.Casting the foundations of steel digital trade using Verifiable Credentials — A North American… was originally published in Mavennet on Medium, where people are continuing the conversation by highlighting and responding to this story.</content>
  </entry>
  <entry>
    <title type="html">The missing link: digitizing supply chains with portable data</title>
    <published>2021-05-17T17:35:09+00:00</published>
    <updated>2021-05-17T17:35:09+00:00</updated>
    <id>https://medium.com/mavennet/the-missing-link-digitizing-supply-chains-with-portable-data-583b66acc9bc?source=rss----9c1cefd992af---4</id>
    <link rel="alternate" type="text/html" href="https://medium.com/mavennet/the-missing-link-digitizing-supply-chains-with-portable-data-583b66acc9bc?source=rss----9c1cefd992af---4"/>
    <author>
      <name>Mahmoud Alkhraishi</name>
    </author>
    <content type="html">Mahmoud Alkhraishi and Patrick MandicFor the past 20 years, the term digital transformation has been one of the top priorities for C-suites in most forward-looking organizations. Yet, while many organizations claim to have succeeded in their path to digital glory and are able to produce incredibly detailed information about their assets and the efficiency of their operations, interacting with other organizations tends to happen on very inglorious traditional rails and standards: email, pdf, and excel.Most cross-organization processes today are still highly manual, and the ones that aren’t, involve strongly “locked-in” dependencies. This is partly due to the hurdle of documenting and proving justifiable investment in process changes and infrastructure. The impact of these gaps between digital islands is massive at the level of costs to these enterprises, including back-office efforts, reconciliation of data, etc. Most importantly, it represents a missed opportunity for competitive advantage and augmenting the top line. After spending millions of dollars and euros in consultants, IT, and technology gurus, why are so many of our organizations still operating as individual intranets that are opaque and old-fashioned when the time comes to share data?This post is meant to provide an overview of the current difficulties digitizing our economy, the importance of interoperability in the context of supply chains, and the technologies that will enable the next qualitative jump. All three are empirically validated by the interoperability showcase sponsored by the US Department of Homeland Security.A TRADE TRUST LAYERToday’s Internet protocols allow us to confidently transport data from one place to another, however they provide few guarantees as to:The data being truthful and verifiableThe binding data to specific identitiesInterpreting trade data in a programmable wayTo be more explicit, we are missing on the one hand a general-purpose trust layer for verifying information independently across contexts, and on the other the ability to express business information for consumption by unknown future digital systems.Documents like a Bill of Landing or Certificate of Origin today do not conform to any digital standard: every organization has their own forms, and even if the information is sent in digital format, it is rarely consumable or verifiable without human intervention. The same happens to Mill Certificates in the steel industry, Delivery Tickets in the Oil and Gas industry or other types of Certificates of Origin. This is not a new concept and some industries have succeeded in implementing some of the fundamentals, as is the case to a certain extent in the automotive industry realizing in the 90s enabling faster processing of information and onboarding.While most organizations today are digital, interactions between them aren’t. The world is still composed of rigid and loosely coupled “digital islands”.Today something similar is happening with the digitization and scalable efficiency gains in the supply chain traceability. Customs clearance and import brokerage are making great gains in their ability to understand and trust the history of a product. Products can be tracked with the equivalent of a digital passport, verifiably tracking, say, CO2 emissions end-to-end at the per-product level, or programming trade finance, using underlying products as collateral. Imagine how the capitalization process would change if you could give your clients verifiable insights into your supply chain, anticipating when a shipment will be late and automatically rerouting and load-balancing across multiple different suppliers discretely. Most of the innovations enabled by these standards have likely not been imagined yet, much as before the general adoption of TCP/IP, very few people could have anticipated Google Maps, Uber or the like.So how do we enable a layer of digital trading operability at large scale? The answer is DIDs for as many actors as possible and VCs for any transactions that need to be verifiable and auditable.THE RISE OF VERIFIABLE CREDENTIALS (VCs) AND DECENTRALIZED IDENTIFIERS (DIDs)With the development of blockchain, a new kind of distributed PKI security infrastructure is emerging: Decentralized Identifiers (DIDs). DIDs are a type of globally unique identifiers that are resolvable and allow owners of an identifier to cryptographically prove ownership interactively or programmatically. These identifiers enable verifiable digital identity without a centralized authority [1]. In essence, they allow a subject to provide an identity document attached to an identifier, which is resolvable in a variety of ways decoupled from centralized providers and platforms. The mature specification for these kind of identifier systems is currently in the W3C Candidate Recommendation stage.Interactive control proofs allow an entity to segment identities and interaction domains, maintaining control over what organizational data or confidential business insights are revealed in each. A DID functions as an address that resolves to a “DID document” outlining authentication mechanisms and other routing information. For more information about Decentralized Identifiers please refer to [DID SPEC].While organizational identities are secured by these portable and self-managed identifiers, transactions and data points are secured by “Verifiable Credentials”, which are machine-readable, ontologically-anchored and privacy-preserving representations of presenting real world credentials. In their most simple form, they are a set of claims made by an entity presented in an “envelope” containing semantic references, identity references, and a digital signature. These claims are immutable and depending on the signature scheme used, they can even be disclosed selectively. [we should add an example here]DIDs and VCs were initially conceived as an answer to centralized ID, intended to give the identity owner control of their own data, creating a systems design movement commonly referred to as self-sovereign identity (SSI). Builders of organizational applications for supply chain, product tracking, and commercial trade have found these decentralized technological building blocks useful to non-human use-cases as well.NEW OPPORTUNITIES EMERGEThe most immediate application is being able to identify specific individual products and their origination across a large network of stakeholders, as if they essentially had a digital passport, which collects the birth of the product and all the major stations along its journey in a digitally verifiable way. This technology, together with the use of immutable timestamps (for example, those recorded on a blockchain) and an ownership registry (avoiding the “double spending” of assets) makes it very hard for any supply chain actors to misreport, counterfeit or misrepresent information.In the medium term, this technology is expected to allow “digital twins” that span organizational and jurisdictional boundaries. Beyond just origination, VCs and DIDs bring many other clear process improvements for cross-border operations.Specifically, Mavennet is using these technologies to track Oil, Gas and Electricity verifiably. Energy is the largest commodity market globally after currencies, and is expected to transform substantially in the coming years. Digitizing this space is helping in many ways, such as:Verifying Country of originPrototyping immutable “digital twin” system for commodity trackingObjectively documenting environmental footprint to lower cost of capitalUnderstanding true end to end environmental impactStreamlining settlementsEncouraging healthier market and lower barrier to entry by circumventing centralized trackingINTEROP TESTING OVERVIEWSo how do we make this technology mainstream sooner? The Silicon Valley Innovation Program (SVIP), an incubation program backed by the US Department of Homeland Security, has been sponsoring efforts to not only support the standardization path of these technologies but also to demonstrate provable interoperability between pioneering vendors. As part of this initiative, eight organizations gathered to test interoperability between their stacks in different verticals. This meant going beyond data model standards and empirically validating actual interoperability across stacks.This interoperability plugfest included organizations from locations from all over the world — a truly global talent pool.SVIP Participants (Image Source: DHS / SVIP)For the scope of this description, we will focus on the digital trade organizations, focusing on digitization of cross-border processes across the following industries:Steel (Transmute)Agriculture (Mesur.io)Oil and Gas (Mavennet Systems Inc.)eCommerce (Spherity)The goal of the exercise on the digital trade side was focused on proving interoperability at 3 levels:Industry-agnostic Verifiable Credential Interoperability: Show the exchange and proper handling of standards-conformant, confidentiality-preserving portable data across industry verticals, specifically testing common issuance and verification processing.Interoperability across common supply chain elements: Show semantic interoperability across low-level business information common across industries even if it is typically expressed differently, such as a bill of lading (BOL).Smooth data transfer between vendors and Cross Border Patrol (CBP) systems. While Customs and Border Patrol does not currently operate this kind of data infrastructure, small-scale pilots validate assumptions about availability, accuracy, and access for government systems.Test Scenario (Image Source: US Department of Homeland Security (DHS) / Silicon Valley Innovation Program (SVIP))Being able to conduct this exercise was the tip of the iceberg building standards to make this possible. This work is being conducted in under the W3C umbrella in the open through the Credentials Community Group (W3C-CCG) and is the expectation is that many more organizations will continue to join and support the initiative as the standard matures. Mainly the bulk of Mavennet’s work has focused on the following standards:VC-HTTP-API: A minimal, vendor-neutral API that allows for the construction and verification of Verifiable Credentials and Presentations for general-purpose scenarios.A Traceability vocabulary: A Linked Data vocabulary used to construct Verifiable Credentials related to supply chain traceability of commodities and products.RevocationList2020/StatusList2021: A Standard to provide a mechanism for publishing status information about issued credentials.VC-HTTP-APIThe VC HTTP API is a primarily internal/backend API standardizing basic functions for the construction and verification of VCs and VPs. It supports and tests for the following features:Construction of W3C standard-conformant Verifiable CredentialsConstruction of W3C standard-conformant Verifiable Presentations (I.e., interactive holder-proofs of 1 or more VCs)Verification of Credential/Presentation “proofs”, I.e. checking signatures and hashes against the resolution material of issuer and/or holder)Support for multiple DID methodsIssuance and verification of revocable credentials using the RevocationList2020 scheme (soon to be updated to StatusList2021)Selective Disclosure using BBS+ signatures.As is always the case with rapidly growing specifications the VC-HTTP-API is undergoing some changes, including an overhaul of its Documentation, a more refined Roadmap, and a rapidly expanding interoperability and test suite to allow for wider and more robust interoperability.Test Environment (Image Source: DHS / SVIP)The 2020 and 2021 Interoperability plugfests used the VC-HTTP-API as scaffolding through which to test each company’s conformance with the VC data model, i.e., the VC “core spec” incubating in the W3C. The tests were structured to allow each company a matrix of optional features to support beyond the core functionality.There are a number of issues with the current test suite including a lack of true end–to-end testing by taking a credential issued by one company and automatically testing it against other companies’ infrastructure, rather the tests are structured to test each individual company’s compliance with static fixtures. The test harness also includes some mandatory tests that are not strictly in scope of the VC data model, such as support for various DID methods.These issues are currently being addressed and a more robust test-suite is currently being built, in order to have true enterprise end-to-end interoperability.TRACEABILITY VOCABULARYThe traceability vocabulary bridges the gap between existing record-keeping systems and the verifiable exchange of supply chain information across organizations envisioned by proponents of these data portability technologies. This vocabulary has been built with the following defining characteristics:Built based on existing Schema.org and GS1 vocabulary elements to maximize composability and reusabilityCurrently focused on five main market segments: Agriculture, E-Commerce, Oil, Gas, and Steel and MetalsHosted transparently under the W3C-CCG umbrella and open to contributionsGS1-compatible extensible representations of Organization, Bill of Lading, and Inspection ReportsStandardizes the creation of Verifiable Credentials from JSON-LD semantic anchors and JSON-Schema validation schemataThe vocabulary is currently undergoing a number of proposal improvements including more robust support for GS1’s EPCIS, and more accessible documentation.INTEROP OVERVIEW VIDEOFor those interested in more details you can see a video of the interop fest showcasing interoperability across different technologies using the Neoflow platform as front-end.Also, if you are interested in learning in more depth the work performed in the broader SVIP Program refer to this presentation.THE FUTUREThere are a number of exciting changes occurring in the space, and Mavennet is happy to be championing work on including standardized GS1 events in the Traceability Vocab, setting up a standardized timestamping service, and providing typescript implementations of standards-compliant libraries.Most importantly however, we’re working on engaging with industry partners and community members to improve the state of the standards and work towards a more secure, private and efficient future.If you’d like to join us please don’t hesitate to reach out to us at info@mavennet.com or review our traceability product Neoflow https://www.mavennet.com/solutions/neoflow/Note: We would like to thank the SVIP program for the support and the long-term vision and Juan Caballero for his feedback on this article.The missing link: digitizing supply chains with portable data was originally published in Mavennet on Medium, where people are continuing the conversation by highlighting and responding to this story.</content>
  </entry>
  <entry>
    <title type="html">ESG, blockchain and the allegory of the cave</title>
    <published>2021-04-19T17:28:48+00:00</published>
    <updated>2021-04-19T17:28:48+00:00</updated>
    <id>https://medium.com/mavennet/esg-blockchain-and-the-allegory-of-the-cave-da3bca665eb?source=rss----9c1cefd992af---4</id>
    <link rel="alternate" type="text/html" href="https://medium.com/mavennet/esg-blockchain-and-the-allegory-of-the-cave-da3bca665eb?source=rss----9c1cefd992af---4"/>
    <author>
      <name>Mavennet</name>
    </author>
    <content type="html">ESG, blockchain, and the allegory of the caveBy Jim OosterbaanIn the year 375 BC, the Greek philosopher Plato described one of the most well-known philosophical theories, the myth of the cave, to explain human perception. The allegory describes a set of individuals in a cave watching the shadows on the walls of the cave cast by moving objects but not seeing the objects themselves. Plato argues that this is how humans perceive reality, a reality that is an imperfect representation of how things are. For instance, when we see a cup we only see an imperfect representation of the perfect concept of a cup.It is in this light that we need to consider ESG (Environmental, Social &amp;amp; Governance) and its evolution, in order to start measuring the impacts of effective ESG practices. But before we get there, how did we get here and why is ESG so important?ESG is generating significant interest and activity. It is not a fad, nor an end state. It is a concept that has evolved since 1970’s — its evolution a reflection of the changes that have rippled through our societies. These changes continue, ESG and what it means will continue to evolve and its prominence will continue to ascend.Socially responsible investing has a long history but became more noticeable in the 1970’s and 1980’s. As socially concerned investors began to react to the impacts of the Vietnam War, Apartheid, and poor environmental stewardship by business, e.g., Love Canal, Bhopal, Exxon Valdez. The first sustainable investing mutual fund (Pax World) was launched in 1971. A list of socially responsible stocks was published in 1972 by Milton Moskowitz. Concurrently, the first personal computers are introduced in the late ‘70’s.During the latter part of the 1980’s in response to the concerns of the impacts of fossil fuels the Intergovernmental Panel on Climate Change is established. By 1994, 26 money funds with a sustainable investment theme are available to investors. Corporate social responsibility reporting continues to grow and evolve in importance, as you see the early adoption by some companies.In an attempt to address a range of human rights, labour, environmental and anti-corruption issues the United Nations launches the Global Compact Initiative. Concurrently, the Global Reporting Initiative is launched and provides companies with standards on how to present their impacts on these issues. Of note, the Global Compact adopts a number of principles articulated in the Sullivan Principles that were formalized in 1977.Concurrently, Western society continues to change and evolve as long-standing issues are acknowledged and begin to be addressed. The deployment of technology begins to change the nature of work and provide people another means to communicate.The term ESG is coined in a study published by the Global Compact — “Who Cares Wins”. The study provides recommendations on how to reflect ESG financially. At this time more than 12,000 companies are signatories to the Global Compact.Corporate Social Responsibility reporting is evolving to ESG reporting. The most widely used standard is the Financial Sustainability Board’s — TCFD — Task Force on Climate Related Financial Disclosure. Other organizations are working to develop complementary or competing standards. Industry 4.0, the Internet of Things, is beginning to dramatically ease measurement, monitoring, and analysis of the impacts of a company’s activities on their surrounding environment. The deployment of technology is beginning to improve supply chain efficiency, measurement, and communication. Concurrently, the explosion of social media further reduces friction in communication. All these trends support and sustain the influence of ESG as it will continue to evolve.Increased ESG activity and reporting by corporations increases transparency to purchasers and consumers of a company’s products and services. Companies that invest in reducing their environmental footprint, work to include stakeholders in their business are able to differentiate their products from competitors who do not. Purchasers and consumers begin to factor into their purchase decision ESG impacts. Here is where we need to start thinking about measuring impact, not its “imperfect shadows”.How do we do measure in a manner that is secure, that enforces accountability, and that minimizes the amount of effort to report? Work has occurred to make this happen but little has been done to facilitate the communication of this information at the technical level. Generating and consuming information that is verifiable, that cannot be altered or lost in translation, and is immutably linked to the organization that created it even if this information changes hands multiple times is important to effectively demonstrate ESG related activity and its results. Also, it provides the opportunity to credibly allocate environmental impact to individual assets or products in large supply chains. Thus providing the basis for differentiating products.Thanks to the efforts of many, the energy industry is ripe to adopt these new technologies that will enable accurate tracking of attributes, such as verifiable trade credentials, blockchain ledgers, immutable timestamps, etc. — without them we are only chasing shadows on a wall.Neoflow can do this. Neoflow is a platform that enables providing secure and verifiable information across energy value chains, thus enabling the concept of an environmental passport. For more information follow Neoflow on @mavennet_ , send us an email at info@mavennet.com or visit our webpage https://www.mavennet.com/solutions/neoflow/.ESG, blockchain and the allegory of the cave was originally published in Mavennet on Medium, where people are continuing the conversation by highlighting and responding to this story.</content>
  </entry>
  <entry>
    <title type="html">Agile Methodology</title>
    <published>2021-03-31T19:03:05+00:00</published>
    <updated>2021-03-31T19:03:05+00:00</updated>
    <id>https://medium.com/mavennet/agile-methodology-3f8b2f18d508?source=rss----9c1cefd992af---4</id>
    <link rel="alternate" type="text/html" href="https://medium.com/mavennet/agile-methodology-3f8b2f18d508?source=rss----9c1cefd992af---4"/>
    <author>
      <name>Anna Edmonds</name>
    </author>
    <content type="html">The Agile WayWritten by Minakshi Talukdar — Senior Product Owner at MavennetOver the past two decades, the Agile methodology has greatly expanded the success rates in Software Development &amp;amp; Implementation accompanied with greater quality of products and successful go to market strategies. This methodology has also boosted the motivation and productivity of the IT team(s)in software companies. The agile methodology has become a radical alternative to command-and-control-style management process.First let us talk about what is ‘Agile’. It is a ‘Methodology’ that combines certain values and principles, also known as the Agile Manifesto. These values and principle exhibit the courage to admit that building software is complex and that it can’t be perfectly planned, as requirements constantly change.Earlier, with the traditional Waterfall Approach, many software development projects were failing or taking much too long to complete. The reason behind this was either IT Teams were spending too much time to refine the entire set of requirements with little time left for the actual implementation or it was difficult to bring new flavors to the requirements, as a result it led to creation of products that are way too different from what the Customers expected for.That is where Agile introduced the idea of Incremental Delivery Approach. This approach denotes that we do not need to build the entire product all at once. Instead, we build small increments (sometimes called as MVP — Minium Viable Product), collect early feedback and then pivot accordingly.How to implement Agile: Out of several frameworks, Scrum is a very popular framework that follows ‘Agile’ Methodology. As per the Scrum Guide defines it. “Scrum is a lightweight framework that helps people, teams and organizations generate value through adaptive solutions for complex problems.” (scrum.org).Scrum helps teams to work together. The source of the word ‘scrum’ perfectly describes it, a rugby team, training for the big game. Scrum encourages teams to learn through experiences, self-organize while working on a problem, and reflect on their wins and losses in order to continuously improve.Various processes, techniques and methods can be employed within the framework. A simple illustration is shown below to explain the different aspects of Scrum and how they work.Scrum Process and Steps&amp;lt;Image Source: https://bitsinglass.com&amp;gt;The fundamental unit of Scrum is a small team of people, who works in Sprints at a sustainable pace. These Sprints are ideally 2–3 weeks long, depending on the project and milestone deliveries. During these sprint cycles, prescribed events such as Sprint Planning, Daily Stand Up, Sprint Review and Retrospective are used to maintain the team’s focus and consistency.Scrum maintains a living artifact called Product Backlog that covers the scope of the product. The Product Backlog items are frequently updated to showcase the right priority and adequate amount of detailing. In other words, the items with higher business priority are placed at the top of the backlog with sufficient details such as task description, Definition of Done etc. At the beginning of each sprint, the Sprint Planning meeting is scheduled to pull out a subset of these prioritized Product Backlog items, as a ‘To Do’ list for the entire Sprint Window. This subset is known as Sprint Backlog. The scrum team works as self-organized team meaning they internally decide who does what, when, and how by setting internal deadlines and task allocations from this Sprint Backlog. The entire Scrum team is accountable for creating a valuable and useful increment (MVP) by the end of every Sprint.As team proceeds throughout the Sprint, the team discusses a quick status update daily, also known as the Daily Stand Up. During these daily meetings the scrum team members discuss three main points: things that they did the day before, what their plan for that day is and any blockers or impediments on their work.Towards the end of the Sprint window, there is a Sprint Review, in which the scrum team reviews the MVP, newly built features or stories with the business team in order to collect feedback. The agenda of the sprint review meeting is to see if the increment meets the Definition of Done. It is a formal description of the state of the increment when it meets the quality measures &amp;amp; requirements of the product. In other words, the moment a Sprint Backlog item meets the Definition of Done, the respective feature becomes eligible to be part of product increment (MVP). Furthermore, this meeting further helps build the pathway for the next future sprint(s), because all the feedback items are discussed and planned in the upcoming sprint(s).Last but not the least, every sprint ends with a Sprint Retrospective meeting which helps the team to plan ways to increase quality and effectiveness. The most impactful improvements are addressed as soon as possible.Now that we have gone through the details of agile, let’s discuss why it is important. Agile is not just about a group of methods and best practices, but also a mindset that brings the ability to respond to new events or challenges way faster. This Agility is required to be competitive and that is why agile ways of working have become mainstream today with most organizations like Mavennet.In Mavennet, we constantly carry out efficient ways to maintain our work environment, culture and mindset which supports true business agility. Being agile allows us to take risks and plan towards innovative solutions. Apart from our product implementation roadmap, we also encourage the agile mindset for individual growth and foster continuous learning in our organization.Briefly to conclude, the best approach when considering an agile adoption relates to an organization’s context. It must be inclusive with the leadership team and their areas of focus with an eye to accelerated value delivery over output and utilization.Agile Methodology was originally published in Mavennet on Medium, where people are continuing the conversation by highlighting and responding to this story.</content>
  </entry>
  <entry>
    <title type="html">Two Critical Questions for your Enterprise Blockchain Application</title>
    <published>2019-10-29T18:15:43+00:00</published>
    <updated>2019-10-29T18:15:43+00:00</updated>
    <id>https://medium.com/mavennet/two-critical-questions-for-your-enterprise-blockchain-application-14efeef97c81?source=rss----9c1cefd992af---4</id>
    <link rel="alternate" type="text/html" href="https://medium.com/mavennet/two-critical-questions-for-your-enterprise-blockchain-application-14efeef97c81?source=rss----9c1cefd992af---4"/>
    <author>
      <name>Rakesh Gohel</name>
    </author>
    <content type="html">IntroductionA public blockchain orchestrates a trust layer by providing transparency, integrity, traceability, and authenticity of data. It records all transactional activities in the world state. These unique properties attract many to develop applications that record transparent activities among the stakeholders, ownership details to provide traceability to the origin, timestamps for the digital content to prove the existence and more. Matt Spoke, the CEO of Aion Foundation, has highlighted why a public blockchain is a good way forward.Though blockchain storage is open and accessible, it incurs a cost for every write-operations executed in this world state. Thus, it’s imperative to host only an essential portion of the application on the blockchain to minimize the cost. The rest of the application needs to be architected to address the below questions.Critical Questions1. Data Privacy — What data to store on a blockchain?Any data going on a public chain are open, accessible and irrevocable. Thus, public blockchain is not GDPR (and CCPA from next year) compliant unless the data has been encoded with quantum-resistant algorithms and stored.Personally Identifiable Information (PII) or sensitive data compromising user privacy shall not be stored on a blockchain, however, blockchain still needs account aka wallet addresses to individually link them with their real users (can be pseudo-anonymous on the blockchain).2. Blockchain — Which blockchain to build an application on?The performance of software directly depends on the performance of its dependencies and their host environments. The blockchain brings a new paradigm of decentralization architecture where every node on the chain constantly updates its states to maintain the world state. In addition to that, a blockchain application also needs to deal with the below areas and their varied implementations.a) ConsensusA blockchain relies on the distributed consensus of the participant nodes. The PoW (Proof of Work) consensus takes more time to achieve a consensus across the system based on the finality gadget watermark compare to any PoS (Proof of Stake) systems. Similarly, other variant consensus algorithms impact transaction confirmation time which the application needs to deal with without sacrificing the response time.b) Smart contractPublic blockchain smarts contract methods are open and can be invoked by any good or bad users, thus, smart contracts need to ensure the non-corruptibility of their data and protect against the misuse of their business logic. Developing a secure and efficient smart contract requires high skill-sets and a lot of effort for pen-testing. Moreover, based on the complexity and monitory impact of such a smart contract, it needs to go through a long process of security audit with an external organization.c) Block time and number of transactionsBlock production time and a maximum number of transactions that can be incorporated in a block decide the throughput of the system. The application can theoretically generate these many transactions, but those transactions still need to complete with the other applications on the chain. For example, Ethereum currently processes 15 transactions per second.d) Accessing the blockchainThough blockchains are open and consumable, a reliable connection is required to communicate with it using a full node or options like 3rd party e.g., Infura, Blockdaemon, Nodesmith, etc. A full node guarantees an up to date blockchain state, but difficult to manage (especially maintenance and upgrades) and is not cost-effective. If you choose a 3rd party service, it introduces an additional dependency with its inner workings of their delivering transactions to the blockchain, transaction pool management, retrying strategies, and resetting policies.e) Gas cost and priceThe gas cost associated to execute a transaction on blockchain fluctuates based on the demand and supply of the network. The underlying network cryptocurrency price also influences it. Based on application usage patterns, it’s wise to estimate the budget gas cost to run the system for the next three to five years. Purchasing gas in advanced when the prices are lower will help lower costs and using it for storage refunds is a good thing to consider.e) Tools and supportLast but not least is to consider the tooling support for the blockchain ecosystem as it plays a crucial role while developing, deploying, debugging and monitoring smart contracts. Getting technical support in the time of need makes a lot more difference.Dealing with Blockchain IntricaciesMurphy’s law is nullified in the ideal world but in reality, things go wrong and we need to plan for and work with. Some of these issues can be addressed, however, the application still needs to deal with the challenges coming from blockchain intricacies.1) Experiencing more transactions than the blockchain can handleMany applications produce tons of microtransactions these days to record every little detail. The blockchains are yet to achieve scalability at this scale, however, it shouldn’t stop one building applications on top of it. This can be addressed using aggregating transactions (if it’s possible logically) and making fewer transaction commits on the blockchain.The other option to high throughput is to use a sidechain network if available. This ensures the public blockchain benefits are still available to the application while offloading some of the work to the sidechain.2) Failure to execute a transactionA blockchain may fail to pick up a transaction submitted by the application, either this could be an issue with the transaction itself or related to decentralized network issues i.e., reorganization of the chain in blockchain, hard forks, transaction pool failure, etc. The application shall retry in all cases by remembering what has been committed to the blockchain and did not go through. In case of failure resulted from the lower gas price, the application shall re-submit the transaction with a higher gas price, otherwise, retry. This is essential to synchronize the blockchain state and the application’s internal state.3) Congestion on the networkThe application shall have a provision for reacting and adapting to the congestion in the network. If increasing gas prices do not work, the application shall retry periodically or exponentially.The good thing about network congestion is that it may not be immediately possible to commit transactions to the network by other means. This will reduce the risk of a double spend. The application can continue functioning using its internal state and commit the transactions later at a higher gas price on availability.4) Inconsistency between blockchain and internal application stateIf an application is built around blockchain tokenomics, the non-blockchain part of the application shall remember the state of the system which has not been confirmed or committed to the blockchain yet.These pending transactions can be accounted for in the calculation when the users access the application, however, if they directly access the blockchain explorer, the balances or state could be off. If users are smart and have access to their private keys, they can send transactions directly to the blockchain outside of the application and deplete the balances or change the state.One way to address is to implement overdraft protection of the balance in the application and allowing to execute transactions when they have balances to a certain extent. The second approach is implementing a house account for the application to perform state changes on behalf of users.To conclude, building enterprise blockchain applications that have millions of users and transactions may not fit into regular software architecture, therefore, architects need to brace with new challenges and unanswered questions to solve at this scale, however, following this guideline will set steps in the right direction.An important aspect that has not been covered here is managing keys that are used to execute transactions on the blockchain. We shall dive deep in another article.Two Critical Questions for your Enterprise Blockchain Application was originally published in Mavennet on Medium, where people are continuing the conversation by highlighting and responding to this story.</content>
  </entry>
  <entry>
    <title type="html">MavenStamp — An Immutability-as-a-Service solution</title>
    <published>2019-02-25T19:32:17+00:00</published>
    <updated>2019-02-25T19:32:17+00:00</updated>
    <id>https://medium.com/mavennet/mavenstamp-an-immutability-as-a-service-solution-afbd142adef9?source=rss----9c1cefd992af---4</id>
    <link rel="alternate" type="text/html" href="https://medium.com/mavennet/mavenstamp-an-immutability-as-a-service-solution-afbd142adef9?source=rss----9c1cefd992af---4"/>
    <author>
      <name>Yayi Liu</name>
    </author>
    <content type="html">MavenStamp — An Immutability-as-a-Service solutionhttps://www.mavenstamp.comWe are excited to announce that our timestamp product, MavenStamp, has been officially launched and integrated into the Metalyfe browser, a decentralized blockchain browser and multi-wallet portal with the goal of catalyzing the adoption of Web 3.0. Through the partnership with Metalyfe, MavenStamp’s features will capture additional value in the blockchain ecosystem to serve the real world better.What is MavenStamp?MavenStamp is a Mavennet product that offers Immutability-as-a-Service by providing safe and traceable proof of data integrity. Through our fast and seamless API, MavenStamp allows any individual to timestamp any digital content and verify its existence without having to learn the complexity of blockchain technology.How does MavenStamp Work?MavenStamp accepts all kinds of data input, such as plain text, a file or a precomputed hash, subsequently, it generates a unique hash, timestamps it and stores it in the blockchain. The timestamp property gives the data an immutable state after submission and the tamper-proof makes it easier for retrieval for your needs.MavenStamp User FlowWhy MavenStamp?Nowadays, people create and store information based on digital assets, which can be easily copied and modified. Enterprises have a high demand to maintain data integrity and protect their sensitive data. In the past time, we need to rely on trustworthy intermediaries to verify the data integrity, but now blockchain technology opens the door to ensure immutability without the intermediaries.MavenStamp is an effective blockchain solution for data immutability and data integrity. Through a seamless API that can be easily integrated with enterprises’ application, MavenStamp enables the user to timestamp data easily and efficiently. Moreover, MavenStamp is cost-effective that helps enterprises to boost the ROI by offering high throughput value at a low cost.Who can benefit from MavenStamp?MavenStamp helps enterprises thrive in a wide range of industries, ranging from financial, supply chain, government, and others. MavenStamp will benefit clients mainly at two levels:Large enterprises that need customized solutions to utilize blockchain and accelerate their business.Small and medium-sized enterprises that need to augment their technology with data immutability and proof of existence in their backend.Through MavenStamp API, enterprises can leverage our Immutability-as-a-Service to build their customized applications without having to implement the blockchain technology directly. For example, with the API integration, insurance enterprises would be able to speed up the claim process by having an immutable record of the car damage image, location and time.MavenStamp use case for Insurance IndustryWhat can we expect in the future?With the Immutability-as-a-Service of MavenStamp, we’re entering a new trustless era that enables enterprises to unlock potential opportunities with data integrity.Our Mavennet team will keep working on developing more extended features and a larger scale adoption for MavenStamp. More importantly, we are expanding the potential of blockchain in a wider range of areas.Please follow us and receive more updates!LinkedIn Twitter Contact UsMavenStamp — An Immutability-as-a-Service solution was originally published in Mavennet on Medium, where people are continuing the conversation by highlighting and responding to this story.</content>
  </entry>
</feed>
