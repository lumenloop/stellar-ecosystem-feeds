<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
  <title type="text">Stellarbeatio - Medium</title>
  <link rel="alternate" type="text/html" href="https://medium.com/stellarbeatio?source=rss----a7a2df9c6160---4"/>
  <link rel="self" type="application/atom+xml" href="http://10.0.0.124:3044/?action=display&amp;bridge=FeedFinderBridge&amp;url=https%3A%2F%2Fmedium.com%2Fstellarbeatio&amp;strip=on&amp;_cache_timeout=3600&amp;format=Atom"/>
  <icon>https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png</icon>
  <logo>https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png</logo>
  <id>http://10.0.0.124:3044/?action=display&amp;bridge=FeedFinderBridge&amp;url=https%3A%2F%2Fmedium.com%2Fstellarbeatio&amp;strip=on&amp;_cache_timeout=3600&amp;format=Atom</id>
  <updated>2025-04-11T22:10:23+00:00</updated>
  <author>
    <name>RSS-Bridge</name>
  </author>
  <entry>
    <title type="html">Federated Voting Part 1: Theory. Learn about a Stellar Consensus Protocol (SCP) building block.</title>
    <published>2025-04-01T09:04:37+00:00</published>
    <updated>2025-04-01T09:04:37+00:00</updated>
    <id>https://medium.com/stellarbeatio/federated-voting-part-1-theory-learn-about-a-stellar-consensus-protocol-scp-building-block-e050a8d0e16e?source=rss----a7a2df9c6160---4</id>
    <link rel="alternate" type="text/html" href="https://medium.com/stellarbeatio/federated-voting-part-1-theory-learn-about-a-stellar-consensus-protocol-scp-building-block-e050a8d0e16e?source=rss----a7a2df9c6160---4"/>
    <author>
      <name>Pieterjan</name>
    </author>
    <content type="html">In previous blog-posts we learned about Federated Byzantine Agreement Systems (FBAS) and their components. We dove deep into consensus topology concepts like quorum slices and quorums and what their properties should be to enable safety (quorum intersection) and liveness (quorum availability) in the system. We also looked at byzantine behavior of nodes, how this affects the safety and liveness guarantees, through intactness and dispensable sets (DSET).Now we will take the next step and discuss how an FBAS actually reaches agreement trough Federated Voting and sending messages. Federated voting is a key building block of the Stellar Consensus Protocol (SCP) to guarantee safety and to get the nodes in an FBAS to agree on something. We will show you when federated voting works, and when it gets stuck.Federated Voting in actionNote: The most important blog-posts to be able to follow along are about decentralized quorums and, to a a lesser extent, intactness.This blog-post is divided in three parts. First we will go over the federated voting protocol theory. In part two we will introduce the Federated Voting Simulator, a web app where you can play around with the protocol to experience how it works. In part 3 we will cover the available scenarios in the simulator, to delve deep into some of the details of the protocol.If you want to dive deeper into federated voting theory, you can consult the SCP whitepaper.If you want to skip the theory, and just see the protocol in action, go directly here and press play!The Federated Voting ProtocolWe will now go over the actual protocol and explain how it can get an FBAS to agree on a value, and how it does this in an optimal safe way.Examples showcasing the protocol and some of its intricacies are saved for the next part, using the simulator.1. Two-Round VotingIn Federated voting, every node uses two rounds of voting to reach agreement:Vote(statement): Nodes initially vote for a (any) statement they deem valid and does not contradict a previous accepted statement. In order for other nodes to rely on these votes, nodes are not allowed to change their vote (!). After a successful round, nodes can accept a statement.Vote(accept(statement)): If a statement is accepted, nodes hold a second vote to confirm the statement. This ensures stronger safety and liveness guarantees.2. Three-Phase AgreementWhile receiving messages/votes from other nodes, each node sees the voted upon statements progressing through three phases:Unknown: Initially, a statement’s status is unknown. The statement could wind up agreed upon, discarded, or could cause the vote to get stuck.Accepted: After the first successful vote, a node accepts a statement, knowing no intact node will accept contradictory statements.Confirmed: After the second successful vote, a node confirms the statement, having greater security guarantees and knowing that all intact nodes will eventually confirm the statement. It is now safe to act upon that statement.Note: An intact node is a node that is well-behaved and not tainted by the behavior of other ill-behaved nodes.Possible phases of a statement A at a specific node and the safety and liveness guarantees3. Accepting a StatementNodes start by voting for any statement they deem valid. What is valid depends upon the context, but it also means that they never accepted a statement in the past that would contradict their current vote. They can also never change their vote!Together with the statement they also send their quorum slice configuration. This allows the nodes in the system to calculate what the quorums are in the FBAS.When a node sees a quorum vote for or accept that statement, and the node is a part of that quorum, they can accept the statement. This is called the ‘ratification’ of a vote for a statement. I.e. The quorum agreed to accept the statement.Note: a quorum voting or accepting a statement means that is possible that part of the nodes in the quorum have voted for the statement, and another part have already accepted the statement.When a quorum of nodes accepted a statement, we have the guarantee that no intact node will accept a different statement when there is “quorum intersection of well behaved nodes”.Note: If you don’t understand why the above is true, read this blog-post about decentralized qourums.But what about intact nodes that voted for another statement? How do we get system wide consensus of intact nodes when these nodes are not allowed to change their vote?To get all the intact nodes, that voted differently, to accept the statement that the quorum accepted, a second mechanism is introduced: Acceptance by a v-blocking set.3.1. V-blocking setA V-blocking set is a set of nodes that has the power to block consensus for a node. Meaning that this set overlaps with every quorum slice of that node. And that one of the v-blocking nodes is present in every quorum that the node is present in. We will demonstrate v-blocking sets in the “successful agreement scenario”If a node observes one of its v-blocking sets accept a statement, it can safely ‘change its mind’ and also accept that statement.Path to acceptance for Intact Node3.2. Why is accepting another statement allowed?This is where intactness comes into play. A node is intact if it is well-behaved and not befouled by too many ill-behaved nodes. This means that there has to exist a quorum that contains only well-behaved nodes. Otherwise the node would never be able to reliably accept a vote! If there is a quorum of well-behaved nodes, you cannot have a v-blocking set made up entirely of ill-behaved nodes. Think about the opposite, if there is a v-blocking set made up entirely of ill-behaved nodes: then by definition, a node in this set would be present in every quorum, and thus every quorum would thus at least contain 1 ill-behaved node. Which contradicts the fact that we had a quorum entirely made up of well-behaved nodes.Thus an intact node, has at least one intact node in every v-blocking set. This means that the intact node in the v-blocking set is telling the truth, and by extension the entire v-blocking set! The v-blocking set observed a quorum vote for this statement, thus the statement is a valid choice. Our intact node can now safely accept this statement.4. Accepting a statement is not enoughAccepting a statement is however not enough to act upon that statement.Because of the v-blocking set mechanism, safety is not optimal for befouled nodes, that nonetheless enjoy quorum intersection of well-behaved nodes. Such a befouled (non-intact) node could be fooled by a v-blocking set, which is at odds with the FBAS safety guarantees: Safety is guaranteed when there is quorum intersection of well behaved nodes. We will demonstrate this in the scenario ‘accepting is not enough: safety’.Also because an intact node accepted a statement, it doesn’t mean that every intact node can. There is a scenario where an intact node crashes in the middle of the vote. Where it helped some nodes in one quorum to accept a value, but then crashes before it could help other nodes to accept. Other intact nodes that have not yet accepted the statement still have quorums available (otherwise they would be befouled), but it could be that the nodes in these other quorums voted for different statements and these quorums will never ratify.If this sounds too abstract, don’t worry, we will visualize this in the ‘accepting is not enough: liveness’ scenario in the simulation tool.5. Confirming a statementIn the second and final voting round, nodes send votes to accept a statement. When a node sees a quorum, that it is a part of, vote to accept a statement it can move that statement to the confirmed phase.Path to confirmation for Intact NodeBecause of quorum ratification and quorum intersection, our intact nodes have safety.But why is the liveness issue solved we had in the accepting phase? The trick is that all intact nodes will now vote to accept the same statement. In the accepting phase the remaining quorums could have different votes within them, impeding ratification, which is now no longer the case. If there is a quorum (and this is the case because the node is intact), we have liveness. We will demonstrate this in the ‘confirming is enough: liveness’ scenario.And now, because we have both safety and liveness, the node can act upon the confirmed statement.5. Stuck voteFederated voting gives you a mechanism that has safety built into it. However it falls short on liveness. In order for a node to accept a statement, there needs to be a quorum that unanimously voted for the same statement to begin with. If there are too many different statements being voted on, it is possible that such a quorum simply does not exist and the vote will get stuck, with no chances of recovery. This is demonstrated in the “stuck vote” scenario.6. Relation to SCPThe Stellar Consensus Protocol uses multiple rounds of federated voting on carefully crafted statements to provide a protocol that can provide safety and liveness.Nodes start out with many possible transaction candidates for a ledger slot, and SCP narrows the candidates down step by step. Through a NOMINATE, PREPARE and CONFIRM phase, each utilizing possibly many rounds of federated voting, a final set is agreed upon by all the nodes, in a timely manner.The inner workings of SCP are out of scope for this blog-post.7. Trust connections vs Network connections (overlay)The quorum slices of the nodes create a network of trust in which to reach agreement. But to actually reach agreement we need a network, an overlay of connections between nodes through which we can exchange messages. These can be completely different then the trust connections. In the real world these connections are messy. Not all nodes are directly connected, connections can stop working, there could be partitions,…The SCP whitepaper assumes that this network (eventually) delivers all messages between well-behaved nodes. To achieve this in a real-world system you could for example implement message gossiping, allow nodes to fetch the consensus history from other nodes,... The Stellar core overlay code is a great way to explore this.Read on to learn about the Federated Voting simulation tool, or jump straight into the scenarios showcasing federated voting.Federated Voting Part 1: Theory. Learn about a Stellar Consensus Protocol (SCP) building block. was originally published in Stellarbeatio on Medium, where people are continuing the conversation by highlighting and responding to this story.</content>
  </entry>
  <entry>
    <title type="html">Federated Voting Part 2: Simulation tool. A Stellar Consensus Protocol Building block.</title>
    <published>2025-04-01T08:56:56+00:00</published>
    <updated>2025-04-01T08:56:56+00:00</updated>
    <id>https://medium.com/stellarbeatio/federated-voting-part-2-simulation-tool-a-stellar-consensus-protocol-building-block-07706b37b9db?source=rss----a7a2df9c6160---4</id>
    <link rel="alternate" type="text/html" href="https://medium.com/stellarbeatio/federated-voting-part-2-simulation-tool-a-stellar-consensus-protocol-building-block-07706b37b9db?source=rss----a7a2df9c6160---4"/>
    <author>
      <name>Pieterjan</name>
    </author>
    <content type="html">Federated Voting Part 2: Simulation tool. Learn about a Stellar Consensus Protocol (SCP) building block.To help you grasp the theory behind Federated Voting, handled in part 1, you can explore the Federated Voting Simulation tool. In this blog-post we will give a rundown on how the simulation works and what the capabilities are.Because every component in the simulation tool has an info button, you can try and learn on the go and skip directly to part 3, where we will discuss the available scenarios. Every scenario showcases an interesting part of the Federated Voting Protocol.Simulation overviewThe simulation tool allows you to explore the behavior of the Federated Voting protocol under different conditions. You can watch different voting scenarios play out, and inspect different widgets showing you why the protocol acts the way it does. You can also create your own scenario by casting votes, disturbing the protocol, modifying the trust configurations of any node and manipulating the network overlay (the actual network connections between nodes).Network OverlayThe federated voting protocol assumes a network that will eventually deliver all messages between well-behaved nodes. By default, the simulation tool provides direct connections between every node. If a message is sent, it is delivered. However a node can act byzantine and ignore a message delivery. This allows the simulation tool to stay close to the SCP whitepaper, and avoid extra complexity. You can however disable the default behavior, define your own overlay and enable message gossiping. Read on to find out how to do this with the overlay widget.ScenariosThe simulation provides interesting scenario’s to explore. If you modify the simulation at any point, the scenario is abandoned and you actually start a new scenario that you can ‘replay’. You can export this scenario to share with others or to reuse later.Select ScenariosSome settings are provided to enable network overlay editing and/or message gossiping.Simulation StepsThe simulation is divided into simulation steps. At every step some actions are executed that advance the protocol, which in turn can generate new actions for the next step.Play or step through a scenarioAt the top of the page you have the “Simulation controls”. Press play to run the current scenario, or go forward and backward manually through the steps.Tip: “n” and “shift-n” are keyboard shortcuts for previous and next step.Simulation Step: ActionsThere are two kind of Actions that drive each simulation step: Protocol Actions and User Actions.Protocol Actions are generated by the protocol, while User Actions are generated by the user. User Actions can for example be an update of the network overlay or a change in the trust configuration of a node. Examples of protocol actions are a broadcast of a vote or gossiping of a vote to neighbors.When you play a step, the actions are executed in the protocol. This generates “events”, things that happened, and the actions that will be executed in the next step.You can view all of these in the “Events and Actions” widget.Events &amp;amp; ActionsYou have the ability to disrupt the protocol actions, for example tamper with a broadcast, to see how the protocol reacts to such disruptions. And you can even create forge fake votes for a node (red envelope button at the right bottom).Forge a messageSystem view and Node viewSystem view: By default you are shown the system view in the dashboard. This means you see the states of all the nodes in the network. For example you can directly observe which nodes transitioned to the accept phase.Node view: You can also inspect the state of a single, selected, node. States of other nodes will then depend on the messages that the selected node have received! For example if another node has transitioned to the accept phase, but the selected node has not received a message indicating this, it will not show this node as accepted.Current viewVisualizing Node StatusThroughout the Simulator, background and border colors are used to indicate the status of a node.Node statusA gray fill color indicates the initial/unknown state. You can also see if the node voted for a specific statement.A blue fill color indicates that the node has accepted the statement printed inside.A green fill color indicates that the node has confirmed the printed statement.A red border indicates that the node is ill-behaved (Crashed, forging messages,…)An orange border indicates that the node is befouled (Impacted by too many ill-behaved nodes)Simulation assumptions &amp;amp; simplificationsTo make the simulation educational and easier to understand and federated voting easier to grok:No Cryptography: In real networks, message authentication is handled by cryptography to prevent forgery. A node can however forge its own messages.Single Slot Consensus: The simulation focuses on a single consensus decision (“what to eat for lunch”)Honest Quorum Sets: Nodes cannot lie about their quorum sets in the simulation.Simplified Trust Structure: Real networks may use more complex nested quorum structures. You have 10 nodes and an array of trusted nodes per node with a threshold at your disposal to create interesting quorum slices. Nested quorum sets are not available.All statements (options for lunch) are valid for every node.By default every node pair has a direct network connection to send messages back and forth. Every sent message is delivered.Federated Voting WidgetsVote selectorThis is the first component you are greeted with. Here you can see and set the vote for each node. Once a vote has been cast in the next simulation step, you can not modify, as a node is not allowed to change its vote. You can however act byzantine, in the ‘events and actions’ widget.Select the votes the nodes will castTrust graphThe trust graph gives you a graph representation of the trust connections of a node, the trust thresholds and their state. In the example below, you can see that a quorum slice of Bob needs two out his three outgoing arrows.Trust GraphThe trust graph also visualizes the actual messages that are sent as you progress through the simulation steps.In the header you have badges that indicate if the FBAS has quorum intersection of well-behaved nodes, if there is a loss of quorum availability, and if the vote is stuck or caused a network split.BadgesProcessed VotesVotes are at the heart of federated voting and drive consensus. Here you can inspect the statements and the votes that are processed by the nodes.Processed Votes for all nodesThe node badges show you the current state of the node.Processed votes by Bob with eventsIf you select a node, you will see the votes for a specific node and the events that caused the node to accept or confirm a statement. In the above example you can see that Bob processed its own vote and that of Alice for “Pizza”, and that they formed a quorum.Consensus TopologyIn the system view, the consensus topology widget shows you the quorums and their quorum intersections in the FBAS.The badges in the header indicate quorum intersection of well behaved nodes and will show if a warning if quorum availability of well behaved nodes is lacking.Consensus topology: system viewQuorums are given the minimal badge if they contain no smaller quorums. These are the most important quorums. Top Tier nodes are the nodes that make up the minimal quorums.Consensus topology: node viewWhen you select a node you also get the Quorum Slices and V-Blocking sets tabs. In the example you can see that Bob is ill-behaved and Alice and Chad are befouled. The FBAS is also lacking quorum intersection of well-behaved nodes.IntactnessHere you can view information on dispensable sets (DSET) and intact nodes. Dispensable sets can be removed from the FBAS without losing safety and liveness. A node is intact if there exists a DSET that contains all ill-behaved nodes, but not the intact node. A befouled node is not intactThe system view shows the DSETs for the FBAS. Selecting a node shows the ones relevant for the node.IntactnessIn our example, you can see that Steve is ill-behaved and has befouled Daisy. There is no DSET containing Steve, that does not contain Daisy. This implies there are no liveness/safety guarantees for Daisy. Alice, Bob and Chad are not impacted.Network OverlayHere you can see how the nodes are connected over the network.Network OverlayBy default all nodes are fully connected, meaning there is a connection between every pair of nodes to send messages back and forth.In the scenario settings you enable editing of the overlay. Now you can click on two consecutive nodes to add a connection, and click a connection to remove it.Edit SettingsEnable message gossiping to have peers re-broadcast messages to their neighbors.Enable editing and/or message gossipingNode trust configurationsHere you can see the trust connections for the nodes. Click on a node on the left to disable/enable it, click on a node to the right to (un)trust it. You can also change the trust threshold to determine how many of the trusted nodes are needed in the quorum slices.Node Trust configurationsWhen you click to apply the changes you can preview the impact on quorum slices, quorums, etc. live in the GUI, before they are executed in the next simulation step.Next up: the scenariosThis was a quick rundown of the Simulation tool. Every widget/component has an info box with more information if needed.Read on to explore the available scenarios.Federated Voting Part 2: Simulation tool. A Stellar Consensus Protocol Building block. was originally published in Stellarbeatio on Medium, where people are continuing the conversation by highlighting and responding to this story.</content>
  </entry>
  <entry>
    <title type="html">Federated Voting part 3: Scenarios. Learn about a Stellar Consensus Protocol (SCP) building block.</title>
    <published>2025-04-01T08:55:22+00:00</published>
    <updated>2025-04-01T08:55:22+00:00</updated>
    <id>https://medium.com/stellarbeatio/federated-voting-part-3-scenarios-learn-about-a-stellar-consensus-protocol-scp-building-block-5f185485d772?source=rss----a7a2df9c6160---4</id>
    <link rel="alternate" type="text/html" href="https://medium.com/stellarbeatio/federated-voting-part-3-scenarios-learn-about-a-stellar-consensus-protocol-scp-building-block-5f185485d772?source=rss----a7a2df9c6160---4"/>
    <author>
      <name>Pieterjan</name>
    </author>
    <content type="html">In part 1 we went over the theory of the Federated Voting protocol. In part 2 we gave an overview on the capabilities of the simulation tool. In this last part we will go over the different available scenarios that will help you understand the protocol:Successful agreement.Voting stuck: too many different votes.Voting succeeded despite node crash.Voting stuck: too many node crashes.Voting stuck for befouled node.Voting stuck: network partition.Accepting not enough: liveness.Accepting not enough: liveness with message gossip fix.Confirming is enough: liveness.Accepting not enough: safety.Network split: ill-behaved node forging messages.Network split: partiallyNetwork safe despite ill-behaved node forging messages.Overlay ring topology with message gossiping.1. Successful AgreementplayIn this scenario we have an FBAS with quorum intersection, no ill-behaved nodes and there are enough nodes voting on the same statement to reach consensus. Press the play button to see the process in action.Trust configThe top tier nodes are Bob, Alice and Chad. We can see that Bob and Alice voted for “pizza” and that they seem to have accepted “pizza”.Quorum ratified PizzaWe can confirm that Alice and Bob form a quorum. You can click on the nodes and view their slices to confirm. A quorum contains a slice for every node. Chad has voted for “burger”, an incompatible statement with “pizza”.If we progress the simulation we can see that Chad can switch to accepting “pizza” because it observers on of its v-blocking sets accept “pizza”V-blocking sets of ChadAlice and Bob form a v-blocking set for Chad. They overlap every one of Chad’s slices. When Chad accepts “pizza”, it instantly confirms “pizza” because it has now seen a quorum accepting “pizza” that it is a part of.Execute the next simulation steps to watch the entire FBAS confirm “Pizza”2. Voting stuck: too many different votesplayThis scenario showcases a key property of federated voting. If there is no quorum that votes for the same statement, federated voting will get stuck.Vote varietyQuorumsIf we look at the quorums and the votes everyone cast, we can see there is no quorum with unanimous agreement. The vote is stuck.3. Vote succeeded despite node crashplayThis scenarios shows the resilience of the protocol despite a node crash. Alice crashes and does not broadcast its vote.Alice crashesIf we look at the intactness component, we see that Alice is a dispensable set for the FBAS. Meaning it could act byzantine without impact on other nodes.Alice crash has no impactPlay out the scenario to see how the still available, and intersected, quorums reach consensus.4. Voting stuck: too many crashesplayIn this scenario both Alice and Daisy crash, befouling the entire FBAS.A key point is that we have the “No Quorum Availability” badge. Meaning liveness is impaired. But we still have Quorum Intersection of well-behaved nodes: there are no safety risks!No intact nodes leftThe only dispensable set containing both Alice and Daisy is the entire FBAS. Meaning all nodes are befouled.No Quorum AvailabilityAll quorums contain at least one crashed node. There is no progress possible (liveness).Quorum intersection of well-behaved nodesLuckily there is no intersection between quorums that only contains ill-behaved nodes. Safety is kept.Play out the scenario to see the vote get stuck.5. Voting stuck for befouled nodeplayThis scenarios shows a badly configured edge node that is befouled, why the remaining FBAS successfully agrees on ‘pizza’.Consensus for intact nodesFrank made the mistake of only relying on Alice in its trust configuration.6. Voting stuck: overlay partitionplayFederated voting assumes a network that eventually delivers all messages. This scenario shows what happens when this is violated when there is a network partition. Steve and Daisy are cut off. The vote is stuck (waiting) as the messages are gossiped between the available connections but will not reach their peers on the other side. Do we keep trying to connect to other nodes to send the messages? When a node connects and heals the partition, does it request previous messages that it missed? …Your implementation (for example stellar core) needs to handles this. The simulation does not and the vote is stuck.Network partitionSteve and Daisy do not receive the necessary messages7. Accepting not enough: LivenessplayIn this scenario we will explain why we need a confirm phase in the Federated Voting protocol.4 node FBASWe have a simple setup with 4 nodes, where every node is a DSET. Meaning that every node can fail without affecting the others.Each node needs two of the others to agree in order to form a quorum.potential consensusBefore any vote is broadcasted, we can see there is a potential for consensus. Alice, Bob and Chad all choose “pizza”.Alice however only broadcasts her vote to Bob. Then the node crashes. Thanks to the the FBAS structure, no node is befouled. There exists a DSET (Alice), that contains all ill behaved nodes, but none of the other nodes.Ill behaved nodeBob receives the votes for pizza from Alice and Chad and can accept pizza.View of BobThe other nodes however, are not so lucky. If we look at the Chad node, it was not able to accept because it did not receive the vote from Alice. Chad does have quorum availability of well behaved nodes. The quorum Bob, Chad, Steve is operational, but Steve voted for Burger!What about the v-blocking set, can Chad change its mind through this mechanism?V-blocking sets of ChadBob sends it broadcasts its accept vote, but as you can see in the screenshot. There is no v-blocking set of compatible accept votes possible.You can also see that the vote is stuck now. As it should be. If Bob would act on its accepted vote, it would be the only one in the network. Other intact nodes cannot accept the same vote.There is however safety. No other intact node can accept a different value. All quorums overlap in at least one well behaved node, which means that all nodes would have to accept pizza, because Bob did.All quorums intersect in at least one well behaved node.That’s why we need the confirm phase. Another vote, where intact nodes can only vote on the same accept value. We will show this in scenario 9, confirming is enough for liveness.8. Accepting not enough: liveness with message gossip fix.playHere is a quick scenario where we replay the previous one, but now with gossiping enabled.Bob re-broadcasts the vote he received from Alice, which helps Steve and Chad accept, leading to consensus.Gossip fix9. Confirming is enough: liveness.playThis scenario demonstrates why intact nodes that enjoy quorum intersection and quorum availability will all confirm the same value, when one intact node has confirmed the value. There is liveness for intact nodes.Alice again crashes, but now it crashes in the confirm phase. It only sends its vote to accept pizza to Bob, and not to Steve and Chad.Bob has received enough accept votes for pizza and sees a quorum ratifying it.Bob confirmsNow when we look at Chad after executing some more steps in the Scenario, we can see that Chad also Confirms pizza! If you remember, in “accepting is not enough: liveness” there was a quorum available for Chad with Steve. This quorum however could not complete because Steve voted for burger.Because Steve is intact, and thanks to the property that all intact nodes accept the same value, Steve changed its mind and accepted Pizza.Quorum available of accept(pizza)In the confirm phase, every intact node has to vote on the same accept statement! This means that if there is a quorum available, it has to (eventually) vote to accept the same statement, and thus confirm the same value. The issues with the Accept phase are solved and our protocol has liveness for intact nodes.Steve votes for Burger, but accepts pizza thanks to a v-blocking set10. Accepting not enough: safety.playIn this scenario we will demonstrate why we need a confirm phase to maintain the FBAS safety guarantee when there is quorum intersection of well behaved nodes.We start with an FBAS of 4 nodes, where every node needs all nodes to agree. Two of them vote for pizza, and two of them vote for burger.unanimous consent neededV-blocking sets are needed to get intact nodes to fall in line with the other intact nodes. However, in this scenario, they lower safety.Every node needs unanimous consent, there exists only 1 quorum: Steve, Bob, Chad, Alice. But this means that every node overlaps with this quorum, and thus forms a v-blocking set:v-blocking sets of size 1When every node is well-behaved, this poses no issues. As a node is only convinced by a v-blocking set accepting a vote. And in our scenario the votes of the nodes are too different, there will not be a single node accepting a statement.But what happens when the Chad node ill-behaves and starts forging messages to influence the vote.Ill-behaved node befouls the systemAll nodes are befouled, but if we look at the badges we can see there is quorum intersection of well behaved nodes. Safety should be guaranteed. There is no quorum of well-behaved nodes available, so the vote can get stuck.Chad now forges two accept votes. To Alice he sends vote(accept(pizza)) and to Steve vote(accept(burger)). Because Chad is a v-blocking set for the two nodes, he manages to convince them to accept different statements!Accept vote conflict between Steven and AliceIn the accept phase, safety is guaranteed for intact nodes. However an FBAS requires safety for every well-behaved node, even if they are befouled, that has quorum intersection of well-behaved nodes. That’s why we need a confirm phase. Where there is no v-blocking mechanism, and only complete quorum ratification is allowed, satisfying safety through quorum intersection.If the nodes would act upon their statements, there would be a network split in this scenario.Quorum intersectionIn our example there is only 1 quorum, thus the quorum intersection is equal to that quorum. We can see that the intersection does not exist entirely out of ill-behaved nodes, thus there should be safety.When we continue the scenario, three of the nodes accept burger, but because Steve accepted pizza, the quorum cannot ratify and the vote is stuck. As it should be.Vote stuck, but safe.11. Network split: ill-behaved node forging messages.playIn this scenario we demonstrate that safety is not guaranteed when there is a lack of quorum intersection of well-behaved nodes. There exists an intersection between two quorums that is made up entirely out of ill-behaved nodes.We have an FBAS where every two top tier nodes form a quorum. Two of the nodes, Chad and Bob, vote differently and Alice has the deciding vote.Risky FBAS setupIf we look at the quorums for Chad and Bob, we see that they overlap in Alice. This poses risks if Alice is malicious!Quorums in the FBASAlice sends two forged votes. To Bob it sends a vote for pizza, to Chad a vote for burger.We can see in the quorum intersection tab, that there exists a quorum intersection made up entirely out of ill-behaved nodes (Alice).Quorum Intersection lostChad and Bob accept conflicting values because they both see their quorum vote for the same value:Accept vote conflict.Alice repeats the process, and also forges conflicting accept(votes). The network confirms different values and is split/forked.Chad sees a quorum accept BurgerBob sees a quorum accept PizzaNetwork split/forkThis scenario demonstrates it is important for you quorums to overlap in multiple nodes (from different organizations).12. Network split: partiallyplayThis scenario demonstrates that even if the network splits, for example in the top tier, you can still guard your (edge) node from the split with your individual trust configuration.In our example Steve and Daisy are edge nodes, and require unanimous consent from the top tier nodes in order to continue.Steve and Daisy guard for top tier splitsAlice does the same trickery as in the previous scenario and causes a network split. But Steve and Daisy detect the danger and stay ‘stuck’ in the Accept Phase.Partial network splitThe downside in our example is that Daisy and Steve have worse liveness properties. They need all Nodes in the top tier to agree. Thus if one of them merely crashes, they will not be able to confirm the value, that the top tier confirmed. Safety and liveness are trade-offs!More interesting scenarios could be when there is a second tier the same size as the top tier, that could be trusted by edge nodes alike. If your quorum set config is for example that both tiers need to agree. This would improve safety, because a top tier split could no longer cause your node to split. It would also also keep your liveness properties up to par because the second tier has the same size as the top tier, and thus an equal amount of nodes would need to crash before your node halts.13. Network safe despite ill-behaved node forging messages.linkIn this example we improve the FBAS setup for safety such that every quorum intersects in multiple nodes. No single node can cause a quorum to diverge.Alice is ill-behaved but has no impactAlice tries the same message forging as before, but because of the FBAS robustness, none are impacted.When we look at the intactness component we see that Alice is a DSET and can be safely ignored.Alice is a DSETBecause Alice cannot influence the quorums, the vote is stuck, as it should be.Vote stuck, but safe.Because there is still quorum availability, another round of voting could yield a consensus without the help of Alice.14. Overlay ring topology with message gossipingplayThis scenario demonstrates how the network overlay can differ from the trust structure. The nodes are connected like a ring and gossip messages to their peers to eventually reach consensus.Ring structureConsensus thanks to message gossipingFederated Voting part 3: Scenarios. Learn about a Stellar Consensus Protocol (SCP) building block. was originally published in Stellarbeatio on Medium, where people are continuing the conversation by highlighting and responding to this story.</content>
  </entry>
  <entry>
    <title type="html">Stellar Network: from decentralized quorums to trust clusters</title>
    <published>2024-11-14T14:54:46+00:00</published>
    <updated>2024-11-14T14:54:46+00:00</updated>
    <id>https://medium.com/stellarbeatio/stellar-network-from-decentralized-quorums-to-trust-clusters-ba4eb2dd8e7e?source=rss----a7a2df9c6160---4</id>
    <link rel="alternate" type="text/html" href="https://medium.com/stellarbeatio/stellar-network-from-decentralized-quorums-to-trust-clusters-ba4eb2dd8e7e?source=rss----a7a2df9c6160---4"/>
    <author>
      <name>Pieterjan</name>
    </author>
    <content type="html">In the Stellar Network, nodes form a network of trust. A directed graph where nodes are connected to the nodes that they trust to reach consensus. In this graph we can calculate the trust cluster for each node. The set of nodes that it depends upon exclusively to reach consensus.Trust cluster detection, visualization and analysis is now possible on Stellarbeat.Trust Cluster filteringIn this blog-post we will build up our knowledge by talking about how the Stellar Network works and how decentralized quorums make the decisions. When we have a good intuition, we will gain insights by discussing the trust graph and work our way up to to trust clusters.A demo network was added to Stellarbeat: Trust Cluster demo, that mimics the example network used below to convey the concepts. You can use this to play around and deepen your knowledge.Trust Cluster demo networkStellar Network = FBASThe Stellar Network is a Federated Byzantine Agreement System. To give you an intuition you can think about it as a set of nodes with individual trust configurations. There is no central authority that tells them which nodes they should trust.The goal of this system is that the nodes enjoy safety and liveness. Safety meaning that every node has the same copy of the distributed ledger. And liveness meaning that they have the ability to add new blocks to the ledger.An FBAS runs a consensus protocol, like SCP, to reach agreement on the contents of the ledger. But SCP can only guarantee safety and liveness under specific conditions.If you want to learn more about the FBAS concept. Check out the SCP whitepaper.Decentralized quorums make the decisionsQuorum Slices: individual trust configurationsA node chooses its own quorum slices. These are sets of nodes it trusts to reach consensus. A slice must always contain the owner node (the trustee) and a node can choose multiple slices for redundancy.Quorum SlicesIn this simple example we have four nodes A,B,C,D. Each have defined a single quorum slice. For example node A trusts B and C with slice {A,B,C}. Node D also trusts node B and C with the slice {B,C,D}. Note that node A and D don’t trust each-other directly.Quorums emergeFrom these configurations quorums emerge. A quorum is a set of nodes that contain a slice for each node. And a node only makes a decision about the contents of the ledger if all the nodes in a quorum agree (unanimously). Without quorums we would not make progress on the ledger. And for this blog-post and example we assume that all nodes are well-behaved and functioning correctly. If for example two quorums would overlap in a single malicious node, that node could cause the network to fork. Or if for example every quorum contains a failed node, there would be no progress.QuorumIn our example you can see that the quorum {A,B,C,D} emerges. In the image you can verify that it contains a slice for each node. Note that even though A and D did not trust each-other directly, they are still part of the same quorum due to their transitive trust connections.Quorum intersectionIn an FBAS, multiple quorums are possible. Nodes choose different slices for redundancy, and new participants could enter the system.Because we want system wide agreement, every quorum should intersect every other quorum. Overlapping quorums that each have unanimous agreement will result in a system where each node has the same copy of the ledger.Quorum IntersectionIn our example three new nodes enter the system: E, F and G. They latch onto node D to become part of our FBAS. Our original FBAS system however, doesn’t trust them back.You can see that a second quorum emerges {A,B,C,D,E,F,G}. It overlaps with the quorum {A,B,C,D} which means that our system can reach agreement.In this system we can also see that the quorum {A,B,C,D} does not need nodes E,F,G to reach consensus. But E,F and G do need A,B,C and D! Their only quorum contains all the nodes in the system.Quorum sets generate quorum slicesStellar uses a handy format called quorum sets to let nodes define their slices. You can see an example in the screenshot. Basically a node chooses sets of nodes it trusts and corresponding thresholds of nodes that could form a quorum slice.Quorum SetIn the above example from Stellarbeat you can translate this to: My quorum slices should contain five organizations from the list below. But the quorum sets work recursively, so a node can also define how many nodes of a certain organization it needs in its slices. There is a tool in the sidebar of the node dashboard on Stellarbeat that lets you see the actual slices.Inspect quorum slicesTrust graph: Visualize the Stellar NetworkThe quorum slice configurations of the individual nodes create a network of trust. We can visualize this in a directed graph called the Trust Graph. When a node trusts a node in a quorum slice, a connection is added to the graph.Quorum SlicesThe trust graph in our example already gives us a quicker and better intuition on configuration of the quorum slices.On Stellarbeat you can also view this trust graph. Orange connections show trusted nodes, green connections trusting nodes. You can also shrink the network graph down to the organization level to get a more compact view.Select Node or Organization trust graphOrganization trust graphStrongly connected components: Bidirectional trustStrongly connected components are sets of nodes that trust each-other transitively. Meaning if you would start with a node in such a set, and follow the connections in the trust graph, you would be able to reach every other node in that set.Strongly Connected ComponentsIn our example there are two strongly connected components. {A,B,C,D} and {E,F,G}. Strongly connected components let you quickly gauge bidirectional trust in the network. And if a strongly connected component trusts another strongly connected component, this means that all the nodes from the first have a trust path to all the nodes of the second. This is shown in our example with the orange arrow. If the nodes of the second strongly connected component would also trust the first one, they would actually be part of the same strongly connected component.In Stellarbeat we show you these components by clustering the nodes together and give the cluster a blue border.Strongly Connected ComponentsTrust clusters. Which nodes are depended upon?A trust cluster shows which nodes are depended upon by a specific node. It shows which nodes are solely relevant for consensus for that node. Another way to look at it is that a node is not impacted in any way, liveness- or safety-wise, by nodes outside its trust cluster.For example, if we start with node D and follow all the nodes it trusts (transitively), we get the cluster {A,B,C,D}. Similarly we would get the same trust cluster if we start from A,B or C.If we start with node G, we get the trust cluster {A,B,C,D,E,F,G}Because the strongly connected components also encode the trust connections, we can see that trust clusters are the sum of their strongly connected components.Stellarbeat allows you to filter down the trust graph to the trust cluster of the selected node.Trust Cluster filteringAnd it also allows you to do a safety and liveness analysis on a specific trust cluster.Analyze trust cluster for safety and liveness thresholdsNetwork Transitive Quorum Set: most important trust clusterAs a last concept, Stellarbeat visualizes the network transitive quorum set, the heart of the network.It is a strongly connected component and therefore has bidirectional trust. It is a trust cluster, meaning the nodes inside this cluster only depend upon themselves. And most importantly, _all_ other nodes in the network have (transitive) trust connections towards these nodes. Their liveness depends upon these nodes.Nodes that do not have connections to the network transitive quorum set, do not have quorum intersection and could lose safety.In our example the Network Transitive QuorumSet is {A,B,C,D}. These nodes form a strongly connected component, they form a trust cluster, are trusted by all other nodes (transitively) and do not trust other nodes.On Stellarbeat, the network transitive quorum set is visualized in the heart of the trust graph with a blue background.Network Transitive Quorum SetFeedbackI hope you enjoyed this post and if you have any feedback or spot any errors, let me know at info@stellarbeat.io or on the Stellar Dev Discord.Stellar Network: from decentralized quorums to trust clusters was originally published in Stellarbeatio on Medium, where people are continuing the conversation by highlighting and responding to this story.</content>
  </entry>
  <entry>
    <title type="html">From well-behaved to intact and correct nodes: Road to understanding the Stellar Consensus Protocol</title>
    <published>2024-08-28T08:58:29+00:00</published>
    <updated>2024-08-28T08:58:29+00:00</updated>
    <id>https://medium.com/stellarbeatio/from-well-behaved-to-intact-and-correct-nodes-f253bb0e3a42?source=rss----a7a2df9c6160---4</id>
    <link rel="alternate" type="text/html" href="https://medium.com/stellarbeatio/from-well-behaved-to-intact-and-correct-nodes-f253bb0e3a42?source=rss----a7a2df9c6160---4"/>
    <author>
      <name>Pieterjan</name>
    </author>
    <content type="html">From well-behaved to intact and correct nodes. Road to understanding the Stellar Consensus Protocol.Nodes in the Stellar Network form a federated byzantine agreement system (FBAS) that runs an agreement protocol, the Stellar Consensus Protocol (SCP) to reach agreement on the contents of their ledgers. They want safety: every node has the same copy of the ledger. And they want liveness: their ledgers should be updated in a timely manner to reflect the latest transactions.This blog-post will give you a practical overview of how an FBAS can guarantee safety and liveness for nodes and will zoom in on the impact of a node’s trust dependencies with the concepts of intactness and dispensable sets.Correct node flowchartThe previous FBAS intuition blog-post can be seen as a prerequisite, but we will start with a recap of the necessary concepts like FBAS, quorum slices and quorums. If you want a deeper dive with more examples, you can start there.This content is based on the SCP whitepaper, section 3 and 4. Go there for the source and if you want more math &amp;amp; definitions.FBAS recapThe nodes in the Stellar Network form a federated byzantine agreement system (FBAS). The main difference with other byzantine agreement systems is that the quorums, sets of nodes that can reach consensus, are determined in a decentralized way. The nodes themselves determine these quorums by choosing their quorum slices, sets of nodes that they trust.A node is part of every one of its quorum slices and, roughly speaking, can be convinced of a statement if everyone in the quorum slice agrees. From the combinations of the quorum slices of the nodes in the network, the quorums emerge. A quorum contains for every node it encompasses, one of its quorum slices. If a quorum slice can convince a single node, a quorum can convince every node in the quorum. Another way to look at it is that a statement needs a 100% approval rate from a quorum to be agreed upon. And because nodes choose multiple slices to trust, multiple quorums can emerge in an FBAS.As an intuition you can think of an FBAS as the sum of the quorum slice configurations of its nodes.Quorum slices to quorum and trust graphCorrect nodesThe end goal for a node is to be correct, which means it enjoys both safety and liveness. These concepts are defined as follows in the SCP whitepaper:Safety: A set of nodes in an FBAS enjoy safety if no two of them ever externalize different values for the same slot. Nodes that lack safety could externalize different values and ‘diverge’.Translated to the Stellar network: A slot is a block in the ledger, and externalize means adding a new set of transactions to the ledger. Diverge means forking.Remark the word ‘ever’. This means that diverging should not happen, past present and future.Liveness: A node in an FBAS enjoys liveness if it can externalize new values without the participation of any failed nodes. Nodes that are well behaved, but lack liveness, are called ‘blocked’.Remark the word ‘can’. This means that there is a situation possible where the network could continuously thwart consensus by reordering or delaying critical messages. The nodes are still considered to have liveness, because consensus is still possible. Consult the SCP whitepaper on how this risk is mitigated in practice.To determine if a node is correct, we have to answer a couple of questions, as visualized in the following flow-chart.Correct node flowchartIs the node well-behaved?We first look at the direct/local behavior of the node. We ask the questions:Does the node follow the consensus protocol?Is the node configured with sensible quorum slices/trust relationships?Does the node (eventually) respond to all requests sent to it?…If yes, then we call the node well-behaved.An ill-behaved node on the other hand can display random, byzantine behavior and doesn’t follow the consensus protocol. This could be with malicious intent to try and take advantage of the system, or it could just simply have crashed.The local node behavior is mostly under the control of the node operator. Keep your node up-to-date, provide a sensible quorum slice configuration, ensure good network access, setup monitoring,…Is the node intact?A node doesn’t operate in isolation. It relies on other nodes (its quorum slices) to reach consensus. Even if a node is well-behaved, if it depends on too many ill-behaved nodes, its ability to reach consensus is compromised.Correct node flowchartTo determine if a node is intact we have to look at the FBAS it is a part of and take ill-behaved nodes into account. We start with two FBAS properties that are crucial for safety and liveness: Quorum Intersection and Quorum Availability.No Quorum intersection, no safetyQuorum intersection means that the quorums that are formed in the FBAS overlap in at least one node. Lack of quorum intersection implies that two sets of nodes could decide upon different values and fork the FBAS. Without quorum intersection, safety is impossible to guarantee.As an intuition (, disregarding faulty and malicious nodes,) remind yourself that quorums need a 100% approval rate before they decide anything. If a quorum decides on a value, all of its nodes agree. If it overlaps with another quorum, the overlapping node(s) also agrees with this value. And because the other quorum also needs 100% approval rate this implies that the decided upon value can only be the same as the one of the overlapping node.However, if two quorums would overlap in only one node, that node could send different messages to each quorum and cause them to diverge. The FBAS would be fragile. Ideally, quorums overlap in multiple nodes. An attack would require coordination of these overlapping nodes which makes it that much harder.No Quorum availability, no livenessA quorum needs 100% unanimity, thus a single node failing in a quorum would result in that quorum never reaching consensus. The problem worsens if that failing node is present in every quorum! If this is the case, the FBAS loses liveness. Another way of saying this is that there is no quorum availability despite faulty nodes.There is also a more subtle problem here. If a node (or a set of nodes) is present in every quorum, they would have the power to veto certain transactions, and be able to censor the network.Dispensable setsArmed with these two concepts we can now get insights into the intact nodes of our FBAS. We will use a theoretical model called dispensable sets, that gives insights into which nodes can fail without impacting the FBAS.A dispensable set (DSET) is a set of nodes in an FBAS, that is, well… dispensable. The other nodes in an FBAS can function correctly without (or despite) the behavior or presence of these dispensable nodes. Dispensable sets tell us which nodes can fail. This is important information, we can for example ask the following questions:Does every individual node form a DSET onto themselves? Meaning no single node failure could harm the FBAS.Does every combination of two nodes form a DSET? Meaning are there combinations of two nodes that can fail without harming the rest?What are the weak spots in our FBAS? What nodes are more vulnerable then the others?Dispensable sets can be found by examining the quorum slices and quorums that emerge from them:For safety, the FBAS should keep the quorum intersection property after deleting the nodes part of a potential DSET from the FBAS and every one of its quorum slices. Doing this we actually modify the slices and check quorum intersection for the newly available quorums.As a special case, an FBAS without nodes has quorum intersection.For liveness A DSET cannot hamper quorum availability for the other nodes. An easy way to check this is if the nodes outside the DSET form a quorum. Meaning every node has the possibility to externalize a new value. In contrast to safety, we do not modify the quorums, but merely check if they can still complete.As a special case, an FBAS without nodes has quorum availability.The two special cases imply that the complete set of nodes of an FBAS always form a DSET.Without looking at the behavior of the nodes in an FBAS we can now calculate all the possible DSETS in an FBAS by testing the above two properties to sets of nodes. As a helper there is also theorem 2 from the SCP whitepaper that tells us that the intersection of two DSETS is also a DSET.To understand if nodes (or the entire FBAS) are impacted by ill-behaved peers, we will start with a (fictitious) visual example of dispensable sets.Dispensable sets: a visual exampleImagine that we know what the ill-behaved nodes are in an FBAS and that all the possible DSETS are shown in the diagram below.DSET visual exampleA DSET can be sliced from the FBAS without impacting the other nodes. This means that node G, which forms a DSET, can act maliciously or crash all it wants without damaging the rest. The same goes for the other DSETS. Remember that the entire FBAS is also a DSET.But what we really want to know is if the two ill-behaved nodes {B,C} can be safely ignored by the well-behaving nodes in our FBAS! If we look at the diagram, {B,C} is not a DSET, not a good start. In fact there are two DSETS that contain the ill-behaved nodes (three if you count the entire FBAS), but they also contain other nodes: {A,B,C,D,E} and {A,B,C,F}. This gives us the intuition that other nodes will be impacted by the ill-behaved nodes, but which ones?Intact nodesTo define if a well-behaved node will be affected we introduce the notion of intact nodes: A node v is called ‘intact’, if there exists a DSET containing all the ill-behaved nodes, but does not contain v. Translated, we have found a set of dispensable nodes, that contains all the ill-behaved nodes but not v, that does not harm quorum intersection and quorum availability for v. Node v will remain intact.If we can’t find such a DSET, we call node v befouled. Note that ill-behaved nodes are always called befouled.Applied to our example we can see that D and E are not befouled, because there exists a DSET {A,B,C,F} that contains all the ill-behaved nodes. The same logic exists for node F. Node A is not so lucky, it is present in every DSET containing all the ill-behaved nodes. Also remember that the intersection of two DSETS is also a DSET (it was omitted from the previous diagram to aid in understanding the concept). This means that {A,B,C} is the set of befouled nodes and can have no guarantees for safety nor liveness.DSET visual example with befouled nodesNow that we went over a visual example, let’s do a complete example by analyzing quorum slices.Dispensable set quorum slice analysis + intuitionsAs an example, we will discuss figure 3 of the SCP whitepaper, and analyze it tier by tier. Note that every node is present in its own quorum slices. For the top tier this means that 3/4 means three nodes including itself and that the middle and bottom tier need two upper layer nodes and themselves.Tiered exampleLet’s start by focusing on the top tier. We can do this because they don’t depend on any other nodes (no outgoing trust connections), and could act as an FBAS onto themselves. In fact they don’t even need to know that the other nodes exist to do their job.Top TierThe quorum slices are:Q(1) = {{1,2,3},{1,2,4}, {1,3,4}} Q(2) = {{1,2,3},{1,2,4}, {2,3,4}} Q(3) = {{1,2,3},{1,3,4}, {2,3,4}} Q(4) = {{1,3,4},{1,2,4}, {2,3,4}}The quorums that can convince the top tier are: {1,2,3}, {1,2,4}, {1,3,4}, {2,3,4}. They contain a slice for each of their nodes. For example {1,2} is not a quorum, because neither has a slice in this quorum (they need at least three nodes). {1,2,3} is a quorum, because {1,2,3} is/contains a slice for node 1, 2 and 3.All quorums overlap in at least 2 nodes. The top tier has both safety and liveness!Every single node forms a DSET. Meaning that the top tier can tolerate a single node failure. Let’s take for example the DSET {1}.First we need quorum availability despite node 1. The remaining FBAS {2,3,4} is a quorum which confirms this. All the nodes can externalize new values.To check quorum intersection we delete the node from the FBAS and every one of its quorum slices. The result is:Q(2) = {{2,3}, {2,4}, {2,3,4}} Q(3) = {{2,3}, {3,4}, {2,3,4}} Q(4) = {{3,4}, {2,4}, {2,3,4}} The quorums are: {2,3}, {2,4}, {3,4} and {2,3,4}. The quorums overlap in node 2, 3 or 4 which confirms we have quorum intersection.We confirmed that {1} is a DSET. Because of the symmetry of the quorum slices of the top tier, we can follow the same logic for every top tier node.Can we tolerate two node failures in the top tier? No, for example let’s analyze potential DSET {1,2}.Quorum availability is already a problem. The set or remaining nodes {3,4} is not a quorum, there is no liveness for the remaining nodes. Quorum intersection is also problematic. The quorums slices after deletion are:Q(3) = {{3}, {4}, {3,4}} Q(4) = {{3,4}, {4}, {3,4}}The quorums are: {3}, {4}, {3,4}.These quorums do not overlap and could cause a fork. If you reason that this lack of safety is not a big issue because there is no liveness, think again. Nodes {1,2} could be malicious, appear to be working fine, but say different things to different nodes.For example: to the quorum {1,2,3} in the original FBAS they could communicate to commit transaction A, but to quorum {1,2,4} they could communicate it’s transaction B that should be externalized. If both of these are valid transactions, node 3 and 4 will not protest and externalize the different values and diverge.Because of the symmetry of the quorum slices, the top tier cannot tolerate any two ill-behaved nodes.Now we add the middle tier.Top + middle tierThe middle tier nodes depend upon the top tier, because they need two of their nodes to reach consensus. Let’s check if the dispensable sets of the top tier are also dispensable for the middle tier.The quorum slices of node 5 are:Q(5) = {{5,1,2}, {5,1,3}, {5,1,4}, {5,2,3}, {5,2,4}, {5,3,4}}And take for example DSET {1}. There is quorum availability: {2,3,4,5} is a quorum. And after deletion: Q(5) = {{5,2}, {5,3}, {5,4}, {5,2,3}, {5,2,4}, {5,3,4}} Q(2) = {{2,3}, {2,4}, {2,3,4}}Q(3) = {{2,3}, {3,4}, {2,3,4}}Q(4) = {{3,4}, {2,4}, {2,3,4}}The quorums in the FBAS are: {5,2,3}, {5,3,4}, {5,2,4}, {5,2,3,4} , {2,3}, {2,4}, {3,4} and {2,3,4}. These quorums overlap, and overlap with the quorums of the top tier, thus the FBAS will remain in sync. Again {1} is dispensable.Because of the symmetry of the middle tier, we can make the same analysis for all middle tier nodes.As an exercise, you can check if {1} is a DSET if a middle tier node would require all 4 of the top tiers nodes to agree.Spoiler: if node 1 would be malicious, it would endanger liveness for the middle tier node, but not for the entire FBAS (the top tier would hum along). Safety would not be an issue for any node.We can check if a middle tier node is a DSET in this FBAS of two tiers. Let’s take for example potential DSET {5}. This is the case, no other middle tier node depends on another middle tier node, and thus it cannot impact them.In fact, any combination of bottom tier nodes forms a DSET in this two tiered FBAS, because no other nodes depend upon them. But this changes when we add the bottom tier in the mix, and we introduce new dependencies.3 tiersThe nodes in the bottom tier need two of the middle tier nodes, and thus transitively, two of the top tier nodes to agree, who in turn need three top tiers including themselves.We can follow the same logic as with the middle tier to find that a single top tier or middle tier node is a DSET for the three-tiered FBAS.And because no other nodes depend upon a bottom tier node, a single bottom tier node is also a DSET. This means that our FBAS can tolerate any one node being ill-behaved!Can the two bottom tier nodes fail? Yes because no other nodes depend upon them. The resulting two-tiered FBAS will function correctly.Can our fbas tolerate two middle-tier nodes being ill-behaved? For example {5,6}?Deleting these nodes will cause problems for the bottom tier nodes, for example node 9. If we take a look at its slices:Q(9) = {{9,5,6},{9,5,7},{9,5,8},{9,6,7},{9,6,8}} And after deletion of {5,6}: Q(9) = {{9},{9,7},{9,8},{9,7},{9,8}}The slice {9} will cause problems, because the node can be convinced in isolation. {9} is a quorum and does not overlap with the others. Node 9 is befouled!If node 5 and 6 are ill-behaved, what are all the befouled nodes? Let’s look at the DSETs that contain them:{5,6,9,10}, {5,6,7,9,10}, …, {5,6,7,8,9,10},…,{1,2,3,4,5,6,7,8,9,10}Node 9 and 10 are befouled, because there does not exist a DSET that contains all the ill-behaved nodes, but not nodes 9 and 10.Node 7 for example is not befouled, because there exists a DSET containing all ill-behaved nodes (for example {5,6,9,10}) that does not contain node 7.If {5,6,9,10} is a DSET, and only node 5 is malicious. Does this mean that node 6 is also befouled? No, Because there exists a DSET, namely {5}, that can be safely deleted, that does not contain node 6.Theorem 2 of the SCP whitepaper states that the intersection of two DSETS is also a DSET. Let’s confirm this in our example. {5,6,9,10} and {7,8,9,10} are DSET’s. Their intersection is {9,10}. Is this also a DSET? Yes, they can be sliced from the FBAS, reflecting the fact that no other nodes depend upon the bottom tier, and thus it can be entirely malicious without impacting the well behaved nodes.Is {6,7,8,9,10} a DSET? Yes! the nodes 1 through 5 do not depend upon them and can keep functioning.Is {7,8,9,10} a DSET? Yes! by the same logic as above.In the following diagram you get a visual overview of the DSETs (non-exhaustive for clarity) and the befouled nodes when node 5 and 6 are ill-behaved.Visualization of DSET analysis exampleLet’s conclude with an interesting observation:Even if other layers can tolerate multiple node failures within their layers, they cannot tolerate two top tier nodes failing. The FBAS is limited by the DSETs of the top tier, which is explored with the concept of minimal quorums. It shows that quorums of non-top tier nodes always contain a quorum from the top tier (and thus consisting only out of top tier nodes).FBAS robustnessAs demonstrated you can use dispensable set analysis to get insights into the robustness of the FBAS. This is important because we don’t know beforehand which nodes are ill-behaved. Nodes could start crashing or malicious nodes could hide their intent. This also makes the flowchart theoretical as it only shows you the circumstances in which safety and liveness can be guaranteed, and maybe more importantly, when these guarantees will break down.Stellarbeat uses a different approach to detect FBAS robustness. Separate safety and liveness buffers are determined through blocking and splitting set analysis. (See FBAS blogpost). Dispensable sets show you which sets of nodes can fail, splitting and blocking sets show which nodes can not fail to keep safety and liveness.Is the node intact? recapUsing the concept of dispensable sets, we can determine which well-behaved nodes are intact, given a set of ill-behaved nodes. Now we can answer the question of the flowchart: Is the node intact? Also note that ill-behaved nodes are automatically considered befouled.Correct node flowchartDoes the consensus protocol guarantee safety and liveness for intact nodes?The consensus protocol plays an important part in an FBAS. As stated in the SCP whitepaper: “The goal of the consensus protocol is to guarantee safety and liveness for intact nodes”. Meaning we need a sound consensus protocol, like SCP, to guarantee correctness for nodes. Also an asynchronous network is assumed that eventually delivers messages between well-behaved nodes.If these ingredients are in place, a node is guaranteed to be correct. Otherwise a node is classified as failed. But remember that correctness can be fleeting, a well-behaved node is only guaranteed correct as long as it’s not tainted by too many ill-behaved nodes. A robust FBAS and good monitoring are necessary for a smooth operation.How the Stellar consensus protocol actually works and how it guarantees safety and liveness for intact nodes will be handled in upcoming blog-posts.Node propertiesWe will conclude with an overview of the different properties a node can achieve, their contexts and how they are linked.Node propertiesFeedbackI hope you enjoyed this blog-post!If you have feedback or spot any mistakes. Let me know on the Stellar dev discord or through info@stellarbeat.ioFrom well-behaved to intact and correct nodes: Road to understanding the Stellar Consensus Protocol was originally published in Stellarbeatio on Medium, where people are continuing the conversation by highlighting and responding to this story.</content>
  </entry>
  <entry>
    <title type="html">StellarBeat.io: Lag detection and network crawler deep dive</title>
    <published>2024-04-30T08:12:41+00:00</published>
    <updated>2024-04-30T08:12:41+00:00</updated>
    <id>https://medium.com/stellarbeatio/stellarbeat-io-lag-detection-and-network-crawler-deep-dive-b7301d11a1e9?source=rss----a7a2df9c6160---4</id>
    <link rel="alternate" type="text/html" href="https://medium.com/stellarbeatio/stellarbeat-io-lag-detection-and-network-crawler-deep-dive-b7301d11a1e9?source=rss----a7a2df9c6160---4"/>
    <author>
      <name>Pieterjan</name>
    </author>
    <content type="html">StellarBeat, a tool that provides insight into the Stellar Network, has a new feature to detect validator lag and an improved network crawler component. To understand how lag detection works, we will take a deep dive into the inner workings of the crawler and parts of the Stellar network.What is lag detection?In the Stellar Blockchain, validator nodes work together to finalize each ledger, a process that ensures the network remains accurate and secure. This involves nodes agreeing on the ledger’s contents by exchanging messages until consensus is reached. Once a ledger is closed, nodes broadcast an “externalize” message, signaling that the ledger’s data is final and can be acted upon.Lag detection measures the time gap between the closing of a ledger and when nodes have successfully externalized it. By tracking these delays, lag detection provides insights into the performance of individual nodes. If a node is consistently slow to externalize, it might indicate issues with its connection, hardware, or software. If more and more top tier nodes are lagging, it could affect the overall speed and reliability of the Stellar network.How to consult lag on StellarbeatYou can consult the lag for every validator on the respective dashboard node pages and their organizations. The nodes overview table offers a sortable lag column.Lag in organization viewWhen a node has a lag higher then 2000 ms, a warning is displayed in the relevant dashboards.High lag warningHow does Stellarbeat track validation lag?To understand how Stellarbeat tracks lag we will take a deep dive starting with what the Stellar Network is, explaining the importance of the network transitive quorum-set, how Stellarbeat crawls the network and finally how the most accurate lag measurements are determined.Understanding network crawlingWhat is the network?When we talk about “crawling the network” in the context of the Stellar Blockchain, what exactly are we referring to? Essentially, it involves mapping out the nodes that operate under the same network passphrase and use specific versions of the overlay and protocol. However, the open nature of Stellar means that nodes can freely join the network and each node independently decides which other nodes it trusts and connects with. This freedom could lead to the potential emergence of distinct networks within the overarching Stellar network, each operating with its own version of the ledger state.StellarBeat focuses on the ledger maintained by nodes that are marked as Tier 1 by the Stellar development organization. You can find a list here: Stellar Development Foundation’s Tier 1 Organizations page.“To help with Stellar’s decentralization, the most advanced teams building on Stellar run validators and strive to join the ranks of “Tier 1 organizations.”How is Stellarbeat configured to track the Tier 1 ledgerStellarBeat is configured manually to trust tier 1 nodes and makes this configuration publicly available, both through the API and on the website interface. This ensures users can understand and verify the basis on which StellarBeat monitors and analyzes the network.You can find this configuration on the website in the left sidebar below info. The quorum-set (see screenshot) contains the nodes and organizations it trusts and the thresholds of nodes needed to agree before Stellarbeat ‘agrees’.Part of the Stellarbeat configurationIf, for example, a notable fork would arise, Stellarbeat could choose to also track this ledger as a different “network”:List of networks tracked by StellarbeatThe network transitive quorum-setThe Stellar network can be visualized as a graph of interconnected nodes. By tracing these connections transitively to their end, we can identify a set of nodes that form the network’s transitive quorum set. A set of nodes trusted by all other nodes.Because the Stellar blockchain is a Federated Byzantine Agreement System (FBAS), a halt of these nodes implies a halt for every node in the network. We say they impact the liveness of every node.On the other hand we have the safety properties of the network. Violation of safety could result in double spending, forks,… However, if the nodes in the transitive quorum-set have safety issues, these don’t necessary propagate to the nodes in the rest of the network. The nodes in the network transitive quorum-set form a first tier, but nodes in the network could also choose to trust a second tier, to keep the top tier honest. If a node detects different results from the two tiers, it would halt to not get into a dangerous ledger state that could enable double spending.At the moment there is no second tier in the Stellar public network. Below is an example of a network setup with two tiers.Edge nodes trust second tier consisting of public nodesPublic nodes Trust their own second tier and the top tierWhy does Stellarbeat manually configure its trusted nodes and thresholds?Currently, the network’s transitive quorum set aligns with the identified Tier 1 by SDF and the nodes Stellarbeat chooses to trust. However, this could change if a top tier node was wrongly configured or compromised, promoting non-top-tier nodes to a higher trust level.Similarly, the emergence of a widely trusted second tier could prompt StellarBeat to adjust its configuration. A network would then only be marked as live if both tiers are validating.For the above reasons and for simplification, Stellarbeat chooses its trusted nodes manually.How is the network crawled?The crawling is executed by the Stellarbeat backend as part of a scan that is executed every three minutes. This scan also handles data enrichment, analysis, storage, triggers notifications,…Stellarbeat architectureIn this blog-post we will focus on the crawler component.To crawl the network and determine the validating states of the nodes, we need to know what the nodes are, be able to connect to them and interpret their consensus messages.To do this, the crawler maintains connections to the trusted tier 1 nodes during the entire crawl, and sequentially connects to other peer nodes with a configured maximum of simultaneous connections, as not to overload the crawl server. It stops when no new nodes are found to connect to.How do we connect to a node?Stellarbeat uses the node connector package that connects over TCP to a Stellar Node. The package converts the data stream and delivers StellarMessages to the crawler.How do we detect all the nodes?Nodes send out Peer Messages that contain a list of peer nodes it knows. It sends them out when a connection was initiated and in response to a request with a GET_PEERS message. The crawler harvests these messages and initiates connections to these new nodes, harvesting their peer messages and repeating this process until no new nodes are detected.When is a node marked as validating?When a node sees that a quorum of its trusted nodes has chosen to close new ledger, it commits to these changes and emits an externalize message with the ledger sequence number and the value. The crawler listens for these messages.However, we have to make sure it’s not just sending out old externalize messages, or messages with wrong values, indicating a fork. We have to compare with the latest ledger closed by the set of tier 1 nodes trusted by Stellarbeat (as described above).If Stellarbeat confirms that a ledger is closed, it gives all connected peers a maximum timeout to confirm they are also externalizing the same ledger sequences, before the connection is closed.The simplified algorithm for a non tier 1 node is:1) connect to node2) wait for consensus of tier 1 nodes on ledger close3) wait for node to close ledger (with a timeout of 10 seconds)4) disconnectHow do we confirm that a ledger is closed?The Stellarbeat crawler trusts a set of top tier nodes as described above. When it receives externalize messages with the same value and ledger id from a majority of these nodes, it confirms that this ledger was indeed closed. A majority is reached when the quorum-set trusted by Stellarbeat reaches it’s threshold.To maximize the chances that the crawler actually picks up these externalize messages, there are two strategies used:- The crawler maintains active connections to the tier 1 nodes during the entire crawl.- Rely on relayed messages from top tier nodes: When a node externalizes, it relays the messages it received from its trusted nodes. Because Stellarbeat is configured to trust the nodes equal to, or part of, the network transitive quorum-set, and these nodes are by definition trusted by all the nodes in the network, we will receive all top tier messages through relay.To be able to maintain connections to tier 1 nodes even though the network could be overwhelmed, we requested to the tier 1 nodes to add the Stellarbeat crawler node to their preferred peers. This ensures a higher chance of connection. Because the Stellarbeat crawler is an outsider to the top tier and ‘looking in’ to the state of the network. There is always a chance that the network could be so overwhelmed that Stellarbeat cannot pick up any externalize messages. In this case you can use the official Stellar dashboard as a second resource.What if the network is halted?In a halted network, no new ledgers are closed and no new externalize messages are sent. If the crawler does not pick up new externalize messages for 90 seconds straight, it marks the network as halted.We still connect to the nodes and listen to them for a limited time to pick up their quorum-set an node information and if they are active in the stellar consensus protocol. A node that is active in the Stellar consensus protocol but cannot reach consensus because the nodes it depends upon are marked as blocked.How do we get the most accurate lag timings?Because we are continuously connected to the top tier nodes, and they externalize first, we get a pretty good estimate when the ledger is closed. To measure lag we need externalize messages from the other nodes.We can receive an externalize message of a node either through a direct connection, or relayed through other nodes.A node has a max allowed incoming connections setting, that could prevent the crawler from connecting. In this case we rely on relayed messages, that inherently add the lag of the sender.But there is a (bad) scenario where the node is not trusted by any other node. This means no other node will relay it’s externalize messages, and if we cannot connect directly to the node because it is at max capacity, we cannot determine its validating state, nor measure its lag. The overloaded metric of a node on Stellarbeat shows if a node does not allow anymore incoming connections.The best case is when we are able to connect to the node directly to pick up its externalize messages. But even then we have to make sure we are connected to the node at the time consensus happens to mitigate the following scenario:1) consensus happens2) node closes ledger and sends externalize message3) crawler connects to the node4) node re-sends externalize message to the crawler5) crawler measures lag based on this ‘late’ externalize messageIn this scenario we can confirm the node is validating, but the lag measurement would be too high.Finally, for all this to work with low latency, we have to make sure the crawler server is not overwhelmed by the amount of messages sent by the nodes. Therefore we have to choose a healthy maximum of simultaneous connections for the crawler itself and keep an eye on the server metrics.Aside: how long does it take to crawl the network?At the moment there are around 110 nodes that are either validating or helping the network by relaying validating messages. In a single crawl around 1000 peer IP addresses are advertised to the crawler and checked.Below are some scenarios for the current Stellarbeat server that handles 40 simultaneous connections, next to the 23 top tier connections. We ignore the connections to nodes that don’t respond (max timeout of 1.5 seconds)Normal network operation: ledgers are closed every 6 seconds. We can crawl every live node in two rounds (110 nodes minus 23 top tier nodes divided by 40 simultaneous connections). 1) Connect to first batch of 40 nodes2) Wait for consensus: 6 seconds3) Wait for timeouts of connected nodes and disconnect: 10 seconds4) Connect to remaining nodes5) Wait for consensus: 6 seconds6) Wait for timeouts of connected nodes and disconnect: 10 secondsFor a total running time of 32 seconds.Slow network: This is the same algorithm as before with higher consensus wait time. For example a 20 seconds consensus time has a total running time of 60 seconds. The worst case of 89 seconds network closing time would result in a crawl time of 198 seconds.Network halted:1) connect to first batch of 40 nodes2) Consensus timeout: 90 seconds3) Wait for timeouts of connected nodes and disconnect: 10 seconds4) Connect to remaining nodes and start a disconnect timeout: 10 secondsFor a total running time of 110 secondsConclusionI hope you found this interesting and if you have any feedback or questions, you can reach me on the Stellar discord (Pieterjan84) or by email info@stellarbeat.ioStellarBeat.io: Lag detection and network crawler deep dive was originally published in Stellarbeatio on Medium, where people are continuing the conversation by highlighting and responding to this story.</content>
  </entry>
  <entry>
    <title type="html">Stellarbeat: Full verification of Stellar History Archives</title>
    <published>2022-12-15T11:26:55+00:00</published>
    <updated>2022-12-15T11:26:55+00:00</updated>
    <id>https://medium.com/stellarbeatio/stellarbeat-io-full-verification-of-stellar-history-archives-bd3ca182c6fe?source=rss----a7a2df9c6160---4</id>
    <link rel="alternate" type="text/html" href="https://medium.com/stellarbeatio/stellarbeat-io-full-verification-of-stellar-history-archives-bd3ca182c6fe?source=rss----a7a2df9c6160---4"/>
    <author>
      <name>Pieterjan</name>
    </author>
    <content type="html">Stellarbeat.io has been scanning Stellar history archives for a while now to locate missing files. Starting now, Stellarbeat will also do a full verification of the archives to check if the files contain the correct data and have not been tampered with.This blog-post will describe how Stellarbeat handles the full verification of the archives and how you can check the results. If you want learn what it takes to verify a history archive and how it works internally, you can read this new, separate, blog-post.Scan ResultsOn the Stellarbeat homepage warnings are displayed next to the node or organization that has a detected verification issue.Verification issue warningYou can find the verification issue details by navigating to the node dashboard page. An archive user can use the Stellar Archivist tool to repair using the — verify flag. Keep in mind that the Stellarbeat scanner stops scanning when it detects an issue, so use the displayed ledger as a starting point. Also, don’t forget to purge your proxy cache after repairing if you configured one.Verification error detailsIf you want to be notified automatically by email about archive issues, you can use the Stellarbeat Notify functionality.Get notified by emailYou can also use the Stellarbeat REST API to write your own integration.REST APIScan schedulingFull verification of archives can be very resource intensive, as every change that occurs to the ledger (such as payments and offers) at any point in time is stored. The scanner has to download all the files and compute they are correct. For example, an archive that is stored on Amazon S3 can take about a day to fully verify using the current implementation.To balance the (bandwidth) costs, resources, and time required for scans, the scanner prioritizes verifying newly written ledgers, and archives with previously detected errors. Because older ledger entries tend to remain unchanged over time, continuously scanning them would be inefficient. However, to guard against tampering and possible archive migration issues, the scanner periodically performs full re-scans of all archives.To achieve this, the scanner performs its verification process in rounds. In every round, the scanner picks up at the ledger it left off and scans all newly written ledgers, or ledgers with previously detected errors, for every archive. Additionally, the scanner selects one archive to undergo a full re-verification, to ensure that it has not been tampered with or migrated.Slow archivesBefore starting a scan of an archive, benchmarks are run to determine the optimal concurrency for downloading its files. The current maximum is 50 simultaneous connections.In some cases, an archive may have limited throughput or bandwidth, which can make it slow to verify, hog up scanner resources and delay scans of other archives. To mitigate this, the scanner will only verify the latest ledgers in a slow archive. Stellarbeat will display a notice on the node dashboard to inform if this is the case. An archive owner could fix this by moving the archive to a cloud storage provider like Amazon s3 or Google cloud storage.Let me know if you have feedback!Stellarbeat: Full verification of Stellar History Archives was originally published in Stellarbeatio on Medium, where people are continuing the conversation by highlighting and responding to this story.</content>
  </entry>
  <entry>
    <title type="html">Stellar History Archives Verification</title>
    <published>2022-12-15T09:22:00+00:00</published>
    <updated>2022-12-15T09:22:00+00:00</updated>
    <id>https://medium.com/stellarbeatio/stellar-history-archives-verification-9775a77241a2?source=rss----a7a2df9c6160---4</id>
    <link rel="alternate" type="text/html" href="https://medium.com/stellarbeatio/stellar-history-archives-verification-9775a77241a2?source=rss----a7a2df9c6160---4"/>
    <author>
      <name>Pieterjan</name>
    </author>
    <content type="html">The history archives of the Stellar blockchain are a crucial component that enables validator nodes to catch up to the latest ledger and provides other applications with access to historical data. In this blog post, we will take a deep dive into the history archives of the Stellar network and learn how to verify they are correct and not tampered with. Through inspecting the first ledgers of the live Stellar public network, we will develop a stronger intuition of how the Stellar blockchain and its history archives operate.What are history archives?Stellar validator nodes operate on the latest ledger. To apply a new transaction they need only the state of the latest ledger and the hash of the previous ledger. Past or historical data is stored in history archives. When a node is running behind, it consults the history archives of its peers to catch up.A ledger represents the state of the Stellar network at a point in time. It is shared across all Core nodes in the network and contains the list of accounts and balances, orders on the distributed exchange, and any other persisting data.History archives are simple storage systems that support two basic operations: retrieving files and storing files. Examples of systems that can be used as history archives include FTP servers, Amazon S3, and Google Cloud storage.This architecture allows a stellar validator to be lean and fast and allows for straightforward and cost effective redundancy of historical data. Other applications, for example auditing, are given easy access to all the Stellar blockchain data through the archives.Where to find the history archives?Node operators are required to publish the location of their history archives on their organization’s website in a toml file, according to the Stellar Ecosystem Proposal 20 (SEP 20).Stellarbeat.io harvest this data for you. You can find the location of the history archives for a specific node on the node’s dashboard page.As an example, we will inspect the history archive of sdf1, a validator with a complete history archive run by the Stellar Development Foundation.History archive base url SDF validatorCheckpoints and the history archive state file (HAS)History archives are divided into checkpoints. A single checkpoint contains 64 ledgers worth of data.Checkpoint &amp;amp; HASEvery checkpoint contains a json file that describes the checkpoint, called a history archive state file, HAS for short.A history archive also publishes a root HAS file with info about its latest checkpoint. You can find it at the fixed location ORGANIZATION_DOMAIN/.well-known/stellar-history.json.Let’s take a look at the root HAS for the sdf1 validator (follow the link to get the live results).Sdf1 root HAS fileThe root HAS file provides us with some essential information, including details about the server and network, the ledger number at which the latest checkpoint was taken, and a list of buckets that we will explore in more detail below.Finding the first checkpointYou can find the HAS file for the very first checkpoint in the history directory of the archive:/history/00/00/00/history-0000003f.jsonThe numeric sub directories represent the six most significant digits in the hex representation of the ledger. The filename contains the complete hexadecimal ledger number. The first checkpoint is taken at ledger 63 , which is represented in hexadecimal as 0000003F. It is important to note that the first checkpoint is not taken at ledger 64, as ledger 0 is counted as a non-existent ledger in the Stellar network. You can view the first checkpoint here.Checkpoint at ledger 63Bucket listThe currentBuckets field in the HAS of a checkpoint contains a bucket list data structure. It consists of different temporal levels that point to buckets (files) that store all the state changes to the blockchain from the first ledger to the latest checkpoint ledger.A history archive contains all the buckets that have ever been referenced in any checkpoint, allowing to efficiently access and retrieve past state changes to the blockchain.Bucket listThe bucket list data structure has two key properties.First, the largest levels in the bucketList contain the oldest state changes and will change the less frequent.Levels increase in size. When new changes come into a level, they are added to its buckets. However when the level is ‘full’, the oldest half of its changes is pushed to the next level, freeing up space in the original level. Every time the content of a bucket has to change, it is replaced with a new bucket containing the new content. This leaves the old bucket unchanged and detached from the level (and the current bucketList), but still archived in the history archive. The bigger a level, the longer it will take before it becomes full and the longer it takes to trigger a change in the next level.This property allows a validator to quickly catch up when it is running behind. Validators have a SQL database that contains the state of the blockchain at time of the latest ledger. For example the actual balance of an account. But they also have their own copy of the bucket list that contains the state changes applied to get the SQL database in its current state. A validator can compare with the bucket list of an up-to-date history archive. Missing state changes are downloaded and applied to its SQL database.Because the smallest buckets contain the most recent changes, chances are a validator only has to download and apply the smaller ones to get back up-to-date. The more a validator is running behind, the bigger the buckets it has to download.Validator catch-upThe second important property of the bucket list is allowing fast incremental cryptographic hashing of the entire ledger state or bucket list. We can quickly calculate a single hash or digital fingerprint of all the data with which we can assert that different (well-behaving) validators are working on the same data.The hash of the bucket list is the hash of the concatenation of the hashes of every individual bucket. Thanks to the first property, we won’t need to rehash the biggest buckets all the time. When they don’t change, their hashes stay the same and we can reuse them for calculating the bucket list hash.We won’t dive deeper into the bucket list, because that would take us too far away from the context of this blog-post. If you are interested you can take a look here.Bucket list of the first checkpointIn the HAS of the first checkpoint we can see the different levels. Every level contains two buckets: curr and snap (we ignore the next bucket that is used to speed up merging of levels). Only level two has a filled snap bucket with the id bff072…b7c. All the other buckets have a value of zero and are empty. The id is the actual hash of the data inside the bucket.Armed with the hash id of a bucket, we can find its location in the bucket directory at /bucket/bf/f0/72/bucket-bff072...b7c.xdr.gz. The sub-directories mirror the first 6 characters of the hash, and the filename contains the full hash.Files in a checkpoint are either .json files, or zipped XDR byte streams to allow efficient transfer between different kinds of computer systems. Because buckets can get large they are stored as the latter.XDR is a data serialization format where the content is defined by plain text schemas. You can find the Stellar XDR schemas here.Aside: Extracting XDRBelow is a short example on how to process an XDR zip using nodejs. You can use Stellar Laboratory to view the contents or decode it using the Stellar base js library.const xdrZip = ...//download the zipconst xdrByteStream = gunzipSync(xdrZip); //extract zip//the first 4 bytes contain the length of the next xdr message. const lengthBuffer = xdrByteStream.slice(0,4);lengthBuffer[0] &amp;amp;= 0x7f; //clear xdr continuation bit//Read 32 bits unsigned integer, big endian orderconst length = lengthBuffer.readUInt32BE(0);const xdrBuffer = xdrByteStream.slice(4, 4+length);  const xdrString = xdrBuffer.toString(&amp;#39;base64&amp;#39;);Bucket entriesA bucket contains bucket entries that contain the state of an account, offers on the integrated decentralized exchange, trust-lines to assets,...Continuing with our example, the first checkpoint contains a single bucket with two bucket entries.The first bucket entry (follow the link to inspect in Stellar laboratory, or copy paste it manually if the input is empty) contains an entry for account GAAZ…CWN7. At ledger 3, its balance changed to 20 lumens, denoted in stroops.First entryThe second entry contains the changed state of account GALP…ZTB. At ledger 3, its balance is all the other lumens in existence minus 20 and a transaction fee.Second entryWe can already guess what transaction happened at ledger 3, but let’s look deeper into the archive to make sure.Other data categoriesNext to the bucket list we have all the fine-grained data that completely describe what happened to get the ledger into its current state. These categories are stored in the transactions, ledger and results directories. Just like the history category where the HAS is located, the sub-directories and filenames are formed by the hex representation of the ledger number.Other data categoriesNote: There is also an optional scp category, that contains the peer-to-peer messages that were crucial for the node to achieve consensus. We will not cover it here.Transactions categoryStellar supports many different operations to modify the ledger. Transactions are bundles of operations that are submitted to the network.Transactions are backed up into history archives through the TransactionHistoryEntry schema.Let’s take a look at the transactions applied in the first checkpoint: /transactions/00/00/00/transactions-0000003f.xdr.gzThe zip contains an XDR structure for every ledger in the checkpoint that contains transactions.After extraction and decoding we find only one transaction for the third ledger with three operations. You can inspect it here (copy paste the link manually if the input is empty).Hello worldIn the third ledger, a transaction from account GAAZ…CWN7 was processed with the memo text (Base64) ‘Hello World’. The transaction contains three operations. First create an account GALP…MZTB with a starting balance of 20 lumens. Then transfer all the existing lumens, minus the transfer fee and the minimum account balance to it. And as a third step set the weight of its masterkey to zero, effectively shutting down the first account on the Stellar public network.Transaction resultsTransactions that are included in the ledger could have failed when applying them. Because a fee was consumed when executing them, and thus modifying the ledger, they cannot be omitted.Transaction results save you the trouble (and the computing time) of effectively having to execute the transactions yourself and show you the direct results.You can find the results of the transactions in the first checkpoint in the results category /results/00/00/00/results-0000003f.xdr.gzYou can inspect the decoded results here (copy paste the link if the input is empty).Transaction results for first checkpointA fee of 300 stroops was charged and all the operations were a success.The ledger categoryIn the ledger category you can find the ledger headers and their hashes for every ledger in the checkpoint. They contain info like when the ledger was closed, the total amount of coins in circulation, what values the consensus algorithm decided upon. And they also contain the expected hashes for verification of the buckets, transactions and result categories.The hash and previousLedgerHash values are of significant importance because they form the actual ledger (or block-) chain. Because the ledger hash is computed on the header value and contains the previousLedgerHash itself, validators know they are working on the same history just by looking at the hash of the most recent ledger.Let’s take a look at the history ledger header for the third ledger (copy paste the link if the input is empty).Verifying the history archive dataNow that we know all the pieces of the puzzle, we can go ahead and actually verify that the data in our history archive is correct. Let’s start with the bucket list.To verify that the data in an individual bucket is correct, we take a sha256 hash of the bucket. The id in the HAS file we used to download the bucket should match this calculated hash.To verify if the bucket list is correct and contains only the right buckets, we concatenate all the individual bucket hashes and take a hash of that. The Ledger Header contains the expected bucketListHash value.For the transactions we need a hash that is identical to the txSetHashValue of the ledger header. We create it by taking the previousLedgerHash value and all the transaction entries in the txes array. However they could be in the wrong order and need to be sorted by their own individual hash values first.To verify the transaction results we hash the txResultSet value and compare it with the txSetResultHash value of the ledger header.To verify if the ledger header is not tampered with, we compute the hash of the header value. The header value contains the transactions, transaction results and bucket list hashes. So if the ledger header hash is correct, we are sure the other data is also correct. To verify if the ledger header hash is correct we compare it with the previousLedgerHash field of the next ledger header.The above steps make sure the chain of ledgers and all other data is correct and not tampered with. But how do we know the last ledger hash is correct? There is no next ledger header.To solve this we have to compare the latest ledger header hash with a trusted source. Maybe you could run your own stellar core or take a look at the ledgers endpoint of a public horizon api you trust. (note: the hash value in the api is hex encoded, the value in stellar laboratory is base64 encoded).Simplified verification overviewEndI hope you found the above interesting. If you want to learn more, you can take a look at the excellent stellar core documentation. If you have any feedback or spot any mistakes, let me know!Sources &amp;amp; linkshttps://github.com/stellar/stellar-core/blob/master/docs/history.mdhttps://www.lumenauts.com/blog/dark-archeology-digging-through-stellar-xdr-archivesstellar core data flow: - https://www.youtube.com/watch?v=axTmLp-F3JA- https://docs.stellarcn.org/stellar-core/software/core-data-flow.pdfStellar archive verification tool: https://github.com/stellar/go/blob/ec5600bd6b2b6900d26988ff670b9ca7992313b8/tools/stellar-archivist/README.mdPublishing your own history archive: https://developers.stellar.org/docs/run-core-node/publishing-history-archivesHistory archive integrations: https://github.com/stellar/stellar-core/blob/master/docs/integration.md#ledger-state-transition-information-transactions-etcStellar History Archives Verification was originally published in Stellarbeatio on Medium, where people are continuing the conversation by highlighting and responding to this story.</content>
  </entry>
  <entry>
    <title type="html">Stellarbeat.io: Scanning Stellar history archives for gaps</title>
    <published>2022-06-10T08:44:12+00:00</published>
    <updated>2022-06-10T08:44:12+00:00</updated>
    <id>https://medium.com/stellarbeatio/stellarbeat-io-scanning-stellar-history-archives-for-gaps-68f6ae08c620?source=rss----a7a2df9c6160---4</id>
    <link rel="alternate" type="text/html" href="https://medium.com/stellarbeatio/stellarbeat-io-scanning-stellar-history-archives-for-gaps-68f6ae08c620?source=rss----a7a2df9c6160---4"/>
    <author>
      <name>Pieterjan</name>
    </author>
    <content type="html">History archives maintain a complete record of the Stellar blockchain, from ledger snapshots down to every individual transaction performed on the network. They allow validators to catch up to the current state of the network, but also serve as an easy integration point for other apps for various use cases like auditing. A full validator maintains a history archive and makes the network more resilient and decentralized.Stellarbeat now periodically scans all history archives for possible missing data. Read on to find out how you can consult the results of the scans and how the scanner operates.Visual Warningshttps://stellarbeat.io shows warnings on different levels (network, organization, node) so you can quickly spot issues.If a gap in a history archive of a node was detected, a warning will appear for the node, its organization and the quorum-sets it is a part of.History gap warning — network level exampleOn the node dashboard you will find extra information to aid in repairing your archive with the handy Stellar Archivist tool.History gap warning — node levelYou get a link to the first detected missing file and the checkpoint you should start repairing at.Email notificationsYou can subscribe to email notifications for a validator you are interested in through the ‘notify’ tab.NotifyWhen a gap is first detected you will be notified. Only when a the detected gap is fixed and a new one is detected will you receive another notification.APIYou can get access to the scan results through a REST API if you want to do some automation.The nodes endpoint has a new historyArchiveGap boolean field that indicates if there was a gap.The new history archive scan results endpoint gives access to the latest scan details.Scan APIYou can find the API documentation here.The scannerThe scanner starts at the beginning of a history archive and works its way up, determining all the necessary files and checking their existence. If a file is missing, the scanner stops and marks the gap. Content and hashes of the files are not (yet) verified.Because different archives have different performance properties, the scanner starts off with a high amount of simultaneous connections and lowers the concurrency when the history archive starts rate limiting or has errors. The duration of a scan can range from one hour to over a day. At the moment every archive is scanned once a week.Note: do not forget to enable caching on your history archive to drive down costs and improve the performance.If you run a history archive, you can identify the scanner through the stellarbeat.io user-agent header in your logs.Where to host the scannerIf you run a private network and want to deploy your own Stellarbeat, you want to be on the lookout for bandwidth costs. In the current implementation, a single scan consumes about 7GiB incoming traffic, and 2.5GiB outgoing traffic. Bandwidth usage will only increase in future versions and with a growing network.Here are some hosting provider notes that could help:Heroku: Stellarbeat has run on Heroku from the beginning without any issues and low operations overhead. However there is a 2TB network bandwidth limit in place for the entire application (=API + Crawler + Scanner + …).Amazon lightsail: Here you get 20TB of traffic per server. However the CPU’s are offered with burst performance capacity. Meaning that you can only have high cpu usage for a limited time, after which you are capped at the baseline CPU performance which, depending on the specific instance, is around 20%.Amazon ec2 General purpose (without CPU burst): Here the stumbling block seems to be the data transfer out. At 0.09 dollar per GB we would pay around 10$ for scanning every history archive in the public network just once.Hetzner: Currently I am running the scanner on Hetzner cloud. It offers 20 TB outgoing network traffic for free and 1.19€ for every extra TB per server. Incoming bandwidth is free. Also there is an option to have dedicated root servers without bandwidth limits.FeedbackIf you have feedback or ideas on how to improve the current setup, be sure to let me know! info@stellarbeat.ioStellarbeat.io: Scanning Stellar history archives for gaps was originally published in Stellarbeatio on Medium, where people are continuing the conversation by highlighting and responding to this story.</content>
  </entry>
  <entry>
    <title type="html">Stellarbeat.io</title>
    <published>2021-06-05T08:32:46+00:00</published>
    <updated>2021-06-05T08:32:46+00:00</updated>
    <id>https://medium.com/stellarbeatio/stellarbeat-io-2c8918aae6de?source=rss----a7a2df9c6160---4</id>
    <link rel="alternate" type="text/html" href="https://medium.com/stellarbeatio/stellarbeat-io-2c8918aae6de?source=rss----a7a2df9c6160---4"/>
    <author>
      <name>Pieterjan</name>
    </author>
    <content type="html">Stellarbeat.io June 2021 release: ISP &amp;amp; country analysis, custom networks, improved network simulation and new API documentationISP &amp;amp; Country liveness and safety analysisSafety and liveness analysis is now available for country locations and ISP’s of the nodes in the network. This is possible thanks to updates to the fbas analyzer tool and it’s wasm export that powers the analysis.A new radar graph shows at a glance the robustness of the network.Liveness and safety in the Stellar public networkThe history charts show the changes and averages over time.Liveness risk history chartNote: we rely on the external service ipstack for the correctness of isp and country information of nodes.Liveness analysis for the Stellar Public NetworkThe liveness graph shows the amount of nodes, organizations,… that need to stop validating before the entire network halts.Liveness buffers Stellar public network at time of releaseAt time of writing, the Stellar public network could halt when a specific set of 6 top tier nodes, 3 top tier organizations, 2 ISP’s or 2 countries fail. We call these blocking sets, as they can ‘block’ network progress.note: if you want to learn more about blocking sets, check out the fbas intuition blogpost.To find out what these sets of ISP’s are, you can click on the ISP node in the radar graph, and run the network analysis.ISP analysisISP liveness riskMicrosoft Corporation and Amazon.com Inc form such a blocking set. If all the nodes hosted on Amazon go down, the network will keep humming along through the Microsoft based nodes. By hovering over an ISP, you can see its top tier nodes.Amazon.com hosted nodesIt is important to interpret the different dimensions (node, organization, isp &amp;amp; country) together. For example if you translate an ISP blocking set to it’s nodes, you will find that it will always contain a node blocking set. No ISP set can halt the network, before all the nodes in a blocking set are failing.The country blocking sets also have a smallest size of two. The current Stellar Public network can handle any single country going dark.Country liveness analysisSafety analysis for the Stellar Public NetworkA network without safety is susceptible to forks and double spends. Nodes can endanger safety when they act maliciously and don’t follow the Stellar consensus protocol.Safety buffers Stellar Public Network at time of releaseAt the time of writing the Stellar public network could lose safety when a specific set of 3 top tier nodes, 3 top tier organizations, 2 ISP’s or 1 country act malicious. We call these splitting sets, because they could split/fork the network.note: if you want to learn more about splitting sets, check out the fbas intuition blogpost.ISP Safety risksLet’s take for example the ISP splitting set that contains the Microsoft Corporation and Amazon.com Inc. In order for this set to endanger the network safety, you would have to believe that these two ISP’s are capable of forcing (or hacking) the different node operators to run malicious software.Country safety risksWhen we look at the country analysis we see that both the United States and Germany form a set of size 1 that could endanger safety. Should we worry? Every node operator in those countries would have to be hacked or forced to run malicious software. And remember to interpret the safety risk dimensions together. The country splitting sets contain at least 3 top tier organizations and nodes. You can hover over the countries to confirm this.Top tier nodes based in GermanyCustom network import/exportStellarbeat has a gui simulation mode with some nice features to help you see the impact of nodes failing, trusting different organizations and nodes,… But making large changes was tedious due to the amount of click work. Also sharing your findings with others was not easy. To solve this, it is now possible to modify, import and export entire networks through a JSON format.Modify the network in jsonWhen you click on the Modify Network tool in the network dashboard, you are greeted with the JSON export of the current network. You can modify the network inline, export or entirely replace it.The data is validated by the JSON schema specification and the schemas are publicly available. You can use them for validation when you want to edit them in your IDE or when you want to generate or modify a network with some custom coding. The schemas are also used for the Stellarbeat API and documentation and are compatible with the FBAS analyzer tool.Modify network toolCustom network generated by FBAS analysis toolImproved network analysis in simulation modeRunning the network analysis, calculating quorum intersection, splitting sets,… is cpu intensive. In fact the running time increases exponentially with the amount of top tier nodes. However when the top tier of the network is symmetric, meaning that every top tier node trusts the same nodes, the analysis can run very quickly.In the past we did not run the network analysis automatically when simulating changes to the network. The ‘wrong’ change could trigger a very long loading screen. The network dashboard did not update the risk messages and statistics, it hid them. You had to run the network analysis tool manually.Thanks to updates to the underlying fbas analyzer tool, we can now run the different network analysis separately, make use of caching and detect when the top tier is symmetric. When the top tier is symmetric or the network contains less than 20 nodes, the analysis is now run automatically on every simulation change and the network dashboard is updated.The Stellar Public network has a symmetric top tier. Now you can easily see the impact of, for example, halting an organization. It will also give you better/faster insights when playing around with the fbas demo networks.Impact of simulating an organization failureNetwork analysis integration in demo networksWhen the top tier is not symmetric and contains more than 20 nodes, you still have to run the analysis manually. However now you can choose the analysis you want to run and limit the running time.Choose your analysisWhen running the analysis for a large non-symmetric network, the browser environment is not ideal. We recommend that you export the network through the ‘modify network’ tool showcased above, and feed it to the offline fbas analyzer tool to inspect the results.API documentationYou can find the new API documentation and playground at https://api.stellarbeat.io/docs/.API documentationAcknowledgmentsA big thanks to Martin Florian and Charmaine Ndolo for all the feedback and FBAS analyzer tool developments.Stellarbeat.io was originally published in Stellarbeatio on Medium, where people are continuing the conversation by highlighting and responding to this story.</content>
  </entry>
</feed>
