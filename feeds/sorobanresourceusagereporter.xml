<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
  <title type="text">Articles - 57Blocks</title>
  <link rel="alternate" type="text/html" href="https://57blocks.io/blog"/>
  <link rel="self" type="application/atom+xml" href="http://10.0.0.124:3044/?action=display&amp;bridge=CSSDateSelectorBridge&amp;home_page=https%3A%2F%2F57blocks.io%2Fblog&amp;url_selector=a&amp;url_pattern=%2Fblog%2F.*&amp;content_selector=.line-numbers&amp;content_cleanup=h1%2Cimg&amp;title_cleanup=&amp;date_selector=&amp;date_format=&amp;author_selector=&amp;limit=3&amp;format=Atom"/>
  <icon>https://github.com/RSS-Bridge/rss-bridge/favicon.ico</icon>
  <logo>https://github.com/RSS-Bridge/rss-bridge/favicon.ico</logo>
  <id>http://10.0.0.124:3044/?action=display&amp;bridge=CSSDateSelectorBridge&amp;home_page=https%3A%2F%2F57blocks.io%2Fblog&amp;url_selector=a&amp;url_pattern=%2Fblog%2F.*&amp;content_selector=.line-numbers&amp;content_cleanup=h1%2Cimg&amp;title_cleanup=&amp;date_selector=&amp;date_format=&amp;author_selector=&amp;limit=3&amp;format=Atom</id>
  <updated>2025-04-11T21:41:17+00:00</updated>
  <author>
    <name>RSS-Bridge</name>
  </author>
  <entry>
    <title type="html">Image Search with AI Models - 57Blocks</title>
    <published>2024-05-21T00:00:00+00:00</published>
    <updated>2024-05-21T00:00:00+00:00</updated>
    <id>https://57blocks.io/blog/image-search-with-ai-models</id>
    <link rel="alternate" type="text/html" href="https://57blocks.io/blog/image-search-with-ai-models"/>
    <content type="html">&lt;div class="line-numbers old-specific-article-style_specificArticle__AvWT7 articleDetail_articleContent__vLTGR"&gt; &lt;h2 id="project-background"&gt;1. Project background&lt;/h2&gt; &lt;p&gt;This project is centered on information and image retrieval technology. Users can upload an image via their mobile device, and the application is tasked with returning matches from the database that are either identical or bear a high resemblance to the uploaded image. Additionally, the app retrieves and displays the related meta information and descriptions for the images already present within the database. The application's algorithmic layer includes advanced techniques like image-based searching, optical character recognition from image texts, text-based retrieval, and conversion from images to text.&lt;/p&gt; &lt;h2 id="algorithm-pipeline"&gt;2. Algorithm pipeline&lt;/h2&gt; &lt;p&gt;Aligned with the project's directives and after a thorough investigation of both the business requirements and the sector's solution landscape, we have devised the following algorithm pipeline:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Upon image upload by the user, a multimodal large language model crafts a comprehensive image description.&lt;/li&gt; &lt;li&gt;Subsequently, an Image Encoder model produces an image embedding, which is used to search for and retrieve matching or similar images from an existing Image Embedding database. The result of this operation is then presented to the user.&lt;/li&gt; &lt;li&gt;The OCR (Optical Character Recognition) model detects textual characters within the image, triggering a Text Embedding model to create text embeddings.&lt;/li&gt; &lt;li&gt;These embeddings are searched within an established Text Embedding database to find images with comparable semantic content.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;/p&gt; &lt;div style="text-align:center"&gt; &lt;p&gt;Fig 1. Algorithm pipline&lt;/p&gt; &lt;/div&gt; &lt;h2 id="algorithm-details"&gt;3. Algorithm details&lt;/h2&gt; &lt;h3 id="image-retrieval"&gt;3.1 Image Retrieval&lt;/h3&gt; &lt;p&gt;Today's image databases, stemming from educational, industrial, medical, and social sectors, have generated the need for robust image retrieval systems. To meet this demand, two principal search methodologies have been developed. &lt;strong&gt;The first is text-based image retrieval,&lt;/strong&gt; which relies on keywords to tag and search images&lt;sup class="footnote-ref"&gt;&lt;a href="https://57blocks.io/blog/image-search-with-ai-models#footnote1"&gt;[1]&lt;/a&gt;&lt;a class="footnote-anchor" id="footnote-ref1" href="https://57blocks.io/blog/image-search-with-ai-models" &gt;&lt;/a&gt;&lt;/sup&gt;. However, this approach faces significant drawbacks: manually tagging extensive databases is impractical, user-generated tags can introduce subjective biases, and these tags are generally language-specific. To address these challenges, &lt;strong&gt;the second method, known as &amp;quot;content-based image retrieval&amp;quot; (CBIR),&lt;/strong&gt; is advocated&lt;sup class="footnote-ref"&gt;&lt;a href="https://57blocks.io/blog/image-search-with-ai-models#footnote2"&gt;[2]&lt;/a&gt;&lt;a class="footnote-anchor" id="footnote-ref2" href="https://57blocks.io/blog/image-search-with-ai-models" &gt;&lt;/a&gt;&lt;/sup&gt;. CBIR circumvents the limitations of text-based retrieval by analyzing the visual content of the images themselves, offering a more automated and objective search process that is not bound by language constraints.&lt;/p&gt; &lt;p&gt;Having recognized the pitfalls of text-based image retrieval, the shift towards CBIR has paved the way for innovative techniques that eschew reliance on linguistic metadata. Instead, CBIR focuses on the intrinsic visual properties of the images. This pivotal transition from text to content aligns with the evolution of feature extraction methods that enable images to be represented in a manner conducive to effective retrieval. Within this context SIFT- and &lt;abbr title="Convolutional Neural Network"&gt;CNN&lt;/abbr&gt;-based models have emerged, reflecting a paradigm shift towards more sophisticated, content-driven search mechanisms. These technologies are at the forefront of CBIR advancement, harnessing the power of visual feature extraction to transform vast image data into succinct, searchable formats.&lt;/p&gt; &lt;p&gt;In image retrieval, both SIFT- and &lt;abbr title="Convolutional Neural Network"&gt;CNN&lt;/abbr&gt;-based (or transformer-based) models follow a general pipeline that converts images into compact feature representations. For SIFT-based approaches, features are extracted using hand-crafted keypoint detectors. Conversely, &lt;abbr title="Convolutional Neural Network"&gt;CNN&lt;/abbr&gt;-based methods leverage densely applied convolutions or patch analysis to discern features.&lt;/p&gt; &lt;p&gt;Regardless of the methodology, an encoding and pooling strategy is implemented to distill these features into condensed vector forms when dealing with smaller codebooks. However, as the codebook size increases to medium or large, SIFT-based models typically rely on an inverted index to maintain efficiency.&lt;/p&gt; &lt;p&gt;In the case of CNNs, features may also be derived through an end-to-end process by using fine-tuned &lt;abbr title="Convolutional Neural Network"&gt;CNN&lt;/abbr&gt; architectures, enhancing the model's adaptability to specific tasks&lt;sup class="footnote-ref"&gt;&lt;a href="https://57blocks.io/blog/image-search-with-ai-models#footnote3"&gt;[3]&lt;/a&gt;&lt;a class="footnote-anchor" id="footnote-ref3" href="https://57blocks.io/blog/image-search-with-ai-models" &gt;&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt; &lt;p&gt;&lt;/p&gt; &lt;div style="text-align:center"&gt; &lt;p&gt;Fig 2. General framework of the CBIR system&lt;sup class="footnote-ref"&gt;&lt;a href="https://57blocks.io/blog/image-search-with-ai-models#footnote1"&gt;[1]&lt;/a&gt;&lt;a class="footnote-anchor" id="footnote-ref1:1" href="https://57blocks.io/blog/image-search-with-ai-models" &gt;&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt; &lt;/div&gt; &lt;p&gt;&lt;/p&gt; &lt;div style="text-align:center"&gt; &lt;p&gt;Fig 3. A general pipeline of SIFT- and &lt;abbr title="Convolutional Neural Network"&gt;CNN&lt;/abbr&gt;-based retrieval models&lt;sup class="footnote-ref"&gt;&lt;a href="https://57blocks.io/blog/image-search-with-ai-models#footnote3"&gt;[3]&lt;/a&gt;&lt;a class="footnote-anchor" id="footnote-ref3:1" href="https://57blocks.io/blog/image-search-with-ai-models" &gt;&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt; &lt;/div&gt; &lt;h4&gt;3.1.1 Image Feature Extraction Model&lt;/h4&gt; &lt;p&gt;In the scope of our project, the ResNet model&lt;sup class="footnote-ref"&gt;&lt;a href="https://57blocks.io/blog/image-search-with-ai-models#footnote4"&gt;[4]&lt;/a&gt;&lt;a class="footnote-anchor" id="footnote-ref4" href="https://57blocks.io/blog/image-search-with-ai-models" &gt;&lt;/a&gt;&lt;/sup&gt;, grounded in &lt;abbr title="Convolutional Neural Network"&gt;CNN&lt;/abbr&gt; architecture, and the CLIP model&lt;sup class="footnote-ref"&gt;&lt;a href="https://57blocks.io/blog/image-search-with-ai-models#footnote5"&gt;[5]&lt;/a&gt;&lt;a class="footnote-anchor" id="footnote-ref5" href="https://57blocks.io/blog/image-search-with-ai-models" &gt;&lt;/a&gt;&lt;/sup&gt;, which is built upon transformer technology, were selected as the fundamental frameworks for feature extraction. We have meticulously performed a series of validation and optimization experiments to enhance and confirm the efficacy of these two robust models.&lt;/p&gt; &lt;p&gt;ResNet is an emblematic model architecture that epitomizes &lt;abbr title="Convolutional Neural Network"&gt;CNN&lt;/abbr&gt;-based deep learning methodologies. In our project's pre-processing phase, preceding the application of ResNet for feature extraction (Relu is a common activation function), we implemented a methodology reminiscent of face recognition.&lt;/p&gt; &lt;p&gt;We generated pairs of identical or closely related samples through extensive data augmentation processes and used Focal loss&lt;sup class="footnote-ref"&gt;&lt;a href="https://57blocks.io/blog/image-search-with-ai-models#footnote6"&gt;[6]&lt;/a&gt;&lt;a class="footnote-anchor" id="footnote-ref6" href="https://57blocks.io/blog/image-search-with-ai-models" &gt;&lt;/a&gt;&lt;/sup&gt; to train a sophisticated multi-class classifier. For the actual feature extraction process, we stripped away the classifier layer of the model and repurposed the remaining structure to capture the feature vectors. Standard practice dictates setting the feature dimensionality to 512 or a higher value to ensure comprehensive feature representation.&lt;/p&gt; &lt;p&gt;&lt;/p&gt; &lt;div style="text-align:center"&gt; &lt;p&gt;Fig 4. Residual learning: a building block&lt;/p&gt; &lt;/div&gt; &lt;p&gt;In 2021, OpenAI heralded a new era in AI with the open-source release of CLIP. This multimodal vision-language model underwent training with an extensive collection of around 400 million image-text pairs. This training regimen significantly enhanced the generalization performance of the Image Encoder. As a result, CLIP's Image Encoder emerged as a powerful tool for generating domain-specific image embeddings. Keen to maximize the feature extraction efficiency in specific areas of interest, our team employed the CLIP model framework to curate custom image-text pairs, allowing us to refine and optimize the image encoder to suit our precise requirements.&lt;/p&gt; &lt;p&gt;&lt;/p&gt; &lt;div style="text-align:center"&gt; &lt;p&gt;Fig 5. CLIP framework&lt;/p&gt; &lt;/div&gt; &lt;h4&gt;3.1.2 Dataset&lt;/h4&gt; &lt;p&gt;In the academic and industrial fields, several open-source image retrieval datasets are designed for different scenarios. Some of these include:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;a href="http://yann.lecun.com/exdb/mnist/" target="_blank"&gt;The MNIST Handwritten Digit Image Database&lt;/a&gt;: This dataset contains 70,000 images, each 28x28 in size, encompassing ten classes of handwritten digits from zero to nine. In image retrieval, using the grayscale pixel values directly as features is common, resulting in a feature dimension of 784.&lt;/li&gt; &lt;li&gt;The &lt;a href="http://www.cs.toronto.edu/~kriz/cifar.html" target="_blank"&gt;CIFAR-10&lt;/a&gt; and &lt;a href="http://www.cs.toronto.edu/~kriz/cifar.html" target="_blank"&gt;CIFAR-100&lt;/a&gt; datasets consist of 60,000 images each, divided into 10 and 100 classes respectively, with a resolution of 32x32 for each image. If CIFAR-100 seems too small, there is a larger option: the Tiny Images Dataset, from which CIFAR-10 and CIFAR-100 are derived. The Tiny Images Dataset comprises 80 million images.&lt;/li&gt; &lt;li&gt;Caltech101 and Caltech256: These datasets contain images across various classes, and as the names suggest, they feature 101 and 256 classes respectively. While they are commonly used for image classification, they are also well-suited for Content-Based Image Retrieval (CBIR). Caltech256, with nearly 30k images, is widely recognized as sufficient for academic publication. However, alternative larger datasets would be needed for industrial applications involving millions of images.&lt;/li&gt; &lt;li&gt;The INRIA Holidays Dataset: This dataset, frequently used in CBIR research, features 1,491 holiday-themed images taken by researchers from the Herve Jegou Institute, including 500 query images (one image per group) and 991 corresponding relevant images. The dataset has 4,455,091 extracted 128-dimensional SIFT descriptors and visual dictionaries derived from Flickr60K.&lt;/li&gt; &lt;li&gt;&lt;a href="http://www.robots.ox.ac.uk/~vgg/data/oxbuildings/" target="_blank"&gt;The Oxford Buildings Dataset&lt;/a&gt; (5k Dataset images): This collection contains 5,062 images and is released by the VGG group at Oxford University. It is often cited in research papers involving vocabulary tree-based retrieval systems.&lt;/li&gt; &lt;li&gt;&lt;a href="http://www.robots.ox.ac.uk/~vgg/data/parisbuildings/" target="_blank"&gt;The Paris Dataset (Oxford Paris)&lt;/a&gt;: The VGG group from Oxford collected 6,412 images of Parisian landmarks from Flickr, including sights like the Eiffel Tower.&lt;/li&gt; &lt;li&gt;The CTurin180 and 201Books DataSets: Telecom Italia made these datasets available for Compact Descriptors for Visual Search. This dataset includes images of 201 book covers captured from multiple angles with a Nokia E7 (6 images for each book, totaling 1.3GB) and video images of 180 buildings in Turin captured with various cameras including the Galaxy S, iPhone 3, Canon A410, and Canon S5 IS (collectively 2.7GB).&lt;/li&gt; &lt;li&gt;&lt;a href="https://purl.stanford.edu/rb470rw0983" target="_blank"&gt;The Stanford Mobile Visual Search Dataset&lt;/a&gt;: Released in February 2011 by Stanford, this dataset contains images from eight categories, such as CD covers and paintings. Each category's images are captured with various cameras, including mobile phones, and there are 500 images across all categories.&lt;/li&gt; &lt;/ol&gt; &lt;h4&gt;3.1.3 Metric&lt;/h4&gt; &lt;p&gt;Image retrieval systems, including Content-Based Image Retrieval (CBIR) systems, are traditionally evaluated using several metrics. Each of these metrics quantifies different aspects of the system's performance, mainly focusing on the relevance of the retrieved images concerning a given query image. The most common evaluation metrics include:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Precision:&lt;/strong&gt; Precision measures the proportion of retrieved images relevant to the query. It is defined as the number of relevant images retrieved divided by the total number retrieved. A higher precision indicates that the retrieval system returned more relevant results.&lt;/p&gt; &lt;p&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Recall:&lt;/strong&gt; Recall quantifies the proportion of relevant images in the database retrieved for the query. It is calculated as the number of retrieved relevant images divided by the database's total number of relevant images. High recall means the system retrieved most of the images relevant to the query.&lt;/p&gt; &lt;p&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;F-Score:&lt;/strong&gt; This metric combines precision and recall into a single number using the harmonic mean, striving for a balance between precision and recall. The F1 Score reaches its best value at one (perfect precision and recall) and worst at zero.&lt;/p&gt; &lt;p&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Mean Average Precision (mAP)&lt;/strong&gt;  is a comprehensive metric that calculates the average precision at each point where recall changes and then takes the mean of these average precisions over all queries in a set. It is beneficial for evaluating systems where the order of retrieved images is significant.&lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;Where:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GTP&lt;/strong&gt; denotes the total number of ground truth positives, that is, the number of truth labels/positive samples;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;n:&lt;/strong&gt; denotes the number of images involved in the retrieval;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;rel@k&lt;/strong&gt; is a schematic function set to one when the first retrieval candidate is a similar sample and zero otherwise.&lt;/li&gt; &lt;/ul&gt; &lt;h3 id="ocr"&gt;3.2 OCR&lt;/h3&gt; &lt;p&gt;Optical Character Recognition (OCR) technology solves the problem of recognizing all kinds of different characters. Both handwritten and printed characters can be recognized and converted into a machine-readable digital data format.&lt;/p&gt; &lt;p&gt;Think of any serial number or code consisting of numbers and letters that you need digitized. OCR can transform these codes into a digital output. The technology uses many different techniques. Put simply, the image taken is processed, the characters extracted, and then recognized.&lt;/p&gt; &lt;p&gt;OCR does not consider the actual nature of the object you want to scan. It simply &amp;quot;takes a look&amp;quot; at the characters you aim to transform into a digital format. For example, if you scan a word it will learn and recognize the letters but not the meaning of the word.&lt;/p&gt; &lt;p&gt;OCR typically consists of two crucial steps: Text Detection and Text Recognition.&lt;/p&gt; &lt;p&gt;&lt;/p&gt; &lt;div style="text-align:center"&gt; &lt;p&gt;Fig 6. OCR process&lt;/p&gt; &lt;/div&gt; &lt;h4&gt;3.2.1 Text Detection&lt;/h4&gt; &lt;p&gt;Text detection is the process of identifying and locating the textual regions within an input image or document. This step involves applying specialized models, such as DBNet, CTPN, and EAST, to efficiently and accurately detect the spatial positions of the text. The output of the text detection step is a set of bounding boxes or text region proposals that encapsulate the textual content.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;DBNet&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;DBNet&lt;sup class="footnote-ref"&gt;&lt;a href="https://57blocks.io/blog/image-search-with-ai-models#footnote7"&gt;[7]&lt;/a&gt;&lt;a class="footnote-anchor" id="footnote-ref7" href="https://57blocks.io/blog/image-search-with-ai-models" &gt;&lt;/a&gt;&lt;/sup&gt; employs a differentiable binarization method, which can train the entire network end-to-end, avoiding the additional post-processing steps required by traditional proposal-based methods.&lt;/p&gt; &lt;p&gt;The model adopts a lightweight encoder-decoder structure, combining convolutional blocks and LSTM modules, which can achieve real-time performance while maintaining high detection accuracy. DBNet also uses multi-scale feature maps for text region prediction, which can better capture the scale variations of the text.&lt;/p&gt; &lt;p&gt;Ultimately, DBNet's output is a binarized text region prediction map, which can be further processed by simple post-processing methods like non-maximum suppression to obtain the final text detection results. This flexible post-processing approach makes DBNet a practical scene text detection solution.&lt;/p&gt; &lt;p&gt;&lt;/p&gt; &lt;div style="text-align:center"&gt; &lt;p&gt;Fig 7. DBNet framework&lt;/p&gt; &lt;/div&gt; &lt;p&gt;&lt;strong&gt;CTPN&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Rather than using the traditional horizontal anchors, CTPN&lt;sup class="footnote-ref"&gt;&lt;a href="https://57blocks.io/blog/image-search-with-ai-models#footnote8"&gt;[8]&lt;/a&gt;&lt;a class="footnote-anchor" id="footnote-ref8" href="https://57blocks.io/blog/image-search-with-ai-models" &gt;&lt;/a&gt;&lt;/sup&gt; employs a set of vertically arranged anchors to better capture the characteristics of text, which often have a long and narrow aspect ratio. Additionally, CTPN introduces a sequential prediction module that combines an &lt;abbr title="Recurrent Neural Network"&gt;RNN&lt;/abbr&gt; with convolutional features. This sequential module can effectively model the inherent sequential property of text, allowing the model to make more accurate text proposals. The output of CTPN is a set of text proposals, which can then be fed into a subsequent text recognition model to obtain the final text transcription results. The flexible architecture of CTPN makes it a powerful and versatile text detection solution, complementing the capabilities of other models like DBNet.&lt;/p&gt; &lt;p&gt;&lt;/p&gt; &lt;div style="text-align:center"&gt; &lt;p&gt;Fig 8. CTPN framework&lt;/p&gt; &lt;/div&gt; &lt;p&gt;&lt;strong&gt;EAST&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;EAST's key innovation&lt;sup class="footnote-ref"&gt;&lt;a href="https://57blocks.io/blog/image-search-with-ai-models#footnote9"&gt;[9]&lt;/a&gt;&lt;a class="footnote-anchor" id="footnote-ref9" href="https://57blocks.io/blog/image-search-with-ai-models" &gt;&lt;/a&gt;&lt;/sup&gt; is its unified detection framework, which combines text region prediction and orientation regression into a single network. This allows the model to simultaneously predict the quadrilateral bounding boxes of text regions and their orientations, eliminating the need for separate post-processing steps.&lt;/p&gt; &lt;p&gt;EAST uses a fully convolutional network architecture, which enables efficient and dense predictions across the entire input image. The model employs a feature fusion module to combine multi-scale features, allowing it to handle text of varying scales and orientations.&lt;/p&gt; &lt;p&gt;Another important aspect of EAST is its pixel-level prediction, which means the model directly outputs pixel-wise scores for text regions rather than relying on text proposal generation. This approach simplifies the detection pipeline and improves overall efficiency.&lt;/p&gt; &lt;p&gt;The output of EAST is a set of quadrilateral bounding boxes representing the detected text regions, and their associated orientation information. This rich output can be directly used for subsequent text recognition tasks, making EAST a powerful and versatile text detection solution.&lt;/p&gt; &lt;p&gt;&lt;/p&gt; &lt;div style="text-align:center"&gt; &lt;p&gt;Fig 9. EAST framework&lt;/p&gt; &lt;/div&gt; &lt;h4&gt;3.2.2 Text Recognition&lt;/h4&gt; &lt;p&gt;The CRNN&lt;sup class="footnote-ref"&gt;&lt;a href="https://57blocks.io/blog/image-search-with-ai-models#footnote10"&gt;[10]&lt;/a&gt;&lt;a class="footnote-anchor" id="footnote-ref10" href="https://57blocks.io/blog/image-search-with-ai-models" &gt;&lt;/a&gt;&lt;/sup&gt; model was the first to propose a three-stage architecture for text recognition, which has since become a commonly used approach. The three-stage architecture of CRNN consists of the following components:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;Feature Extraction:&lt;br&gt; a. The first stage is a &lt;abbr title="Convolutional Neural Network"&gt;CNN&lt;/abbr&gt; that extracts visual features from the input text image.&lt;br&gt; b. The &lt;abbr title="Convolutional Neural Network"&gt;CNN&lt;/abbr&gt; learns to capture the spatial and local patterns within the text, effectively encoding the visual representations.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Sequence Modeling:&lt;br&gt; a. The second stage is an &lt;abbr title="Recurrent Neural Network"&gt;RNN&lt;/abbr&gt;, typically a Bidirectional LSTM (Bi-LSTM) network.&lt;br&gt; b. The &lt;abbr title="Recurrent Neural Network"&gt;RNN&lt;/abbr&gt; takes the &lt;abbr title="Convolutional Neural Network"&gt;CNN&lt;/abbr&gt;-extracted features as input and models the inherent sequential nature of the text, capturing the contextual relationships between characters.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Transcription:&lt;br&gt; a. The final stage is a Transcription module, often a Connectionist Temporal Classification (CTC) layer or an attention-based decoder.&lt;br&gt; b. This module interprets the sequence-level representations from the &lt;abbr title="Recurrent Neural Network"&gt;RNN&lt;/abbr&gt; and generates the final text output, typically in the form of a character sequence or word-level transcription.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The three-stage architecture of CRNN has become a standard approach in many modern text recognition models, as it effectively combines the strengths of &lt;abbr title="Convolutional Neural Network"&gt;CNN&lt;/abbr&gt; for feature extraction, &lt;abbr title="Recurrent Neural Network"&gt;RNN&lt;/abbr&gt; for sequence modeling, and the final transcription module for generating the recognized text.&lt;/p&gt; &lt;p&gt;This modular design allows for greater flexibility and ease of optimization, as the individual components can be fine-tuned or replaced independently to improve the overall text recognition performance.&lt;/p&gt; &lt;p&gt;The pioneering work of CRNN has paved the way for many subsequent advancements in text recognition, solidifying the three-layer architecture as a foundational concept in modern OCR systems.&lt;/p&gt; &lt;p&gt;&lt;/p&gt; &lt;div style="text-align:center"&gt; &lt;p&gt;Fig 10. RCNN framework&lt;/p&gt; &lt;/div&gt; &lt;h4&gt;3.2.3 OCR Solution&lt;/h4&gt; &lt;p&gt;We have evaluated various open-source OCR software options, such as EasyOCR, PaddleOCR, and Tesseract OCR. We also considered the OCR solutions offered by major cloud platforms, including AWS, GCP, and Azure. However, we found that the open-source OCR solutions could not meet our desired performance requirements even after fine-tuning the models.&lt;/p&gt; &lt;p&gt;In our design, it is imperative to employ OCR methods for extracting crucial textual information. However, in practical application scenarios, the key text often appears blurry, posing challenges for accurate extraction. In light of this situation, we analyzed the two critical processes in OCR.&lt;/p&gt; &lt;p&gt;Regarding text detection, blurry text detection is arduous for text detection algorithms. Our evaluation of Paddle OCR and AWS OCR revealed suboptimal results in text detection. Specifically, AWS OCR's text detection encountered three issues: tag omission, incomplete coverage of bounding boxes, and semantic fragmentation of bounding boxes. Faced with these challenges, we sought a more suitable approach that deviated from traditional text detection methods and instead adopted YOLO. Although not explicitly designed for text detection, YOLO functions more like a boundary finder for textual images. While conventional text detection methods can identify text boundaries in various scenarios, YOLO demonstrates even more remarkable performance when dealing with blurry text in a single scene.&lt;/p&gt; &lt;p&gt;We opted for AWS OCR for text recognition because it has the absolute advantage of achieving an impressive accuracy rate of up to 98% after effectively handling text orientation challenges.&lt;/p&gt; &lt;p&gt;Considering the above, we ultimately chose the concatenated approach of YOLO (text detection) and AWS OCR (text recognition) as the solution for this practical implementation.&lt;/p&gt; &lt;h3 id="text-retrieval"&gt;3.3 Text Retrieval&lt;/h3&gt; &lt;p&gt;Text matching is a quintessential task in Natural Language Processing (NLP). This domain can be broadly sorted from experimental studies in scholarly papers into ad-hoc text retrieval, paraphrase identification, natural language inference (NLI), and question-answering (QA) matching. Furthermore, additional tasks such as entity disambiguation can leverage the principles of text matching. Despite divergent goals across various text-matching tasks, the underlying models tend to be strikingly similar, offering a level of interchangeability with potential variations in efficacy.&lt;/p&gt; &lt;p&gt;These text-matching endeavors aim to identify the most suitable document or to aggregate a list of documents in a descending order of relevance to a given query. To articulate the context, we denote the pair of texts subjected to the matching process as text_left and text_right. Here, text_left corresponds to the text of the query, and text_right represents a candidate document.&lt;/p&gt; &lt;p&gt;Conventionally, text-matching tasks have employed a feature-centric strategy, harnessing both texts' tf-idf, BM25, or lexical-level attributes, then applying standard machine learning algorithms like Logistic Regression or Support Vector Machines for training. The appeal of this methodology lies in its interpretability; however, the dependence on manual feature discovery and iterative experimentation has shown limitations in generalizability. Moreover, the model's performance is relatively unremarkable due to inherent restrictions in the number of features, limiting the model's parameter space.&lt;/p&gt; &lt;p&gt;The advent of deep learning since 2012, coupled with the proliferation of powerful GPUs, has made it feasible to cultivate large deep neural networks. This technological renaissance has sent ripples through numerous disciplines, including computer vision and natural language processing. Text matching has yet to be spared within NLP, as evidenced by Microsoft's groundbreaking introduction of the Deep Structured Semantic Model (DSSM) in 2013. This move signified a paradigm shift, propelling text matching into the deep learning epoch.&lt;/p&gt; &lt;p&gt;In contrast to the traditional feature-centric approach, text-matching methods in the deep learning era can be distilled into two main categories: representation-based matching and interaction-based matching. In representation-based matching, each text is handled independently during the initial phase. Texts are encoded via deep neural net layers to form their respective representations. Subsequently, these representations are a basis for calculating the texts' similarity through specialized similarity computation functions.&lt;/p&gt; &lt;p&gt;Conversely, the interaction-based matching approach critiques the late-stage similarity calculation of the prior method for its heavy dependence on representation quality and potential loss of fundamental text characteristics, such as lexical and syntactic features&lt;sup class="footnote-ref"&gt;&lt;a href="https://57blocks.io/blog/image-search-with-ai-models#footnote11"&gt;[11]&lt;/a&gt;&lt;a class="footnote-anchor" id="footnote-ref11" href="https://57blocks.io/blog/image-search-with-ai-models" &gt;&lt;/a&gt;&lt;/sup&gt;. Advocating for a more preemptive engagement of textual features, this method prioritizes capturing core characteristics early on. These foundational matching features are aggregated to derive a matching score at a more advanced processing level.&lt;/p&gt; &lt;p&gt;In our project, we deploy natural language processing models, including text-embedding-3-small, text-embedding-ada-002, and GTE Base, to generate embeddings for character text data recognized in images. We then calculate the cosine similarity of these text embeddings against those of query texts, which allows us to gauge the semantic relatedness of the texts. This approach forms the cornerstone of our text retrieval system, effectively harnessing semantic similarity to identify and retrieve relevant text information.&lt;/p&gt; &lt;h3 id="image-to-text-image-caption"&gt;3.4 Image-to-Text (Image Caption)&lt;/h3&gt; &lt;p&gt;Image Captioning is the task of describing the content of an image in words. This task lies at the intersection of computer vision and natural language processing. Most image captioning systems use an encoder-decoder framework, where an input image is encoded into an intermediate representation of the information in the image and then decoded into a descriptive text sequence.&lt;/p&gt; &lt;p&gt;In the last year, considerable progress has been seen in multimodal large language models (MM-LLMs). By adopting economical and efficient training methodologies, these cutting-edge models have fortified existing large language models to accommodate inputs or outputs across multiple modalities. The resultant models preserve the inherent reasoning and decision-making prowess that LLMs are known for while extending their capabilities to various multimodal tasks. Notably, functionalities such as generating descriptive captions for images and answering questions based on visual content are among their crucial advancements.&lt;/p&gt; &lt;p&gt;&lt;/p&gt; &lt;div style="text-align:center"&gt; &lt;p&gt;Fig 11. MM-LLMs&lt;sup class="footnote-ref"&gt;&lt;a href="https://57blocks.io/blog/image-search-with-ai-models#footnote12"&gt;[12]&lt;/a&gt;&lt;a class="footnote-anchor" id="footnote-ref12" href="https://57blocks.io/blog/image-search-with-ai-models" &gt;&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt; &lt;/div&gt; &lt;p&gt;MM-LLMs focusing on multimodal understanding typically encompass the first three components: modality encoders, the core LLM backbone, and modality generators. Throughout the training phase, these elements are generally maintained in a frozen state. Optimization efforts are concentrated on the input and output projectors, which are relatively lightweight. As a result, a small fraction of the overall parameters—commonly about 2%—are trainable within MM-LLMs. This percentage is determined by the size of the principal LLM integrated into the MM-LLM framework. This configuration allows MM-LLMs to undergo cost-effective training, making performance in assorted multimodal tasks more attainable.&lt;/p&gt; &lt;p&gt;&lt;/p&gt; &lt;div style="text-align:center"&gt; &lt;p&gt;Fig 12. The general model architecture of MM-LLMs and the implementation choices for each component&lt;sup class="footnote-ref"&gt;&lt;a href="https://57blocks.io/blog/image-search-with-ai-models#footnote12"&gt;[12]&lt;/a&gt;&lt;a class="footnote-anchor" id="footnote-ref12:1" href="https://57blocks.io/blog/image-search-with-ai-models" &gt;&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt; &lt;/div&gt; &lt;p&gt;We also carried out validations and tests for image caption generation using GPT4 vision, LLaVa&lt;sup class="footnote-ref"&gt;&lt;a href="https://57blocks.io/blog/image-search-with-ai-models#footnote13"&gt;[13]&lt;/a&gt;&lt;a class="footnote-anchor" id="footnote-ref13" href="https://57blocks.io/blog/image-search-with-ai-models" &gt;&lt;/a&gt;&lt;/sup&gt;&lt;sup class="footnote-ref"&gt;&lt;a href="https://57blocks.io/blog/image-search-with-ai-models#footnote14"&gt;[14]&lt;/a&gt;&lt;a class="footnote-anchor" id="footnote-ref14" href="https://57blocks.io/blog/image-search-with-ai-models" &gt;&lt;/a&gt;&lt;/sup&gt;, and Qwen-vl&lt;sup class="footnote-ref"&gt;&lt;a href="https://57blocks.io/blog/image-search-with-ai-models#footnote15"&gt;[15]&lt;/a&gt;&lt;a class="footnote-anchor" id="footnote-ref15" href="https://57blocks.io/blog/image-search-with-ai-models" &gt;&lt;/a&gt;&lt;/sup&gt; models. We meticulously analyzed the precision of the image descriptions provided by these models. Given that the GPT4 vision has yet to be made available for open-source use, we based our further enhancements on the Qwen-vl model&lt;sup class="footnote-ref"&gt;&lt;a href="https://57blocks.io/blog/image-search-with-ai-models#footnote16"&gt;[16]&lt;/a&gt;&lt;a class="footnote-anchor" id="footnote-ref16" href="https://57blocks.io/blog/image-search-with-ai-models" &gt;&lt;/a&gt;&lt;/sup&gt;, selecting it as our foundational model for fine-tuning after our comparative assessments were concluded.&lt;/p&gt; &lt;h2 id="conclusion"&gt;4. Conclusion&lt;/h2&gt; &lt;p&gt;During our image retrieval endeavor, we have diligently addressed the distinctive traits of images pertinent to the business sector, considering crucial image features, textual information, and details from both sides of the image. By analyzing and valorizing this information, we have implemented strategies such as reverse image search, textual retrieval, and image search via text. Our deployment of advanced deep learning algorithms encompasses an array of models—ranging from those specialized in computer vision and natural language processing to multimodal algorithms, large language models, and comprehensive multimodal language-vision models. Through systematic performance enhancement across each model phase, we have successfully realized an effective and precise retrieval process for images and accompanying text.&lt;/p&gt; &lt;h2 id="reference"&gt;Reference&lt;/h2&gt; &lt;hr class="footnotes-sep"&gt; &lt;section class="footnotes"&gt; &lt;ol class="footnotes-list"&gt; &lt;li id="footnote1" class="footnote-item"&gt;&lt;p&gt;Hameed, I. M., Abdulhussain, S. H., Mahmmod, B. M., &amp;amp; Pham, D. T. (2021). Content-based image retrieval: A review of recent trends. Cogent Engineering, 8(1). https://doi.org/10.1080/23311916.2021.1927469 &lt;a href="https://57blocks.io/blog/image-search-with-ai-models#footnote-ref1" class="footnote-backref"&gt;↩︎&lt;/a&gt; &lt;a href="https://57blocks.io/blog/#footnote-ref1:1" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li id="footnote2" class="footnote-item"&gt;&lt;p&gt;Raghunathan, B., &amp;amp; Acton, S. T., “A content based retrieval engine for circuit board inspection,” in Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), vol. 1, pp. 104–108 1999. Kobe, Japan. &lt;a href="https://57blocks.io/blog/image-search-with-ai-models#footnote-ref2" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li id="footnote3" class="footnote-item"&gt;&lt;p&gt;L. Zheng, Y. Yang and Q. Tian, &amp;quot;SIFT Meets &lt;abbr title="Convolutional Neural Network"&gt;CNN&lt;/abbr&gt;: A Decade Survey of Instance Retrieval,&amp;quot; in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 40, no. 5, pp. 1224-1244, 1 May 2018, doi: 10.1109/TPAMI.2017.2709749. &lt;a href="https://57blocks.io/blog/image-search-with-ai-models#footnote-ref3" class="footnote-backref"&gt;↩︎&lt;/a&gt; &lt;a href="https://57blocks.io/blog/#footnote-ref3:1" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li id="footnote4" class="footnote-item"&gt;&lt;p&gt;He K , Zhang X , Ren S ,et al.Deep Residual Learning for Image Recognition[J].IEEE, 2016.DOI:10.1109/CVPR.2016.90. &lt;a href="https://57blocks.io/blog/image-search-with-ai-models#footnote-ref4" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li id="footnote5" class="footnote-item"&gt;&lt;p&gt;Radford, Alec et al. “Learning Transferable Visual Models From Natural Language Supervision.” International Conference on Machine Learning (2021). &lt;a href="https://57blocks.io/blog/image-search-with-ai-models#footnote-ref5" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li id="footnote6" class="footnote-item"&gt;&lt;p&gt;Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Dollar; Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2017, pp. 2980-2988. &lt;a href="https://57blocks.io/blog/image-search-with-ai-models#footnote-ref6" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li id="footnote7" class="footnote-item"&gt;&lt;p&gt;Minghui Liao, Zhaoyi Wan, Cong Yao, Kai Chen, and Xiang Bai. 2020. Real-time scene text detection with differentiable binarization. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 11474--11481. &lt;a href="https://57blocks.io/blog/image-search-with-ai-models#footnote-ref7" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li id="footnote8" class="footnote-item"&gt;&lt;p&gt;Tian, Z. Huang, W., He, T., He, P., &amp;amp; Qiao, Y. (2016). Detecting text in natural image with connectionist text proposal network. In Proceedings of European conference on computer vision (ECCV) (pp. 56–72). Springer. &lt;a href="https://57blocks.io/blog/image-search-with-ai-models#footnote-ref8" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li id="footnote9" class="footnote-item"&gt;&lt;p&gt;Zhou, X.; Yao, C.; Wen, H.; Wang, Y.; Zhou, S.; He, W.; Liang, J. East: An efficient and accurate scene text detector.In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, HI, USA, 21–26 July 2017; pp. 2642–2651. &lt;a href="https://57blocks.io/blog/image-search-with-ai-models#footnote-ref9" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li id="footnote10" class="footnote-item"&gt;&lt;p&gt;Shi, B., Bai, X., Yao, C.: An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition. TPAMI 39(11), 2298–2304 (2016). &lt;a href="https://57blocks.io/blog/image-search-with-ai-models#footnote-ref10" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li id="footnote11" class="footnote-item"&gt;&lt;p&gt;&amp;quot;Google Landmarks Dataset v2 - A Large-Scale Benchmark for Instance-Level Recognition and Retrieval&amp;quot;T. Weyand*, A. Araujo*, B. Cao, J. Sim. Proc. CVPR'20. &lt;a href="https://57blocks.io/blog/image-search-with-ai-models#footnote-ref11" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li id="footnote12" class="footnote-item"&gt;&lt;p&gt;Zhang D, Yu Y, Li C, et al. Mm-llms: Recent advances in multimodal large language models[J]. arXiv preprint arXiv:2401.13601, 2024. &lt;a href="https://57blocks.io/blog/image-search-with-ai-models#footnote-ref12" class="footnote-backref"&gt;↩︎&lt;/a&gt; &lt;a href="https://57blocks.io/blog/#footnote-ref12:1" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li id="footnote13" class="footnote-item"&gt;&lt;p&gt;Liu H, Li C, Li Y, et al. Improved baselines with visual instruction tuning[J]. arXiv preprint arXiv:2310.03744, 2023. &lt;a href="https://57blocks.io/blog/image-search-with-ai-models#footnote-ref13" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li id="footnote14" class="footnote-item"&gt;&lt;p&gt;Liu H, Li C, Wu Q, et al. Visual instruction tuning[J]. Advances in neural information processing systems, 2024, 36. &lt;a href="https://57blocks.io/blog/image-search-with-ai-models#footnote-ref14" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li id="footnote15" class="footnote-item"&gt;&lt;p&gt;Huang P S, He X, Gao J, et al. Learning deep structured semantic models for web search using clickthrough data[C]//Proceedings of the 22nd ACM international conference on Information &amp;amp; Knowledge Management. 2013: 2333-2338. &lt;a href="https://57blocks.io/blog/image-search-with-ai-models#footnote-ref15" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li id="footnote16" class="footnote-item"&gt;&lt;p&gt;Bai J, Bai S, Yang S, et al. Qwen-vl: A frontier large vision-language model with versatile abilities[J]. arXiv preprint arXiv:2308.12966, 2023. &lt;a href="https://57blocks.io/blog/image-search-with-ai-models#footnote-ref16" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;/section&gt; &lt;/div&gt;</content>
    <link rel="enclosure" type="image/png" href="https://raw.githubusercontent.com/57blocks/website-article/main/articles/Image%20Search%20with%20AI%20Models/thumb_h.png"/>
  </entry>
  <entry>
    <title type="html">Drive iOS App Downloads with Clip - 57Blocks</title>
    <published>2025-03-25T00:00:00+00:00</published>
    <updated>2025-03-25T00:00:00+00:00</updated>
    <id>https://57blocks.io/blog/drive-ios-app-downloads-with-clip</id>
    <link rel="alternate" type="text/html" href="https://57blocks.io/blog/drive-ios-app-downloads-with-clip"/>
    <content type="html">&lt;div class="line-numbers old-specific-article-style_specificArticle__AvWT7 articleDetail_articleContent__vLTGR"&gt;&lt;p&gt;Our client, Hobnob, created an app for event planning that allows users to manage everything from invitations to guest lists to ticket sales and share content before, during, and after the event. This event planning app has been in the AppStore for over five years and has over 90,000 reviews. Hobnob spent thousands on monthly advertising to drive engagement and conversion, but the app didn't stand out and wasn't downloaded often in the highly competitive AppStore.&lt;/p&gt; &lt;p&gt;This is a challenge we have seen many times. We have observed some key reasons why apps aren't downloaded:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;If users don't quickly understand how an app will add value to their lives, they won't download it.&lt;/li&gt; &lt;li&gt;Increasingly complex app functionality requires a higher learning curve for users to perform simple tasks. Rather than spending time learning to use a new app, most will either not download it or continue to find new ways to use familiar apps.&lt;/li&gt; &lt;li&gt;The large file size of an app makes downloading time-consuming and takes up mobile phone storage space.&lt;/li&gt; &lt;li&gt;Moreover, a new app may not be trusted, potentially exposing personal privacy issues.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ideally, a trial version of an app would solve these problems. And Apple created just that with their offering of App Clip.&lt;/p&gt; &lt;h2 id="key-advantages-of-app-clip"&gt;Key Advantages of App Clip&lt;/h2&gt; &lt;p&gt;Hobnob wanted to increase downloads and attract more attention to their app, but after running multiple marketing campaigns, they ran out of ideas. The company needed a new approach.&lt;/p&gt; &lt;p&gt;We introduced them to App Clip. An App Clip is a relatively new form of an app launched in iOS14. Its purpose is to allow users to complete quick tasks from the main app without installing the entire app.&lt;/p&gt; &lt;p&gt;During our research, we learned that App Clips can:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Increase app conversion rates&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The average cost per acquisition (CPA) was &lt;a href="https://splitmetrics.com/blog/apple-search-ads-cost/" title="ref"&gt;$2.58 in 2023&lt;/a&gt;. Research shows that integrating App Clips into iOS applications can  &lt;a href="https://www.appsflyer.com/blog/tips-strategy/increase-app-downloads/" title="ref"&gt;increase the conversion rate to download and use an app by over 20%&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If App Clip users are satisfied with their experience, they are more likely to install the complete app to access all its features.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Provide a better user experience&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;App Clips enable customers to use an app's key features without downloading the entire app. This scenario is ideal for one-time use functions such as restaurant ordering, renting bicycles, and other one-off activities.&lt;/p&gt; &lt;p&gt;It enhances user participation by offering a defined, quick task that is easy to learn. By making the app easy to try and experience its usefulness, users are encouraged to download the entire app, subsequently increasing the conversion rate.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Offer numerous engagement options&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Integrating technologies like QR Codes, NFC Tags, App Clip Codes, Safari, Links in Messages, and cards in Maps in a Clip is straightforward. It's very versatile and flexible. &lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Reduce development costs&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;They require less development time (and cost less) to create because only part of the app's functionality is reused or modified.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Offer faster download times&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Since the app file size is smaller, it can be downloaded and used quickly.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Provide security and privacy&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;iOS will automatically delete the App Clip after a period of inactivity, and users can be anonymous since no account is required to access any functionality.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;App Clips have a wide range of uses&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;TikTok used an App Clip to encourage users to try and eventually install the TikTok app. &lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://x.com/illscience/status/1879273352013267154?s=46&amp;t=jqD_eUNYNaL8-a-0NmDklA" title="Explode"&gt;Explode&lt;/a&gt;, an app still under development, has received widespread attention for its Clip functionality.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;App Clips can be used to pay for parking when leaving the lot.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;However, there are a few disadvantages. Pictures cannot be stored locally, APN notifications are only valid for eight hours, and some iOS frameworks cannot be used.&lt;/p&gt; &lt;p&gt;Since the benefits far outweighed the disadvantages, Hobnob thought the App Clip was a great idea and commissioned 57Blocks to develop a version of their original app. We built an App Clip specifically for parties. A host can send or print a QR Code for guests to install the App Clip and share photos taken during a party. The feedback has been positive, and guests continue to download and use the App Clip.&lt;/p&gt; &lt;p&gt;One developer developed and launched the Clip version of their app within two months. Although we could have leveraged some of the existing code to give us a head start, we decided to start fresh to avoid referencing code sections, and to reduce complexity and file size.  Within months, the App Clip attracted new users, providing the client a new growth channel without incurring marketing and advertising costs.&lt;/p&gt;  &lt;h2 id="technical-development"&gt;Technical Development&lt;/h2&gt; &lt;p&gt;Although an App Clip may reuse an app's code directly and be efficient to create, the entire development, configuration, and testing process differs from that of traditional Apps.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Although an App Clip cannot exist independently and is based on the existing app, some Xcode development work is required to create the final product.&lt;/li&gt; &lt;li&gt;The App Clip's primary purpose is to entice users to download and continue to use the entire app.&lt;/li&gt; &lt;li&gt;Clips only provide the app's core functional modules. Due to Apple restrictions, the maximum size of an App Clip in its uncompressed state cannot exceed 15MB (iOS16 and above, iOS15 and below cannot exceed 10MB).&lt;/li&gt; &lt;li&gt;The small size requires more stringent feature requirements to ensure users can flash download and open the App Clip.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;One potential impediment: Since the codes in the entire app usually reference each other, an App Clip may have to reference module B to reference module A, and then reference module C, and so forth. This increases the code complexity and size rapidly.&lt;/p&gt; &lt;p&gt;That is why we recommend that the App Clip code be as independent as possible from the full app. Should you consider rewriting the App Clip code, the workload is not significant.&lt;/p&gt; &lt;p&gt;In addition to completing the development of essential App Clip functions, user data continuity and additional live activity/dynamic island further improve its value.&lt;/p&gt;  &lt;h2 id="blocks-app-clip-developer-notes-a-code-reference-workaround"&gt;57Blocks App Clip Developer Notes: A Code Reference Workaround&lt;/h2&gt; &lt;p&gt;&lt;em&gt;In some scenarios where code reference is required, macro definitions can be used. For example, in App &lt;code&gt;Build Setting&lt;/code&gt; -&amp;gt; &lt;code&gt;Other Swift Flags&lt;/code&gt; add &lt;code&gt;-DFULL_APP&lt;/code&gt;, and determine whether it is an App in the code:&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-swift"&gt;#if FULL_APP  //do something in App #else  //do something in Clip &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;Local data can be passed to the full App through the app group. First, you need to enable &lt;code&gt;&amp;quot;App Groups&amp;quot;&lt;/code&gt; at Capability, then in Clip:&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-swift"&gt;let sharedUserDefaults = UserDefaults(suiteName: &amp;quot;group.ClipToApp&amp;quot;) sharedUserDefaults.set(encodedData, forKey: &amp;quot;someKeywords&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;In Full App:&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-swift"&gt;let data = sharedUserDefaults.data(forKey: &amp;quot;someKeywords&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;Clip passes the user login information to the Full App, and the user can automatically log in after downloading the App. Clip logs in through AppleID and uses app group to pass login information:&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-swift"&gt;let credential = authorization.credential as? ASAuthorizationAppleIDCredential sharedUserDefaults.set(credential.user, forKey: &amp;quot;SavedAppleUserID&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;Full App gets AppleID:&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-swift"&gt;let userId = sharedUserDefaults.data(forKey: &amp;quot;SavedAppleUserID&amp;quot;) ASAuthorizationAppleIDProvider().getCredentialState(forUserID: userId) {state, error in …} &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;This App Clip logs in via credentials, uses keychain to pass login information, and kSecAttrLabel to distinguish keychain entries:&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-swift"&gt;let query = [     kSecAttrService: service,     kSecAttrAccount: account,     kSecClass: kSecClassGenericPassword,     kSecAttrLabel as String: &amp;quot;appClip&amp;quot; ] as CFDictionary let attributesToUpdate = [kSecValueData: credentialData] as CFDictionary SecItemUpdate(query, attributesToUpdate) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;Full App queries the keychain data corresponding to kSecAttrLabel:&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-swift"&gt;let query = [     kSecAttrService: service,      kSecAttrAccount: account,      kSecClass: kSecClassGenericPassword,     kSecReturnData: true,      kSecAttrLabel as String: &amp;quot;appClip&amp;quot; ] as CFDictionary  var result: AnyObject? SecItemCopyMatching(query, &amp;amp;result) &lt;/code&gt;&lt;/pre&gt; &lt;h3 id="how-to-make-qr-codesurlsmessages-more-effective"&gt;How to Make QR codes/URLs/Messages More Effective&lt;/h3&gt; &lt;p&gt;A noteworthy feature of this App Clip is that when the camera scans the QR code, Safari opens the corresponding URL, and Message sends the corresponding URL. App Clip cards will automatically display in the camera, Safari, and Message.&lt;/p&gt; &lt;p&gt;This feature mainly requires the configuration of the domain, including the AASA file, metadata on the web, and advanced Clip experience setting in AppstoreConnect：&lt;/p&gt; &lt;pre&gt;&lt;code class="language-html"&gt;&amp;quot;appclips&amp;quot;:  {  &amp;quot;apps&amp;quot;:[&amp;quot;ABCDE12345.com.example.MyApp.Clip&amp;quot;] }  &amp;lt;meta name=&amp;quot;apple-itunes-app&amp;quot; content=&amp;quot;app-id=myAppStoreID, app-clip-bundle-id=appClipBundleID, app-clip-display=card&amp;quot;&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;h3 id="app-clip-testing-process"&gt;App Clip Testing Process&lt;/h3&gt; &lt;p&gt;The typical process for using an App Clip is as follows:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Scan the QR code to use Safari to open the URL&lt;/li&gt; &lt;li&gt;Display App Clip cards&lt;/li&gt; &lt;li&gt;Click &amp;quot;Open&amp;quot; to download and use App Clip&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Because it differs from the full app, the above steps to display App Clip cards require Apple's services. The App Clip won't be updated if it does not pass the AppStore review and is not published. In order to test a new version of this App Clip, we needed to configure up to three test URLs in TestFlight and then click and start it in TestFlight, which is equivalent to passing the URL as a startup parameter to the App Clip. It will then begin processing based on the URL.&lt;/p&gt; &lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt; &lt;p&gt;App Clip is a relatively new type of app that has been added to iOS in recent years. Its original design intention is to encourage users to access an app quickly, enhancing the user experience and ultimately increasing the app conversion rate.&lt;/p&gt; &lt;p&gt;Our event planning App Clip:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Helped Hobnob improve adoption and conversion.&lt;/li&gt; &lt;li&gt;Received overwhelmingly positive feedback from users.&lt;/li&gt; &lt;li&gt;Leveraged the existing app's functionality so it was low-cost to produce.&lt;/li&gt; &lt;li&gt;Further improves the value by enabling better user data continuity and additional live activity/dynamic island.&lt;/li&gt; &lt;li&gt;Has received positive user feedback.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;However, keep in mind that App Clip development is very different from standard app development. We hope that 57Blocks' experience in project establishment, development, configuration, post-testing methods, and sharing some core logic codes helps developers streamline and successfully complete App Clip development.&lt;/p&gt; &lt;/div&gt;</content>
    <link rel="enclosure" type="image/png" href="https://raw.githubusercontent.com/57blocks/website-article/main/articles/Drive%20iOS%20App%20Downloads%20with%20Clip/ArticleImage_h.png"/>
  </entry>
  <entry>
    <title type="html">Deep Dive into Resource Limitations in Solana Development — CU Edition - 57Blocks</title>
    <published>2025-02-20T00:00:00+00:00</published>
    <updated>2025-02-20T00:00:00+00:00</updated>
    <id>https://57blocks.io/blog/deep-dive-into-resource-limitations-in-solana-development-cu-edition</id>
    <link rel="alternate" type="text/html" href="https://57blocks.io/blog/deep-dive-into-resource-limitations-in-solana-development-cu-edition"/>
    <content type="html">&lt;div class="line-numbers old-specific-article-style_specificArticle__AvWT7 articleDetail_articleContent__vLTGR"&gt;&lt;p&gt;Many developers face a common issue when building Solana programs (or &amp;quot;smart contracts&amp;quot;). Although their program's logic may appear correct, unexpected errors occur when the program runs. These errors often contain terms like &amp;quot;limit&amp;quot; or &amp;quot;exceed,&amp;quot; indicating that the program has hit one of Solana's resource constraints.&lt;/p&gt; &lt;p&gt;As a high-performance blockchain, Solana's core features, such as parallel processing, significantly boost transaction throughput. However, behind this efficiency lies a strict resource management mechanism. Developers need to understand these limitations to build and optimize Solana programs effectively.&lt;/p&gt; &lt;p&gt;This article exposes the various resource limitations in Solana development, focusing on Compute Unit (CU) restrictions, and provides analyses of multiple real-world scenarios and optimization strategies to illustrate how to avoid such program errors and improve program performance.&lt;/p&gt; &lt;h2 id="introduction-to-solana"&gt;Introduction to Solana&lt;/h2&gt; &lt;p&gt;&lt;a href="https://solana.com/" target="_blank"&gt;Solana&lt;/a&gt; was launched in 2020 and quickly emerged as a popular blockchain network, becoming one of the leading ecosystems in the cryptocurrency industry. In 2024, Solana accounted for 49.3% of global cryptocurrency investors' interest in specific chains, establishing its dominant position in the market.&lt;/p&gt; &lt;p&gt;Solana is a high-performance public blockchain platform that differs from traditional blockchain networks like Bitcoin and Ethereum. Its core characteristics include high throughput and low latency, thanks to its innovative Proof of History (PoH) consensus mechanism and efficient parallel processing architecture, enabling it to process thousands of transactions per second.&lt;/p&gt; &lt;h2 id="types-of-resource-limitations"&gt;Types of Resource Limitations&lt;/h2&gt; &lt;p&gt;Programs running on Solana are subject to several types of resource limitations. These limitations are designed to maintain the network's efficiency and stability while providing developers with clear boundaries for development. These constraints cover various requirements, from computational power to data storage, ensuring that programs can use system resources fairly while maintaining performance.&lt;/p&gt; &lt;h3 id="cu-limitations"&gt;CU Limitations&lt;/h3&gt; &lt;p&gt;In the Solana blockchain, a CU is the smallest unit used to measure the computational resources consumed during transaction execution. Each transaction on the chain consumes a certain number of CUs depending on the operations it performs (e.g., account writes, cross-program invocations, or system calls). Every transaction has a CU limit, which can be set to a default value or modified by the program. When a transaction exceeds the CU limit, processing is halted, resulting in a failure. Common operations like executing instructions, transferring data between programs, and performing cryptographic calculations consume CUs. CU systems are designed to manage resource allocation, prevent network abuse, and improve overall efficiency. For more details, refer to the official documentation &lt;a href="https://solana.com/docs/core/fees#compute-unit-limit" target="_blank"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;CU limits for a transaction containing only one instruction would default to 200,000. The limit can be adjusted using the &lt;code&gt;SetComputeUnitLimit&lt;/code&gt; instruction, but it cannot exceed the maximum transaction limit of 1.4 million CUs.&lt;/p&gt; &lt;h3 id="storage-limitations"&gt;Storage Limitations&lt;/h3&gt; &lt;p&gt;In Solana, each account's data structure is called AccountInfo, which includes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;the account's state,&lt;/li&gt; &lt;li&gt;program code (if it's a program account),&lt;/li&gt; &lt;li&gt;balance (in lamports, where 1 SOL = 1 billion lamports), and&lt;/li&gt; &lt;li&gt;the associated owner program (program ID).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In Solana's account model, each account is owned by a program, and only the program that owns the account can modify the account data or reduce the balance. Adding balance, however, is not restricted. This model ensures the security of account data and the controllability of operations. For more information, check out the documentation &lt;a href="https://solana.com/docs/core/accounts#accountinfo" target="_blank"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;h3 id="transaction-size-limitations"&gt;Transaction Size Limitations&lt;/h3&gt; &lt;p&gt;Solana follows a maximum transmission unit (MTU) limit of 1,280 bytes, in line with the IPv6 MTU standard, to ensure the efficient and reliable transmission of cluster information via UDP. After accounting for the 48-byte IPv6 and fragmentation headers, 1,232 bytes remain for data, such as serialized transactions. Each Solana transaction, including its signature and message parts, cannot exceed 1,232 bytes. Each signature occupies 64 bytes, with the number of signatures depending on the transaction requirements. The message contains instructions, accounts, and metadata, each taking 32 bytes. The total size of a transaction varies depending on the number of instructions it includes. This limit ensures efficient transmission of transaction data across the network. For more details, refer to the documentation &lt;a href="https://solana.com/docs/core/transactions#transaction-size" target="_blank"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;h3 id="call-depth-limitations"&gt;Call Depth Limitations&lt;/h3&gt; &lt;p&gt;To ensure efficient program execution, Solana imposes limits on each program's call stack depth. If this limit is exceeded, a &lt;code&gt;CallDepthExceeded&lt;/code&gt; error is triggered. Solana also supports direct calls between programs (cross-program invocations), but the depth of such calls is also limited. Exceeding this depth triggers a &lt;code&gt;CallDepth&lt;/code&gt; error. These restrictions aim to enhance network performance and resource management efficiency. For more information, refer to the documentation &lt;a href="https://solana.com/docs/programs/limitations#call-stack-depth-object-object-error" target="_blank"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;h3 id="stack-size-limitations"&gt;Stack Size Limitations&lt;/h3&gt; &lt;p&gt;In Solana's virtual machine architecture, each stack frame has a size limit, using fixed stack frames instead of variable stack pointers. If the stack frame exceeds this limit, the compiler will issue a warning but not block compilation. At runtime, if the stack size is exceeded, an &lt;code&gt;AccessViolation&lt;/code&gt; error occurs. Regarding heap management, Solana uses a simple heap with fast allocation via the &lt;strong&gt;bump&lt;/strong&gt; model, which does not support memory deallocation or reallocation. Each Solana program has access to this memory and can implement custom heap management as needed. These limitations help optimize performance and resource allocation. For more details, refer to the documentation &lt;a href="https://solana.com/docs/programs/faq#stack" target="_blank"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;h3 id="program-derived-addresses-pda-account-limitations"&gt;Program Derived Addresses (PDA) Account Limitations&lt;/h3&gt; &lt;p&gt;In Solana, Program Derived Addresses (PDAs) offer developers a deterministic method for generating account addresses using predefined &lt;strong&gt;seeds&lt;/strong&gt; (such as strings, numbers, or other account addresses) and the program ID. This mechanism mimics an on-chain hash map function.&lt;/p&gt; &lt;p&gt;Additionally, Solana allows programs to sign transactions using their derived PDAs. The advantage of PDAs is that developers do not need to remember specific account addresses; instead, they only need to remember the input used to derive the address, simplifying account management and improving development efficiency. For more details, check out the documentation &lt;a href="https://solana.com/developers/courses/native-onchain-development/program-derived-addresses#seeds" target="_blank"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;h3 id="limitations-summary"&gt;Limitations Summary&lt;/h3&gt; &lt;h4&gt;CU&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;The maximum CU limit per transaction is &lt;strong&gt;1.4 million&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;The default CU limit per instruction (each transaction contains multiple instructions) is &lt;strong&gt;200,000&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;The CU limit per block is &lt;strong&gt;48 million&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;The CU limit per user in a block is &lt;strong&gt;12 million&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h4&gt;Storage&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;The maximum storage size for each account in Solana is &lt;strong&gt;10MB&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h4&gt;Transaction Size&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;The maximum size of each transaction in Solana is &lt;strong&gt;1,232 bytes&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h4&gt;Call Depth&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;The maximum call stack depth per program is &lt;strong&gt;64 layers&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;The maximum depth for cross-program calls is &lt;strong&gt;4 layers&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h4&gt;Stack Size&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;Each stack frame has a size limit of &lt;strong&gt;4KB&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;The program heap size is &lt;strong&gt;32KB&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h4&gt;PDA&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;The length of each PDA seed cannot exceed &lt;strong&gt;32 bytes&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;The total number of seeds cannot exceed &lt;strong&gt;16 seeds&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2 id="detailed-analysis-of-cu-limitations"&gt;Detailed Analysis of CU Limitations&lt;/h2&gt; &lt;p&gt;Having introduced the various resource limitations in Solana, let's now focus on CU limitations. As mentioned earlier, CU is the smallest unit used to measure the computational resources consumed during transaction execution. Each transaction's CU consumption cannot exceed 1.2 million CUs. Since this concept might not be intuitive for developers new to Solana, we offer examples to help you understand CU consumption better.&lt;/p&gt; &lt;h3 id="displaying-cu-consumption"&gt;Displaying CU Consumption&lt;/h3&gt; &lt;p&gt;Before analyzing CU consumption in programs, let's first review how to display CU consumption in Solana programs. In Solana, the &lt;code&gt;log&lt;/code&gt; function can output logs, including CU consumption. Here is a simple example:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-rust"&gt;use solana_program::log::sol_log_compute_units;  sol_log_compute_units(); // other code sol_log_compute_units(); &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Each call to &lt;code&gt;sol_log_compute_units&lt;/code&gt; outputs the current CU consumption. Below is a sample log after the program runs:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;Program consumption: 149477 units remaining # other logs Program consumption: 137832 units remaining &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We can calculate the program's CU consumption by comparing the difference between two &lt;code&gt;sol_log_compute_units&lt;/code&gt; calls. In this example, the difference between the two values represents CUs consumed by the operations performed between the log calls.&lt;/p&gt; &lt;p&gt;We can also encapsulate this CU logging functionality into a Rust macro for easier program usage. Here is how the code would look:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-rust"&gt;// build a macro to log compute units #[macro_export] macro_rules! compute_fn {     ($msg:expr=&amp;gt; $($tt:tt)*) =&amp;gt; {{         msg!(concat!($msg, &amp;quot; {&amp;quot;));         sol_log_compute_units();         let res = { $($tt)* };         sol_log_compute_units();         msg!(concat!(&amp;quot; } // &amp;quot;, $msg));         res     }}; }  // use the macro to log compute units compute_fn!(&amp;quot;create account&amp;quot; =&amp;gt; {     // create account code }); &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Encapsulating logging as a macro makes it much more convenient to call within the program and access additional information, making debugging easier.&lt;/p&gt; &lt;h3 id="solana-program-examples"&gt;Solana Program Examples&lt;/h3&gt; &lt;p&gt;To better understand CU consumption in Solana programs, you can explore example programs demonstrating CU usage of different operations.&lt;/p&gt; &lt;p&gt;Solana provides many learning resources for beginners, including simple example programs (&lt;code&gt;program-examples&lt;/code&gt;) designed to help developers understand the Solana development process. These examples can be found in this GitHub &lt;a href="https://github.com/solana-developers/program-examples" target="_blank"&gt;repository&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Each example program in this repository typically has two versions: one implemented as a native program and the other using the Anchor framework.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;a href="https://www.anchor-lang.com/" target="_blank"&gt;Anchor&lt;/a&gt; is a widely used development framework for the Solana blockchain. It is designed to simplify the creation of programs (smart contracts) and decentralized applications (DApps). It provides developers with an efficient and intuitive set of tools and libraries, significantly lowering the barrier to entry for Solana application development. Solana officially recommends using Anchor for development.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;In the following sections, we'll reference examples from this repository to analyze CU consumption in Solana. We'll break this down from the perspectives of operations and programs.&lt;/p&gt; &lt;h3 id="operation-examples"&gt;Operation Examples&lt;/h3&gt; &lt;p&gt;Let's start by examining some common operations in Solana and analyzing their respective CU consumption.&lt;/p&gt; &lt;h4&gt;Transfer SOL&lt;/h4&gt; &lt;p&gt;Transferring SOL is one of the most common operations in Solana. Each transfer consumes a certain number of CUs. How many CUs are required for a single transfer? To find out, we can conduct a simple experiment on Solana's Devnet by performing a SOL transfer and then checking the transaction's CU consumption in the &lt;a href="https://explorer.solana.com/" target="_blank"&gt;Solana Explorer&lt;/a&gt;. The result is as follows:&lt;/p&gt; &lt;p&gt;&lt;/p&gt; &lt;p&gt;From the transaction details in the Explorer, we can see that the transfer consumed &lt;strong&gt;150&lt;/strong&gt; CUs. This value is not fixed but generally does not vary significantly. The number of CUs consumed is unrelated to the transfer amount but is determined by the number and complexity of the instructions in the transaction.&lt;/p&gt; &lt;h4&gt;Create Account&lt;/h4&gt; &lt;p&gt;Creating an account is another common operation in Solana. Each account creation consumes a certain number of CUs. We can analyze CU consumption by running an account creation example.&lt;/p&gt; &lt;p&gt;You can find a &lt;a href="https://github.com/solana-developers/program-examples/blob/main/basics/create-account/anchor/programs/create-system-account/src/lib.rs#L20C1-L33C12" target="_blank"&gt;sample program for account creation&lt;/a&gt; in the &lt;code&gt;basic/create-account&lt;/code&gt; directory in the &lt;code&gt;program-examples&lt;/code&gt; repository. By adding CU logging statements to the code, we can verify CU consumption during execution. Below is the log output after running the program:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;[2024-12-08T07:34:47.865105000Z DEBUG solana_runtime::message_processor::stable_log] Program consumption: 186679 units remaining [2024-12-08T07:34:47.865181000Z DEBUG solana_runtime::message_processor::stable_log] Program 11111111111111111111111111111111 invoke [2] [2024-12-08T07:34:47.865209000Z DEBUG solana_runtime::message_processor::stable_log] Program 11111111111111111111111111111111 success [2024-12-08T07:34:47.865217000Z DEBUG solana_runtime::message_processor::stable_log] Program consumption: 183381 units remaining [2024-12-08T07:34:47.865219000Z DEBUG solana_runtime::message_processor::stable_log] Program log: Account created successfully. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This test found that creating an account consumes approximately &lt;strong&gt;3,000&lt;/strong&gt; CUs. This value is not fixed but typically remains within a close range.&lt;/p&gt; &lt;h4&gt;Create a Simple Data Structure&lt;/h4&gt; &lt;p&gt;Next, let's analyze CU consumption by creating a simple data structure. An example of this can be found in the &lt;code&gt;basic/account-data&lt;/code&gt; directory of the &lt;code&gt;program-examples&lt;/code&gt; repository. The full example program can be found &lt;a href="https://github.com/solana-developers/program-examples/blob/main/basics/account-data/anchor/" target="_blank"&gt;here&lt;/a&gt;. Below is the data structure defined in the program:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-rust"&gt;use anchor_lang::prelude::*;  #[account] #[derive(InitSpace)] // automatically calculate the space required for the struct pub struct AddressInfo {     #[max_len(50)] // set a max length for the string     pub name: String, // 4 bytes + 50 bytes     pub house_number: u8, // 1 byte     #[max_len(50)]     pub street: String, // 4 bytes + 50 bytes     #[max_len(50)]     pub city: String, // 4 bytes + 50 bytes } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This data structure contains three string fields and one &lt;code&gt;u8&lt;/code&gt; field. Each string field has a maximum length of 50. Testing reveals that creating this simple data structure consumes approximately &lt;strong&gt;7,000&lt;/strong&gt; CUs.&lt;/p&gt; &lt;h4&gt;Counter&lt;/h4&gt; &lt;p&gt;Solana's official example programs include a simple counter program. This can be found in the &lt;code&gt;basic/counter&lt;/code&gt; directory of the &lt;code&gt;program-examples&lt;/code&gt; repository. The example defines a basic counter data structure and provides instructions for creating and incrementing the counter. The full example program is available &lt;a href="https://github.com/solana-developers/program-examples/tree/main/basics/counter/anchor" target="_blank"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Testing indicates that initializing the counter consumes approximately &lt;strong&gt;5,000&lt;/strong&gt; CUs, while incrementing the counter consumes about &lt;strong&gt;900&lt;/strong&gt; CUs.&lt;/p&gt; &lt;h4&gt;Transfer Token&lt;/h4&gt; &lt;p&gt;Another relatively common but more complex operation in Solana is transferring Tokens.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;In Solana, Tokens are implemented through the SPL Token standard, an official Solana standard for issuing, transferring, and managing tokens. SPL Token provides a standard interface to simplify token-related operations on Solana.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;The &lt;code&gt;program-examples&lt;/code&gt; repository includes an example program for transferring tokens, which can be found in the &lt;code&gt;token/transfer-tokens&lt;/code&gt; directory. This program also includes operations for creating tokens, minting tokens, and burning tokens. The full example code is available &lt;a href="https://github.com/solana-developers/program-examples/tree/main/tokens/transfer-tokens/native" target="_blank"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Testing results reveal the following CU consumption for token-related operations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Creating a token consumes approximately &lt;strong&gt;3,000&lt;/strong&gt; CUs.&lt;/li&gt; &lt;li&gt;Minting a token consumes approximately &lt;strong&gt;4,500&lt;/strong&gt; CUs.&lt;/li&gt; &lt;li&gt;Burning a token consumes approximately &lt;strong&gt;4,000&lt;/strong&gt; CUs.&lt;/li&gt; &lt;li&gt;Transferring a token consumes approximately &lt;strong&gt;4,500&lt;/strong&gt; CUs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We can also observe CU consumption of token transfers in an actual transaction. For example, in &lt;a href="https://explorer.solana.com/tx/FiqGufYKmKeGWfnyRXAkSx3UXPwp8iyZroBPCmcSNrdxNm1ydFqtBCvfq7iU5hTscc11ZuxzHP5dowVQFbgKv5s" target="_blank"&gt;this transaction&lt;/a&gt;, CU consumption for the token transfer can be seen in the log output at the bottom of the transaction details:&lt;/p&gt; &lt;p&gt;&lt;/p&gt; &lt;h4&gt;Summary&lt;/h4&gt; &lt;p&gt;Here is a summary of CU consumption for common operations:&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Action&lt;/th&gt; &lt;th&gt;CU Cost (approx.)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Transfer SOL&lt;/td&gt; &lt;td&gt;150&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Create Account&lt;/td&gt; &lt;td&gt;3,000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Create Simple data struct&lt;/td&gt; &lt;td&gt;7,000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Counter&lt;/td&gt; &lt;td&gt;5,000 (Init) &lt;br&gt; 900 (Add count)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Token&lt;/td&gt; &lt;td&gt;3,000 (Create) &lt;br&gt; 4,500 (Mint) &lt;br&gt; 4,000 (Burn) &lt;br&gt; 4,500 (Transfer)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h3 id="program-examples"&gt;Program Examples&lt;/h3&gt; &lt;p&gt;After reviewing the CU consumption of common operations in Solana, let's examine the CU consumption of frequently used program constructs and syntax.&lt;/p&gt; &lt;h4&gt;Loop Statements&lt;/h4&gt; &lt;p&gt;Loops are one of the common constructs in Solana programs. By analyzing loop statements, we can understand their CU consumption. Below is a comparison of CU usage for different loop sizes:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-rust"&gt;// simple msg print, cost 226 CU msg!(&amp;quot;i: {}&amp;quot;, 1);  // simple print for loop 1 time, cost 527 CU for i in 0..1 {     msg!(&amp;quot;i: {}&amp;quot;, i); }  // simple print for loop 2 times, cost 934 CU for i in 0..2 {     msg!(&amp;quot;i: {}&amp;quot;, i); } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Tests reveal that a simple &lt;code&gt;msg!&lt;/code&gt; statement consumes 226 CU. Adding a loop that runs once increases CU consumption to 527 CU while running the loop twice raises it to 934 CU. From this, we can deduce the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Initializing a loop costs approximately &lt;strong&gt;301 CU&lt;/strong&gt; (527 - 226).&lt;/li&gt; &lt;li&gt;Each iteration costs about &lt;strong&gt;181 CU&lt;/strong&gt; (934 - 2×226 - 301).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;To further verify the cost of CUs loops, we can use a more computationally expensive operation, such as printing account addresses:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-rust"&gt;// print account address, cost 11809 CU msg!(&amp;quot;A string {0}&amp;quot;, ctx.accounts.address_info.to_account_info().key());  // print account address in for loop 1 time, cost 12108 CU for i in 0..1 {     msg!(&amp;quot;A string {0}&amp;quot;, ctx.accounts.address_info.to_account_info().key()); }  // print account address in for loop 2 times, cost 24096 CU for i in 0..2 {     msg!(&amp;quot;A string {0}&amp;quot;, ctx.accounts.address_info.to_account_info().key()); } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As shown, loops' CU consumption depends on the logic executed within them. However, the loop itself has a relatively small overhead, roughly &lt;strong&gt;200–300 CU&lt;/strong&gt;.&lt;/p&gt; &lt;h4&gt;If Statements&lt;/h4&gt; &lt;p&gt;&lt;code&gt;If&lt;/code&gt; statements are another common construct. Let's analyze their CU consumption:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-rust"&gt;// a base function consumed 221 CU pub fn initialize(_ctx: Context&amp;lt;Initialize&amp;gt;) -&amp;gt; Result&amp;lt;()&amp;gt; {     Ok(()) }  // after add if statement, the CU consumed is 339 CU pub fn initialize(_ctx: Context&amp;lt;Initialize&amp;gt;) -&amp;gt; Result&amp;lt;()&amp;gt; {     if true {         Ok(())     } else {         Ok(())     } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Tests show that an empty function consumes 221 CU. Adding an &lt;code&gt;if&lt;/code&gt; statement increases the consumption to 339 CU. Therefore, a basic &lt;code&gt;if&lt;/code&gt; statement consumes approximately &lt;strong&gt;100 CU&lt;/strong&gt;.&lt;/p&gt; &lt;h4&gt;Different Data Structure Sizes&lt;/h4&gt; &lt;p&gt;The size of a data structure also affects CU consumption. Here is a comparison of different-sized data structures:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-rust"&gt;// use a default vector and push 10 items, it will consume 628 CU let mut a Vec&amp;lt;u32&amp;gt; = Vec::new(); for _ in 0..10 {     a.push(1); }  // use a 64-bit vector and do the same things, it will consume 682 CU let mut a Vec&amp;lt;u64&amp;gt; = Vec::new(); for _ in 0..10 {     a.push(1); }  // use an 8-bit vector and do the same things, it will consume 462 CU let mut a: Vec&amp;lt;u8&amp;gt; = Vec::new(); for _ in 0..10 {     a.push(1); } &lt;/code&gt;&lt;/pre&gt; &lt;ul&gt; &lt;li&gt;A &lt;code&gt;Vec&amp;lt;u32&amp;gt;&lt;/code&gt; storing 10 items consumes approximately &lt;strong&gt;628 CU&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;A &lt;code&gt;Vec&amp;lt;u64&amp;gt;&lt;/code&gt; storing 10 items consumes approximately &lt;strong&gt;682 CU&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;A &lt;code&gt;Vec&amp;lt;u8&amp;gt;&lt;/code&gt; storing 10 items consumes approximately &lt;strong&gt;462 CU&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This shows that the size of the data structure impacts CU usage, with larger types consuming more.&lt;/p&gt; &lt;h4&gt;Hash Functions&lt;/h4&gt; &lt;p&gt;Hash functions are a commonly used feature in Solana programs. Let's analyze their CU consumption using a simple example:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-rust"&gt;use solana_program::hash::hash;  pub fn initialize(_ctx: Context&amp;lt;Initialize&amp;gt;) -&amp;gt; Result&amp;lt;()&amp;gt; {     let data = b&amp;quot;some data&amp;quot;;     let _hash = hash(data);     Ok(()) } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Comparing a program with and without a hash function, we find that using Solana's hash function to compute a hash value consumes approximately &lt;strong&gt;200 CU&lt;/strong&gt;.&lt;/p&gt; &lt;h4&gt;Function Calls&lt;/h4&gt; &lt;p&gt;Function calls are essential in programming. Below is an example of analyzing CU consumption of calling a function:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-rust"&gt;pub fn initialize(_ctx: Context&amp;lt;Initialize&amp;gt;) -&amp;gt; Result&amp;lt;()&amp;gt; {     let result = other_func(_ctx)?;     Ok(()) }  pub fn other_func(_ctx: Context&amp;lt;Initialize&amp;gt;) -&amp;gt; Result&amp;lt;()&amp;gt; {     Ok(()) } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Tests indicate that the number of CUs consumed when calling a function depends on its logic. Calling an empty function consumes approximately &lt;strong&gt;100 CU&lt;/strong&gt;.&lt;/p&gt; &lt;h4&gt;CU Consumption Summary&lt;/h4&gt; &lt;p&gt;CU consumption of common program constructs is summarized below:&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Program Construct&lt;/th&gt; &lt;th&gt;CU Cost (approx.)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;For Loop&lt;/td&gt; &lt;td&gt;301 (Init) &lt;br&gt; 181 (Per Iteration)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;If Statement&lt;/td&gt; &lt;td&gt;100&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Different Data Sizes&lt;/td&gt; &lt;td&gt;462 (Vec u8) &lt;br&gt; 628 (Vec u32) &lt;br&gt; 682 (Vec u64)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Hash Function&lt;/td&gt; &lt;td&gt;200&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Function Call&lt;/td&gt; &lt;td&gt;100&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id="comparison-of-cu-consumption-between-native-programs-and-anchor-programs-on-solana"&gt;Comparison of CU Consumption Between Native Programs and Anchor Programs on Solana&lt;/h2&gt; &lt;p&gt;As previously mentioned, two main ways to develop Solana programs are using native programs and the Anchor framework. Anchor is the official framework recommended by Solana, providing a set of efficient and intuitive tools and libraries that significantly reduce the barrier to entry for Solana application development. You might be curious about the difference in CU consumption between native and Anchor programs. Next, we will compare both program's CU consumption using the example of &lt;strong&gt;Token Transfer&lt;/strong&gt; operations.&lt;/p&gt; &lt;h3 id="cu-consumption-of-native-programs"&gt;CU Consumption of Native Programs&lt;/h3&gt; &lt;p&gt;Let's first examine the native program CU consumption for transferring tokens. The source code for Solana programs is available in &lt;a href="https://github.com/solana-labs/solana-program-library" target="_blank"&gt;this repository&lt;/a&gt;, and the core method for processing token transfers, &lt;code&gt;process_transfer&lt;/code&gt;, can be found &lt;a href="https://github.com/solana-program/token/blob/main/program/src/processor.rs#L229-L343" target="_blank"&gt;here&lt;/a&gt;. In this method, we break down the steps involved and tally up the CU consumption for each step. The results of our analysis are as follows:&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Process&lt;/th&gt; &lt;th&gt;CU Cost&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Base consumption &lt;br&gt; Cost to run an empty method&lt;/td&gt; &lt;td&gt;939&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Transfer initialization &lt;br&gt; Includes account checks and initialization&lt;/td&gt; &lt;td&gt;2,641&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Checking if an account is frozen&lt;/td&gt; &lt;td&gt;105&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Checking if the source account has sufficient balance&lt;/td&gt; &lt;td&gt;107&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Verifying Token type match&lt;/td&gt; &lt;td&gt;123&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Checking Token address and expected decimal places&lt;/td&gt; &lt;td&gt;107&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Handling self-transfers&lt;/td&gt; &lt;td&gt;107&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Updating account balances&lt;/td&gt; &lt;td&gt;107&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Handling SOL transfers&lt;/td&gt; &lt;td&gt;103&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Saving account states&lt;/td&gt; &lt;td&gt;323&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;The total CU consumption for the token transfer operation is about 4,555 CU, which aligns closely with our previous test result (4,500 CU). The transfer initialization step has the highest cost, which consumes 2,641 CU. We can further break down the initialization phase into more detailed steps with the following CU consumption:&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Process&lt;/th&gt; &lt;th&gt;CU Cost&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Initializing the source account&lt;/td&gt; &lt;td&gt;106&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Initializing mint information&lt;/td&gt; &lt;td&gt;111&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Initializing the destination account&lt;/td&gt; &lt;td&gt;106&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Unpacking the source account&lt;/td&gt; &lt;td&gt;1,361&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Unpacking the destination account&lt;/td&gt; &lt;td&gt;1,361&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;The unpacking operations for both accounts consume the most CU, with each unpacking operation costing around 1,361 CU, which is significant. Developers should be aware of this during the development process.&lt;/p&gt; &lt;h3 id="cu-consumption-of-anchor-programs"&gt;CU Consumption of Anchor Programs&lt;/h3&gt; &lt;p&gt;Now that we have seen native programs' CU consumption, let's look at CUs consumption of Anchor programs. An example of an Anchor program can be found in the &lt;code&gt;program-examples&lt;/code&gt; repository under the &lt;code&gt;tokens/transfer-tokens&lt;/code&gt; directory. The source code for the token transfer operation can be found &lt;a href="https://github.com/solana-developers/program-examples/blob/main/tokens/transfer-tokens/anchor/programs/transfer-tokens/src/instructions/transfer.rs" target="_blank"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Upon running this instruction for the first time, we were surprised to find that CU consumption for an Anchor program performing a token transfer is around 80,000 to 90,000 CU—nearly &lt;strong&gt;20 times&lt;/strong&gt; that of the native program!&lt;/p&gt; &lt;p&gt;Why is CU consumption of an Anchor program so much higher? An Anchor program generally consists of two parts: one for account initialization and the other for instruction execution. Both parts contribute to the CU consumption. When we analyzed the source code of this program, we noticed the following:&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Process&lt;/th&gt; &lt;th&gt;CU Cost&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;The initialization cost of the Anchor framework&lt;/td&gt; &lt;td&gt;10,526&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Account initialization (from lines 9-34 in the source code)&lt;/td&gt; &lt;td&gt;20,544&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;The token transfer instruction (from lines 36-67 in the source code)&lt;/td&gt; &lt;td&gt;50,387&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;The total CU consumption of the program&lt;/td&gt; &lt;td&gt;81,457&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;Various accounts, such as &lt;code&gt;sender_token_account&lt;/code&gt; and &lt;code&gt;recipient_token_account&lt;/code&gt;, and programs like &lt;code&gt;token_program&lt;/code&gt; and &lt;code&gt;associated_token_program&lt;/code&gt;, need to be initialized during account initialization, which costs 20,544 CU.&lt;/p&gt; &lt;p&gt;The total cost of executing the token transfer instruction is 50,387 CU. Further breakdown of this process reveals:&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Process&lt;/th&gt; &lt;th&gt;CU Cost&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Function initialization costs (even an &lt;br&gt; empty method consumes this much CU)&lt;/td&gt; &lt;td&gt;6,213&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Print statement #1 (lines 38-41 in the source code) &lt;br&gt; implicitly converts the account address to Base58 encoding, &lt;br&gt; which is highly resource-intensive. &lt;br&gt; This is one of the reasons why &lt;br&gt; &lt;a href="https://solana.com/developers/guides/advanced/how-to-optimize-compute" target="_blank"&gt;Solana recommends avoiding this operation&lt;/a&gt;&lt;/td&gt; &lt;td&gt;11,770&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Print statement #2 (lines 42-45)&lt;/td&gt; &lt;td&gt;11,645&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Print statement #3 (lines 46-49)&lt;/td&gt; &lt;td&gt;11,811&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;The transfer instruction (lines 52-62), &lt;br&gt; where the &lt;code&gt;anchor_spl::token::transfer&lt;/code&gt; method is called. &lt;br&gt; This method wraps up the native &lt;code&gt;transfer&lt;/code&gt; method, &lt;br&gt; adding some extra functionality in addition to calling it&lt;/td&gt; &lt;td&gt;7,216&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Other miscellaneous costs add up&lt;/td&gt; &lt;td&gt;1,732&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Total to execute the token transfer instruction&lt;/td&gt; &lt;td&gt;50,387&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;From this analysis, we found that the actual CU consumption for the token transfer portion of the program is &lt;strong&gt;7,216&lt;/strong&gt; CU. However, due to the initialization of the Anchor framework, account initialization, and print statements, the total CU consumption for the program reaches 81,457 CU.&lt;/p&gt; &lt;p&gt;Although Anchor programs consume more CU, the framework provides more functionality and convenience, making this consumption understandable. Developers can choose the appropriate development method based on their needs.&lt;/p&gt; &lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article summarizes various resource limits in Solana development, focusing on the CU limit. We discuss the CU consumption for common operations and programs and compare the CU consumption of native and Anchor programs. Whether you are a beginner or an experienced developer on Solana, we hope this article helps you better understand CU consumption in Solana, enabling you to plan program resources more effectively and optimize performance during development.&lt;/p&gt; &lt;p&gt;We have also compiled some optimization tips based on Solana's official documentation to help developers avoid pitfalls related to CU limits. The tips are as follows:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Measure compute usage: Displaying CU consumption in logs can help assess the compute cost of code snippets, allowing you to identify high-cost areas.&lt;/li&gt; &lt;li&gt;Reduce logging: Logging operations (such as using the &lt;code&gt;msg!&lt;/code&gt; macro) significantly increase CU consumption, especially when dealing with Base58 encoding and string concatenation. For logging public keys and other data, it is recommended to log only essential information and use more efficient methods, such as &lt;code&gt;.key().log()&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Choose appropriate data types: Larger data types (like &lt;code&gt;u64&lt;/code&gt;) consume more CUs than smaller ones (like &lt;code&gt;u8&lt;/code&gt;). Use smaller data types whenever possible to reduce CU usage.&lt;/li&gt; &lt;li&gt;Optimize serialization operations: Serialization and deserialization operations increase CU consumption. Using zero-copy techniques to interact directly with account data can help reduce the overhead of these operations.&lt;/li&gt; &lt;li&gt;Optimize PDA lookup: The computational complexity of the &lt;code&gt;find_program_address&lt;/code&gt; function depends on how many attempts are needed to find a valid address. Storing the bump value during initialization and reusing it in subsequent operations can reduce CU consumption.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In future articles, we will discuss other aspects of Solana's resource limits and provide more insights. If you have any questions or comments, please feel free to click the &lt;strong&gt;Build With Us&lt;/strong&gt; button below.&lt;/p&gt; &lt;h2 id="references"&gt;References&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://solana.com/docs/core/fees" target="_blank"&gt;Solana Docs: Core Fees&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://solana.com/docs/core/accounts" target="_blank"&gt;Solana Docs: Core Accounts&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://solana.com/docs/core/transactions" target="_blank"&gt;Solana Docs: Core Transactions&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://solana.com/developers/courses/native-onchain-development/program-derived-addresses" target="_blank"&gt;Solana Docs: Program-Derived Addresses&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://solana.com/docs/programs/limitations" target="_blank"&gt;Solana Docs: Program Limitations&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://solana.com/docs/programs/faq" target="_blank"&gt;Solana Docs: Program FAQ&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/solana-developers/program-examples" target="_blank"&gt;Solana Developers: Program Examples&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.anchor-lang.com/" target="_blank"&gt;Anchor Lang&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://solana.com/developers/guides/advanced/how-to-optimize-compute" target="_blank"&gt;Solana Developers: How to Optimize Compute&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;</content>
    <link rel="enclosure" type="image/jpeg" href="https://raw.githubusercontent.com/57blocks/website-article/main/articles/Deep%20Dive%20into%20Resource%20Limitations%20in%20Solana%20Development%20CU%20Edition/thumb_h.jpg"/>
  </entry>
</feed>
